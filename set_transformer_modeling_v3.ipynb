{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Noonanav/phage_host_set_transformers/blob/main/set_transformer_modeling_v3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bcvLxj3ltRqd",
      "metadata": {
        "id": "bcvLxj3ltRqd"
      },
      "source": [
        "# Set Transformer for Strain-Phage Interaction Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "VH3zarJ2tRqe",
      "metadata": {
        "id": "VH3zarJ2tRqe"
      },
      "source": [
        "## Block 1: Import Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "lpJZMZIStRqe",
      "metadata": {
        "id": "lpJZMZIStRqe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e729396-09cb-4fcb-aa0d-d89960f1891f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import Dict, List, Tuple, Optional\n",
        "import warnings\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import random\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, average_precision_score, confusion_matrix\n",
        "from sklearn.metrics import roc_curve, precision_recall_curve, auc\n",
        "import seaborn as sns\n",
        "import pickle\n",
        "import os\n",
        "from pathlib import Path\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "bKbBEmcJ8r7S",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bKbBEmcJ8r7S",
        "outputId": "4dda8452-6524-485e-8f8c-3df167ca5866"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_w0bWlDjtRqe",
      "metadata": {
        "id": "_w0bWlDjtRqe"
      },
      "source": [
        "# Block 2: Data Loading and Preprocessing\n",
        "Description: This block handles the loading of strain and phage embeddings, splitting the interactions into train and test sets, creating datasets and data loaders, and managing variable-length inputs.\n",
        "\n",
        "## Hyperparameters and Architectural Decisions\n",
        "### `filter_interactions_by_strain` function:\n",
        "- `test_size`: Proportion of strains used for testing.\n",
        "  - Current value: 0.2\n",
        "  - Possible values to try: 0.1, 0.15, 0.25, 0.3\n",
        "\n",
        "### `create_data_loaders` function:  \n",
        "- `batch_size`: Number of samples per batch during training and testing.\n",
        "  - Current value: 1\n",
        "  - Possible values to try: 2, 4, 8, 16, 32\n",
        "- `num_workers`: Number of parallel workers for data loading.\n",
        "  - Current value: 0\n",
        "  - Possible values to try: 2, 4, 8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "zA1vfH63tRqe",
      "metadata": {
        "id": "zA1vfH63tRqe"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "def load_embeddings_with_ids(embedding_dir: str,\n",
        "                            genome_ids_csv: Optional[str] = None,\n",
        "                            id_column: str = \"genome_id\") -> Dict[str, Tuple[np.ndarray, List[str]]]:\n",
        "    \"\"\"\n",
        "    Load embeddings from a directory (.npy files).\n",
        "\n",
        "    Args:\n",
        "        embedding_dir: Path to directory containing embedding files (.npy)\n",
        "        genome_ids_csv: Optional path to CSV containing genome IDs to load\n",
        "        id_column: Column name in CSV containing the genome IDs\n",
        "\n",
        "    Returns:\n",
        "        Dictionary mapping strain/phage IDs to:\n",
        "            - a numpy array of shape [num_genes, embedding_dim]\n",
        "            - a list of gene IDs corresponding to the array rows\n",
        "    \"\"\"\n",
        "    embedding_dir = Path(embedding_dir)\n",
        "    embeddings = {}\n",
        "\n",
        "    # If genome_ids_csv is provided, load specific genome IDs\n",
        "    if genome_ids_csv:\n",
        "        try:\n",
        "            genome_df = pd.read_csv(genome_ids_csv)\n",
        "            if id_column not in genome_df.columns:\n",
        "                raise ValueError(f\"Column '{id_column}' not found in {genome_ids_csv}\")\n",
        "            genome_ids = set(genome_df[id_column].astype(str))\n",
        "            print(f\"Loaded {len(genome_ids)} genome IDs from {genome_ids_csv}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading genome IDs from {genome_ids_csv}: {e}\")\n",
        "            print(\"Falling back to loading all embeddings in directory\")\n",
        "            genome_ids = None\n",
        "    else:\n",
        "        genome_ids = None\n",
        "\n",
        "    # Get list of all embedding files\n",
        "    file_paths = list(embedding_dir.glob('*.npy'))\n",
        "    if not file_paths:\n",
        "        print(f\"No .npy files found in {embedding_dir}\")\n",
        "        return embeddings\n",
        "\n",
        "    # Track which genomes were found/not found\n",
        "    found_genomes = set()\n",
        "\n",
        "    # Process each file\n",
        "    for file_path in file_paths:\n",
        "        identifier = file_path.stem  # filename without extension\n",
        "\n",
        "        # Skip if not in the list of genome IDs (if a list was provided)\n",
        "        if genome_ids is not None and identifier not in genome_ids:\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            print(f'Loading: {identifier}')\n",
        "\n",
        "            # Load the dictionary format embeddings\n",
        "            embeddings_dict = np.load(file_path, allow_pickle=True).item()\n",
        "\n",
        "            # Extract genes and their IDs\n",
        "            gene_ids = list(embeddings_dict.keys())\n",
        "            embedding_list = [embeddings_dict[gene_id] for gene_id in gene_ids]\n",
        "\n",
        "            # Check if we have valid embeddings\n",
        "            if not embedding_list:\n",
        "                print(f\"Warning: No embeddings found in {file_path}\")\n",
        "                continue\n",
        "\n",
        "            embeddings_array = np.stack(embedding_list)\n",
        "\n",
        "            # Store both the array and the gene IDs\n",
        "            embeddings[identifier] = (embeddings_array, gene_ids)\n",
        "            found_genomes.add(identifier)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {file_path}: {e}\")\n",
        "\n",
        "    # Report on results\n",
        "    print(f\"Successfully loaded {len(embeddings)} embeddings from {embedding_dir}\")\n",
        "\n",
        "    # If genome IDs were provided, report on missing genomes\n",
        "    if genome_ids is not None:\n",
        "        missing_genomes = genome_ids - found_genomes\n",
        "        if missing_genomes:\n",
        "            print(f\"Warning: {len(missing_genomes)} genomes from the CSV were not found:\")\n",
        "            for genome in sorted(list(missing_genomes)[:10]):  # Show first 10\n",
        "                print(f\"  - {genome}\")\n",
        "            if len(missing_genomes) > 10:\n",
        "                print(f\"  ...and {len(missing_genomes) - 10} more\")\n",
        "\n",
        "    return embeddings\n",
        "\n",
        "def filter_interactions_by_strain(interactions_df: pd.DataFrame,\n",
        "                                  random_state: int = 42) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Split interactions into train/test by strain.\n",
        "    \"\"\"\n",
        "    unique_strains = interactions_df['strain'].unique()\n",
        "    train_strains, test_strains = train_test_split(\n",
        "        unique_strains,\n",
        "        test_size=0.2,\n",
        "        random_state=random_state\n",
        "    )\n",
        "\n",
        "    train_df = interactions_df[interactions_df['strain'].isin(train_strains)]\n",
        "    test_df = interactions_df[interactions_df['strain'].isin(test_strains)]\n",
        "\n",
        "    print(f\"Train set: {len(train_df)} interactions, {len(train_strains)} strains\")\n",
        "    print(f\"Test set:  {len(test_df)} interactions, {len(test_strains)} strains\")\n",
        "    return train_df, test_df\n",
        "\n",
        "class StrainPhageDataset(Dataset):\n",
        "    def __init__(self,\n",
        "                 interactions_df: pd.DataFrame,\n",
        "                 strain_embeddings: Dict[str, Tuple[np.ndarray, List[str]]],\n",
        "                 phage_embeddings: Dict[str, Tuple[np.ndarray, List[str]]]):\n",
        "        \"\"\"\n",
        "        Dataset for strain-phage interactions.\n",
        "        interactions_df has columns: ['strain', 'phage', 'interaction'].\n",
        "        \"\"\"\n",
        "        self.interactions = interactions_df.reset_index(drop=True)\n",
        "        self.strain_embeddings = strain_embeddings\n",
        "        self.phage_embeddings = phage_embeddings\n",
        "\n",
        "        # Check for missing embeddings\n",
        "        missing_strains = set(self.interactions['strain']) - set(strain_embeddings.keys())\n",
        "        missing_phages = set(self.interactions['phage']) - set(phage_embeddings.keys())\n",
        "\n",
        "        if missing_strains or missing_phages:\n",
        "            raise ValueError(\n",
        "                f\"Missing embeddings for {len(missing_strains)} strains \"\n",
        "                f\"and {len(missing_phages)} phages.\"\n",
        "            )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.interactions)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.interactions.iloc[idx]\n",
        "        strain_emb = torch.tensor(\n",
        "            self.strain_embeddings[row['strain']][0], dtype=torch.float32\n",
        "        )  # shape [n_s, 1280]\n",
        "        phage_emb = torch.tensor(\n",
        "            self.phage_embeddings[row['phage']][0], dtype=torch.float32\n",
        "        )   # shape [n_p, 1280]\n",
        "        label = torch.tensor(row['interaction'], dtype=torch.float32)\n",
        "        return strain_emb, phage_emb, label, row['strain'], row['phage']\n",
        "\n",
        "def collate_variable_sets(batch):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      batch: list of (strain_emb, phage_emb, label, strain_id, phage_id)\n",
        "        - strain_emb: shape [S, emb_dim]\n",
        "        - phage_emb:  shape [P, emb_dim]\n",
        "        - label: single scalar or shape (1,)\n",
        "        - strain_id: string identifier for strain\n",
        "        - phage_id: string identifier for phage\n",
        "\n",
        "    Returns:\n",
        "      strain_padded: (B, S_max, emb_dim)\n",
        "      phage_padded : (B, P_max, emb_dim)\n",
        "      strain_mask  : (B, S_max) boolean\n",
        "      phage_mask   : (B, P_max) boolean\n",
        "      label_batch  : (B, 1)\n",
        "      strain_ids   : list of strain identifiers\n",
        "      phage_ids    : list of phage identifiers\n",
        "    \"\"\"\n",
        "    # Unpack the batch\n",
        "    strains, phages, labels, strain_ids, phage_ids = zip(*batch)\n",
        "\n",
        "    # 1) Find max set sizes\n",
        "    max_strain_len = max(s.shape[0] for s in strains)\n",
        "    max_phage_len  = max(p.shape[0] for p in phages)\n",
        "\n",
        "    # 2) Emb dim (assume consistent)\n",
        "    emb_dim = strains[0].shape[1]\n",
        "    batch_size = len(batch)\n",
        "\n",
        "    # 3) Allocate zero-padded Tensors + boolean masks\n",
        "    strain_padded = torch.zeros(batch_size, max_strain_len, emb_dim, dtype=torch.float32)\n",
        "    phage_padded  = torch.zeros(batch_size, max_phage_len,  emb_dim, dtype=torch.float32)\n",
        "\n",
        "    strain_mask   = torch.zeros(batch_size, max_strain_len, dtype=torch.bool)\n",
        "    phage_mask    = torch.zeros(batch_size, max_phage_len,  dtype=torch.bool)\n",
        "\n",
        "    label_batch   = torch.zeros(batch_size, 1, dtype=torch.float32)\n",
        "\n",
        "    # 4) Copy each sample's data into padded Tensors\n",
        "    for i, (s_emb, p_emb, label) in enumerate(zip(strains, phages, labels)):\n",
        "        s_len = s_emb.shape[0]\n",
        "        p_len = p_emb.shape[0]\n",
        "\n",
        "        strain_padded[i, :s_len, :] = s_emb\n",
        "        phage_padded[i, :p_len, :]  = p_emb\n",
        "        strain_mask[i, :s_len]      = True\n",
        "        phage_mask[i,  :p_len]      = True\n",
        "\n",
        "        label_batch[i, 0] = float(label)\n",
        "\n",
        "    return strain_padded, phage_padded, strain_mask, phage_mask, label_batch, strain_ids, phage_ids\n",
        "\n",
        "def create_data_loaders(train_df: Optional[pd.DataFrame],\n",
        "                            test_df: pd.DataFrame,\n",
        "                            strain_embeddings: Dict[str, np.ndarray],\n",
        "                            phage_embeddings: Dict[str, np.ndarray],\n",
        "                            batch_size: int = 1) -> Tuple[DataLoader, DataLoader]:\n",
        "\n",
        "        train_loader = None # Initialize train_loader as None\n",
        "        if train_df is not None and not train_df.empty: # add a check before proceeding:\n",
        "            train_dataset = StrainPhageDataset(train_df, strain_embeddings, phage_embeddings)\n",
        "            train_loader = DataLoader(\n",
        "                train_dataset,\n",
        "                batch_size=batch_size,\n",
        "                shuffle=True,\n",
        "                num_workers=0,\n",
        "                pin_memory=True,\n",
        "                collate_fn=collate_variable_sets\n",
        "            )\n",
        "\n",
        "        test_dataset  = StrainPhageDataset(test_df, strain_embeddings, phage_embeddings)\n",
        "        test_loader = DataLoader(\n",
        "            test_dataset,\n",
        "            batch_size=batch_size,\n",
        "            shuffle=False,\n",
        "            num_workers=0,\n",
        "            pin_memory=True,\n",
        "            collate_fn=collate_variable_sets\n",
        "        )\n",
        "\n",
        "        return train_loader, test_loader # now, it can return None for `train_loader`"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inspect Embeddings"
      ],
      "metadata": {
        "id": "-sI2ylD3hZJT"
      },
      "id": "-sI2ylD3hZJT"
    },
    {
      "cell_type": "code",
      "source": [
        "def inspect_embedding_structure(embedding_dir: str):\n",
        "    \"\"\"\n",
        "    Load the first embedding file and inspect its structure\n",
        "    to understand how gene IDs are stored.\n",
        "    \"\"\"\n",
        "    embedding_dir = Path(embedding_dir)\n",
        "\n",
        "    # Get the first file only\n",
        "    file_paths = list(embedding_dir.glob('*.npy'))\n",
        "    if not file_paths:\n",
        "        print(f\"No .npy files found in {embedding_dir}\")\n",
        "        return\n",
        "\n",
        "    first_file = file_paths[0]\n",
        "    identifier = first_file.stem  # filename without extension\n",
        "    print(f'Inspecting file: {first_file}')\n",
        "\n",
        "    # Load the file\n",
        "    embedding = np.load(first_file, allow_pickle=True)\n",
        "\n",
        "    # Print basic info\n",
        "    print(f\"\\nEmbedding identifier: {identifier}\")\n",
        "    print(f\"Embedding type: {type(embedding)}\")\n",
        "    print(f\"Embedding shape: {embedding.shape}\")\n",
        "\n",
        "    # If it's a numpy array, show a sample\n",
        "    if isinstance(embedding, np.ndarray):\n",
        "        if embedding.ndim == 2:\n",
        "            print(\"\\nSample (first 3 rows, first 5 columns):\")\n",
        "            print(embedding[:3, :5])\n",
        "        else:\n",
        "            print(\"\\nUnexpected array dimensions. Full structure:\")\n",
        "            print(embedding)\n",
        "\n",
        "    # If it's a dictionary or object array, examine structure\n",
        "    elif isinstance(embedding, (dict, np.ndarray)) and hasattr(embedding, 'item'):\n",
        "        if hasattr(embedding.item(), 'keys'):\n",
        "            print(\"\\nEmbedding appears to be a dictionary-like object\")\n",
        "            print(\"Keys:\", embedding.item().keys())\n",
        "            # Sample the first item\n",
        "            first_key = list(embedding.item().keys())[0]\n",
        "            print(f\"\\nSample for key '{first_key}':\")\n",
        "            print(embedding.item()[first_key][:5] if isinstance(embedding.item()[first_key], (list, np.ndarray)) else embedding.item()[first_key])\n",
        "\n",
        "    # Check any attributes\n",
        "    if hasattr(embedding, '__dict__'):\n",
        "        print(\"\\nAttributes:\")\n",
        "        for attr in dir(embedding):\n",
        "            if not attr.startswith('_'):\n",
        "                print(f\"  {attr}\")\n",
        "\n",
        "    return embedding  # Return for further inspection if needed\n",
        "\n",
        "# Call the function\n",
        "sample_embedding = inspect_embedding_structure('/content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/strains')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LILa4QK3hcLg",
        "outputId": "b9cd0d58-eeeb-40fb-fb54-f8eaef809966"
      },
      "id": "LILa4QK3hcLg",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inspecting file: /content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/strains/ECOR17.npy\n",
            "\n",
            "Embedding identifier: ECOR17\n",
            "Embedding type: <class 'numpy.ndarray'>\n",
            "Embedding shape: (49, 1280)\n",
            "\n",
            "Sample (first 3 rows, first 5 columns):\n",
            "[[ 2.3735325  -4.012263   -1.0772631  -1.048405   -2.309111  ]\n",
            " [ 0.8825483  -3.5288646  -1.7427737  -0.44220394  1.2755007 ]\n",
            " [ 0.3647181  -5.709437   -2.2260387  -0.1618739  -4.900755  ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xVpPLTqbtRqf",
      "metadata": {
        "id": "xVpPLTqbtRqf"
      },
      "source": [
        "# Block 3: Set Transformer Blocks\n",
        "Description: This block implements the core components of the Set Transformer architecture, including the Multi-head Attention Block (MAB), Induced Set Attention Block (ISAB), and Pooling by Multihead Attention (PMA).\n",
        "\n",
        "## Hyperparameters and Architectural Decisions\n",
        "### `MAB` class:\n",
        "- `num_heads`: Number of attention heads in the multi-head attention mechanism.\n",
        "  - Current value: Determined by the `num_heads` parameter passed to the `ISAB` and `PMA` classes\n",
        "  - Possible values to try: 4, 8, 12, 16\n",
        "- `ln`: Whether to apply layer normalization after the attention operation.\n",
        "  - Current value: False\n",
        "  - Possible values to try: True\n",
        "\n",
        "### `ISAB` class:\n",
        "- `num_inds`: Number of inducing points used to reduce computational complexity.\n",
        "  - Current value: Determined by the `strain_inds` and `phage_inds` parameters in the `StrainPhageTransformer` class\n",
        "    - For the strain encoder: `strain_inds=128`\n",
        "    - For the phage encoder: `phage_inds=64`\n",
        "  - Possible values to try: 32, 64, 96, 128\n",
        "- `ln`: Whether to apply layer normalization.\n",
        "  - Current value: False\n",
        "  - Possible values to try: True\n",
        "\n",
        "### `PMA` class:\n",
        "- `num_heads`: Number of attention heads in the pooling mechanism.\n",
        "  - Current value: Determined by the `num_heads` parameter passed to the `StrainPhageTransformer` class\n",
        "  - Possible values to try: 4, 8, 12, 16  \n",
        "- `num_seeds`: Number of output vectors after pooling.\n",
        "  - Current value: 1\n",
        "  - Possible values to try: 2, 4, 8\n",
        "- `ln`: Whether to apply layer normalization.  \n",
        "  - Current value: False\n",
        "  - Possible values to try: True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "kHIKiwyrtRqf",
      "metadata": {
        "id": "kHIKiwyrtRqf"
      },
      "outputs": [],
      "source": [
        "class MAB(nn.Module):\n",
        "    \"\"\"Multi-head Attention Block.\"\"\"\n",
        "    def __init__(self, dim_Q, dim_K, dim_V, num_heads, ln=False, temperature=0.1):\n",
        "        super(MAB, self).__init__()\n",
        "        self.dim_V = dim_V\n",
        "        self.num_heads = num_heads\n",
        "        self.temperature = temperature  # Add this parameter\n",
        "        self.fc_q = nn.Linear(dim_Q, dim_V)\n",
        "        self.fc_k = nn.Linear(dim_K, dim_V)\n",
        "        self.fc_v = nn.Linear(dim_K, dim_V)\n",
        "        self.fc_o = nn.Linear(dim_V, dim_V)\n",
        "        self.ln = nn.LayerNorm(dim_V) if ln else nn.Identity()\n",
        "\n",
        "    def forward(self, Q, K, mask=None, return_attn=False):\n",
        "        \"\"\"\n",
        "        Q: [B, n_q, d]\n",
        "        K: [B, n_k, d]\n",
        "        mask: [B, n_k] boolean (True = real, False = pad)\n",
        "        \"\"\"\n",
        "        Q_ = self.fc_q(Q)\n",
        "        K_ = self.fc_k(K)\n",
        "        V_ = self.fc_v(K)\n",
        "\n",
        "        B, n_q, _ = Q_.shape\n",
        "        _, n_k, _ = K_.shape\n",
        "        dim_split = self.dim_V // self.num_heads\n",
        "\n",
        "        # reshape\n",
        "        Q_ = Q_.view(B, n_q, self.num_heads, dim_split).transpose(1,2)  # [B,h,n_q,d_h]\n",
        "        K_ = K_.view(B, n_k, self.num_heads, dim_split).transpose(1,2)\n",
        "        V_ = V_.view(B, n_k, self.num_heads, dim_split).transpose(1,2)\n",
        "\n",
        "        # compute attention logits with temperature scaling\n",
        "        A = torch.matmul(Q_, K_.transpose(-2, -1)) / (np.sqrt(dim_split) * self.temperature)  # [B,h,n_q,n_k]\n",
        "\n",
        "        # if we have a mask, apply it to A\n",
        "        if mask is not None:\n",
        "            # mask: [B, n_k], we need [B,h,n_q,n_k] for broadcast\n",
        "            mask_expanded = mask.unsqueeze(1).unsqueeze(2)  # [B,1,1,n_k]\n",
        "            # fill masked positions with -inf\n",
        "            A = A.masked_fill(~mask_expanded, float('-inf'))\n",
        "\n",
        "        A = torch.softmax(A, dim=-1)  # [B,h,n_q,n_k]\n",
        "\n",
        "        # Weighted sum\n",
        "        O = torch.matmul(A, V_)  # [B,h,n_q,d_h]\n",
        "        O = O.transpose(1,2).contiguous().view(B, n_q, self.dim_V)\n",
        "        O = self.ln(O + F.relu(self.fc_o(O)))\n",
        "\n",
        "        if return_attn:\n",
        "            return O, A\n",
        "        else:\n",
        "            return O\n",
        "\n",
        "class ISAB(nn.Module):\n",
        "    \"\"\"\n",
        "    Induced Set Attention Block.\n",
        "    Reduces complexity by using M learned 'inducing' vectors.\n",
        "    \"\"\"\n",
        "    def __init__(self, dim_in, dim_out, num_heads, num_inds, ln=False, temperature=0.1):\n",
        "        super(ISAB, self).__init__()\n",
        "        self.num_inds = num_inds\n",
        "        self.I = nn.Parameter(torch.Tensor(1, num_inds, dim_out))\n",
        "        nn.init.xavier_uniform_(self.I)\n",
        "\n",
        "        self.mab1 = MAB(dim_out, dim_in, dim_out, num_heads, ln=ln, temperature=temperature)\n",
        "        self.mab2 = MAB(dim_in, dim_out, dim_out, num_heads, ln=ln, temperature=temperature)\n",
        "\n",
        "    def forward(self, X, mask=None):\n",
        "        # Use self.mab1, not self.isab1:\n",
        "        H = self.mab1(self.I.repeat(X.size(0),1,1), X, mask=mask)\n",
        "        H = self.mab2(X, H)\n",
        "        return H\n",
        "\n",
        "class PMA(nn.Module):\n",
        "    \"\"\"\n",
        "    Pooling by Multihead Attention to reduce a set to num_seeds outputs.\n",
        "    By default, set num_seeds=1 to get a single vector.\n",
        "    \"\"\"\n",
        "    def __init__(self, dim, num_heads, num_seeds=1, ln=False, temperature=0.1):\n",
        "        super(PMA, self).__init__()\n",
        "        self.num_seeds = num_seeds\n",
        "        self.S = nn.Parameter(torch.Tensor(1, num_seeds, dim))\n",
        "        nn.init.xavier_uniform_(self.S)\n",
        "        self.mab = MAB(dim, dim, dim, num_heads, ln=ln, temperature=temperature)\n",
        "\n",
        "    def forward(self, X, mask=None):\n",
        "        B = X.size(0)\n",
        "        S = self.S.repeat(B,1,1)\n",
        "        # X as \"K\", so pass mask\n",
        "        O = self.mab(S, X, mask=mask)\n",
        "        return O\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fHd602MPtRqf",
      "metadata": {
        "id": "fHd602MPtRqf"
      },
      "source": [
        "# Block 4: Cross-Attention  \n",
        "Description: This block implements the cross-attention mechanism, allowing strain embeddings to attend to phage embeddings and vice versa.\n",
        "\n",
        "## Hyperparameters and Architectural Decisions\n",
        "### `CrossAttention` class:\n",
        "- `num_heads`: Number of attention heads in the cross-attention mechanism.\n",
        "  - Current value: Determined by the `num_heads` parameter passed to the `StrainPhageTransformer` class\n",
        "  - Possible values to try: 4, 8, 12, 16\n",
        "- `ln`: Whether to apply layer normalization.\n",
        "  - Current value: False\n",
        "  - Possible values to try: True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "y471SGL8tRqg",
      "metadata": {
        "id": "y471SGL8tRqg"
      },
      "outputs": [],
      "source": [
        "class CrossAttention(nn.Module):\n",
        "    \"\"\"Cross-attention between two sets.\"\"\"\n",
        "    def __init__(self, dim, num_heads, ln=False, temperature=0.1):\n",
        "        super(CrossAttention, self).__init__()\n",
        "        self.mab = MAB(dim, dim, dim, num_heads, ln=ln, temperature=temperature)\n",
        "\n",
        "    def forward(self, X, Y, mask=None, return_attn=False):\n",
        "        # Pass mask & return_attn to MAB\n",
        "        return self.mab(X, Y, mask=mask, return_attn=return_attn)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fM2C1Sl7tRqg",
      "metadata": {
        "id": "fM2C1Sl7tRqg"
      },
      "source": [
        "# Block 5: Encoders and Complete Model\n",
        "Description: This block defines the set encoders for strain and phage embeddings and the complete `StrainPhageTransformer` model, which combines the set encoders, cross-attention blocks, pooling, and classification layers.\n",
        "\n",
        "## Hyperparameters and Architectural Decisions\n",
        "### `SetEncoder` class:\n",
        "- `num_heads`: Number of attention heads in the ISAB layers.\n",
        "  - Current value: Determined by the `num_heads` parameter passed to the `StrainPhageTransformer` class\n",
        "  - Possible values to try: 4, 8, 12, 16\n",
        "- `num_inds`: Number of inducing points used in the ISAB layers.\n",
        "  - Current value: Determined by the `strain_inds` and `phage_inds` parameters in the `StrainPhageTransformer` class\n",
        "    - For the strain encoder: `strain_inds=128`\n",
        "    - For the phage encoder: `phage_inds=64`\n",
        "  - Possible values to try: 32, 64, 96, 128\n",
        "- `ln`: Whether to apply layer normalization.\n",
        "  - Current value: False\n",
        "  - Possible values to try: True\n",
        "\n",
        "### `StrainPhageTransformer` class:\n",
        "- `embedding_dim`: Dimension of the input embeddings.\n",
        "  - Current value: 1280\n",
        "  - Possible values to try: 512, 768, 1024 (depending on the embedding model)\n",
        "- `num_heads`: Number of attention heads in the cross-attention blocks and PMA layers.\n",
        "  - Current value: 8\n",
        "  - Possible values to try: 4, 12, 16\n",
        "- `strain_inds`: Number of inducing points used in the strain encoder.\n",
        "  - Current value: 128\n",
        "  - Possible values to try: 64, 96, 160, 192  \n",
        "- `phage_inds`: Number of inducing points used in the phage encoder.\n",
        "  - Current value: 64\n",
        "  - Possible values to try: 32, 48, 80, 96\n",
        "- `dropout`: Dropout rate applied in the classifier.\n",
        "  - Current value: 0.1\n",
        "  - Possible values to try: 0.0, 0.2, 0.3, 0.4\n",
        "- `ln`: Whether to apply layer normalization in the set encoders, cross-attention blocks, PMA layers, and classifier.\n",
        "  - Current value: True\n",
        "  - Possible values to try: False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "TwbLmM_vtRqg",
      "metadata": {
        "id": "TwbLmM_vtRqg"
      },
      "outputs": [],
      "source": [
        "class SetEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Encodes a set of embeddings into a refined set\n",
        "    using ISAB layers.\n",
        "    NOTE: We do NOT pool down to a single token here.\n",
        "          That happens after cross-attention.\n",
        "    \"\"\"\n",
        "    def __init__(self, dim_input, dim_output, num_heads, num_inds, ln=False, temperature=0.1):\n",
        "        super(SetEncoder, self).__init__()\n",
        "        self.isab1 = ISAB(dim_input, dim_output, num_heads, num_inds, ln=ln, temperature=temperature)\n",
        "        self.isab2 = ISAB(dim_output, dim_output, num_heads, num_inds, ln=ln, temperature=temperature)\n",
        "\n",
        "    def forward(self, X, mask=None):\n",
        "        H = self.isab1(X, mask=mask)\n",
        "        H = self.isab2(H, mask=mask)\n",
        "        return H\n",
        "\n",
        "\n",
        "class StrainPhageTransformer(nn.Module):\n",
        "    \"\"\"\n",
        "    1) Encode strain genes and phage genes via ISAB stacks.\n",
        "    2) Cross-attend them at the gene level.\n",
        "    3) Pool each cross-attended set to a single vector via PMA.\n",
        "    4) Concatenate and classify.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 embedding_dim=1280,\n",
        "                 num_heads=8,\n",
        "                 strain_inds=128,\n",
        "                 phage_inds=64,\n",
        "                 dropout=0.1,\n",
        "                 ln=True,\n",
        "                 temperature=0.1):  # Add this parameter\n",
        "        super().__init__()\n",
        "\n",
        "        # Save the temperature parameter\n",
        "        self.temperature = temperature\n",
        "\n",
        "        # 1) Set Transformer encoders (no final pooling)\n",
        "        self.strain_encoder = SetEncoder(\n",
        "            dim_input=embedding_dim,\n",
        "            dim_output=embedding_dim,\n",
        "            num_heads=num_heads,\n",
        "            num_inds=strain_inds,\n",
        "            ln=ln,\n",
        "            temperature=temperature  # Pass temperature to SetEncoder\n",
        "        )\n",
        "        self.phage_encoder = SetEncoder(\n",
        "            dim_input=embedding_dim,\n",
        "            dim_output=embedding_dim,\n",
        "            num_heads=num_heads,\n",
        "            num_inds=phage_inds,\n",
        "            ln=ln,\n",
        "            temperature=temperature  # Pass temperature to SetEncoder\n",
        "        )\n",
        "\n",
        "        # 2) Cross-attention blocks\n",
        "        self.strain_to_phage = CrossAttention(embedding_dim, num_heads, ln=ln, temperature=temperature)\n",
        "        self.phage_to_strain = CrossAttention(embedding_dim, num_heads, ln=ln, temperature=temperature)\n",
        "\n",
        "        # 3) PMA for final pooling to a single vector\n",
        "        self.strain_pma = PMA(embedding_dim, num_heads, num_seeds=1, ln=ln, temperature=temperature)\n",
        "        self.phage_pma  = PMA(embedding_dim, num_heads, num_seeds=1, ln=ln, temperature=temperature)\n",
        "\n",
        "        # 4) Final classifier\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(embedding_dim * 2, embedding_dim),\n",
        "            nn.LayerNorm(embedding_dim) if ln else nn.Identity(),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(embedding_dim, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self,\n",
        "            strain_genes,\n",
        "            phage_genes,\n",
        "            strain_mask=None,\n",
        "            phage_mask=None,\n",
        "            return_attn=False):\n",
        "        \"\"\"\n",
        "        strain_genes: [B, n_s, 1280]\n",
        "        phage_genes:  [B, n_p, 1280]\n",
        "        return_attn : if True, returns (logits, (strain_attn, phage_attn)) for interpretability\n",
        "        \"\"\"\n",
        "        # 1) Encode each genome with set encoders\n",
        "        # print(\"strain_genes shape:\", strain_genes.shape)\n",
        "        # print(\"phage_genes shape:\", phage_genes.shape)\n",
        "        strain_enc = self.strain_encoder(strain_genes, mask=strain_mask)\n",
        "        phage_enc  = self.phage_encoder(phage_genes,  mask=phage_mask)\n",
        "\n",
        "        # print(\"strain_enc shape:\", strain_enc.shape)\n",
        "        # print(\"phage_enc shape:\", phage_enc.shape)\n",
        "\n",
        "        # 2) Cross-attention\n",
        "        if return_attn:\n",
        "            strain_attended, strain_attn = self.strain_to_phage(strain_enc, phage_enc, mask=phage_mask, return_attn=True)\n",
        "            phage_attended,  phage_attn  = self.phage_to_strain(phage_enc, strain_enc, mask=strain_mask, return_attn=True)\n",
        "        else:\n",
        "            strain_attended = self.strain_to_phage(strain_enc, phage_enc, mask=phage_mask)\n",
        "            phage_attended  = self.phage_to_strain(phage_enc, strain_enc, mask=strain_mask)\n",
        "            strain_attn = None\n",
        "            phage_attn  = None\n",
        "\n",
        "        # 3) Pool each cross-attended set\n",
        "        strain_pooled = self.strain_pma(strain_attended, mask=strain_mask)\n",
        "        phage_pooled  = self.phage_pma(phage_attended,  mask=phage_mask)\n",
        "\n",
        "        strain_vec = strain_pooled.squeeze(1)  # [B, d]\n",
        "        phage_vec  = phage_pooled.squeeze(1)   # [B, d]\n",
        "\n",
        "        # 4) Classifier\n",
        "        combined = torch.cat([strain_vec, phage_vec], dim=-1)  # [B, 2*d]\n",
        "        logits   = self.classifier(combined)                   # [B, 1]\n",
        "\n",
        "        # print(\"logits shape:\", logits.shape)  # Add this line\n",
        "\n",
        "        # Return attention if requested\n",
        "        if return_attn:\n",
        "            return logits, (strain_attn, phage_attn)\n",
        "        else:\n",
        "            return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "nkthkAkutRqg",
      "metadata": {
        "id": "nkthkAkutRqg"
      },
      "source": [
        "# Block 6: Training Functions\n",
        "Description: This block contains the functions for training the model, including the training loop, validation, early stopping, and visualization of training history.\n",
        "\n",
        "## Hyperparameters and Architectural Decisions\n",
        "### `train_model` function:\n",
        "- `num_epochs`: Maximum number of training epochs.\n",
        "  - Current value: 100\n",
        "  - Possible values to try: 50, 150, 200\n",
        "- `learning_rate`: Learning rate used by the optimizer.\n",
        "  - Current value: 1e-4\n",
        "  - Possible values to try: 1e-3, 5e-4, 1e-5\n",
        "- `patience`: Number of epochs to wait for improvement before triggering early stopping.\n",
        "  - Current value: 7\n",
        "  - Possible values to try: 5, 10, 15\n",
        "- `pos_weight_val`: Weight assigned to the positive class in the loss function to handle class imbalance.\n",
        "  - Current value: Computed based on class distribution in the training data\n",
        "  - Possible values to try: Adjust the computed value by a factor of 0.8, 1.2, 1.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "bu6HhqTttRqg",
      "metadata": {
        "id": "bu6HhqTttRqg"
      },
      "outputs": [],
      "source": [
        "class EarlyStopping:\n",
        "    \"\"\"\n",
        "    Early stopping to stop training when a metric (e.g., MCC) has stopped improving.\n",
        "    If 'mode' is 'max', we look for metric going UP. If 'mode' is 'min', we look for metric going DOWN.\n",
        "    \"\"\"\n",
        "    def __init__(self, patience=7, min_delta=0.0, mode='max'):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.mode = mode  # 'max' or 'min'\n",
        "        self.best_metric = None\n",
        "        self.counter = 0\n",
        "        self.early_stop = False\n",
        "\n",
        "    def __call__(self, current_metric):\n",
        "        if self.best_metric is None:\n",
        "            self.best_metric = current_metric\n",
        "        else:\n",
        "            # For 'max' mode, improvement means current_metric >= best_metric + min_delta\n",
        "            if self.mode == 'max':\n",
        "                if current_metric < (self.best_metric + self.min_delta):\n",
        "                    self.counter += 1\n",
        "                else:\n",
        "                    self.best_metric = current_metric\n",
        "                    self.counter = 0\n",
        "            else:  # 'min' mode\n",
        "                if current_metric > (self.best_metric - self.min_delta):\n",
        "                    self.counter += 1\n",
        "                else:\n",
        "                    self.best_metric = current_metric\n",
        "                    self.counter = 0\n",
        "\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "\n",
        "def train_epoch(model, train_loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    for batch in train_loader:\n",
        "        # Unpack only the tensors we need (first 5 elements)\n",
        "        strain_emb, phage_emb, strain_mask, phage_mask, labels = batch[:5]\n",
        "\n",
        "        strain_emb  = strain_emb.to(device)  # [B, S_max, emb_dim]\n",
        "        phage_emb   = phage_emb.to(device)   # [B, P_max, emb_dim]\n",
        "        strain_mask = strain_mask.to(device) # [B, S_max]\n",
        "        phage_mask  = phage_mask.to(device)  # [B, P_max]\n",
        "        labels      = labels.to(device)      # [B, 1]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(strain_emb, phage_emb, strain_mask, phage_mask)\n",
        "        loss = criterion(logits, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        preds = torch.sigmoid(logits).reshape(-1).detach().cpu().numpy()\n",
        "        all_preds.extend(preds)\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    epoch_loss = total_loss / len(train_loader)\n",
        "    epoch_mcc = matthews_corrcoef(\n",
        "        (np.array(all_labels) > 0.5).astype(int),\n",
        "        (np.array(all_preds) > 0.5).astype(int)\n",
        "    )\n",
        "    return epoch_loss, epoch_mcc\n",
        "\n",
        "def validate(model, val_loader, criterion, device):\n",
        "    \"\"\"Validate the model on a held-out set.\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            # Unpack only the tensors we need (first 5 elements)\n",
        "            strain_emb, phage_emb, strain_mask, phage_mask, labels = batch[:5]\n",
        "\n",
        "            strain_emb  = strain_emb.to(device)\n",
        "            phage_emb   = phage_emb.to(device)\n",
        "            strain_mask = strain_mask.to(device)\n",
        "            phage_mask  = phage_mask.to(device)\n",
        "            labels      = labels.to(device)\n",
        "\n",
        "            logits = model(strain_emb, phage_emb, strain_mask, phage_mask)\n",
        "            loss   = criterion(logits, labels)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            preds = torch.sigmoid(logits).cpu().numpy()\n",
        "            all_preds.extend(preds)\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    val_loss = total_loss / len(val_loader)\n",
        "    val_mcc = matthews_corrcoef(\n",
        "        (np.array(all_labels) > 0.5).astype(int),\n",
        "        (np.array(all_preds) > 0.5).astype(int)\n",
        "    )\n",
        "    return val_loss, val_mcc\n",
        "\n",
        "def train_model(model, train_loader, val_loader,\n",
        "                num_epochs=100,\n",
        "                learning_rate=1e-4,\n",
        "                patience=7,\n",
        "                device='cuda',\n",
        "                pos_weight_val=None):\n",
        "\n",
        "    if pos_weight_val is not None:\n",
        "        criterion = nn.BCEWithLogitsLoss(\n",
        "            pos_weight=torch.tensor([pos_weight_val], device=device)\n",
        "        )\n",
        "    else:\n",
        "        criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode='max', factor=0.5, patience=5, verbose=True\n",
        "    )\n",
        "    early_stopping = EarlyStopping(patience=patience, mode='max')\n",
        "    history = {'train_loss': [], 'val_loss': [], 'train_mcc': [], 'val_mcc': []}\n",
        "\n",
        "    # Calculate total steps for tqdm\n",
        "    total_steps = len(train_loader) * num_epochs\n",
        "\n",
        "    # Create a single tqdm loop for the entire training process\n",
        "    pbar_epochs = tqdm(total=num_epochs, desc=\"Training\", position=0, leave=True)\n",
        "\n",
        "    try:\n",
        "        for epoch in range(num_epochs):\n",
        "            # Train and validate\n",
        "            train_loss, train_mcc = train_epoch(model, train_loader, optimizer, criterion, device)\n",
        "            val_loss, val_mcc = validate(model, val_loader, criterion, device)\n",
        "\n",
        "            # Update scheduler and history\n",
        "            scheduler.step(val_mcc)\n",
        "            history['train_loss'].append(train_loss)\n",
        "            history['val_loss'].append(val_loss)\n",
        "            history['train_mcc'].append(train_mcc)\n",
        "            history['val_mcc'].append(val_mcc)\n",
        "\n",
        "            # Update tqdm progress bar after each epoch\n",
        "            pbar_epochs.update(1)  # Increment by 1 for each completed epoch\n",
        "            pbar_epochs.set_postfix({\n",
        "                'Epoch': epoch + 1,\n",
        "                'train_loss': f'{train_loss:.3f}',\n",
        "                'val_loss': f'{val_loss:.3f}',\n",
        "                'train_mcc': f'{train_mcc:.3f}',\n",
        "                'val_mcc': f'{val_mcc:.3f}'\n",
        "            })\n",
        "\n",
        "            # Check early stopping\n",
        "            early_stopping(val_mcc)\n",
        "            if early_stopping.early_stop:\n",
        "                print(\"\\nEarly stopping triggered.\")\n",
        "                break\n",
        "\n",
        "    finally:\n",
        "        pbar_epochs.close()  # Close tqdm progress bar\n",
        "\n",
        "    return history\n",
        "\n",
        "def plot_training_history(history):\n",
        "    epochs = len(history['train_loss'])\n",
        "    plt.figure(figsize=(12,5))\n",
        "\n",
        "    # Loss\n",
        "    plt.subplot(1,2,1)\n",
        "    plt.plot(range(1, epochs+1), history['train_loss'], label='Train Loss')\n",
        "    plt.plot(range(1, epochs+1), history['val_loss'],   label='Val Loss')\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.title(\"Loss vs. Epochs\")\n",
        "    plt.legend()\n",
        "\n",
        "    # MCC\n",
        "    plt.subplot(1,2,2)\n",
        "    plt.plot(range(1, epochs+1), history['train_mcc'], label='Train MCC')\n",
        "    plt.plot(range(1, epochs+1), history['val_mcc'],   label='Val MCC')\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"MCC\")\n",
        "    plt.title(\"MCC vs. Epochs\")\n",
        "    plt.legend()\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "def init_attention_weights(model):\n",
        "    \"\"\"Initialize weights to encourage attention differentiation.\"\"\"\n",
        "    for name, param in model.named_parameters():\n",
        "        if 'fc_q' in name or 'fc_k' in name:\n",
        "            # Add small random noise to break symmetry\n",
        "            with torch.no_grad():\n",
        "                param.add_(torch.randn_like(param) * 0.1)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xonZjalVvTFv",
      "metadata": {
        "id": "xonZjalVvTFv"
      },
      "source": [
        "## Model Performance Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "YLe8ULcGqqsb",
      "metadata": {
        "id": "YLe8ULcGqqsb"
      },
      "outputs": [],
      "source": [
        "def predict_and_evaluate(model, data_loader, device, plot_dir=None, fold_num=None):\n",
        "    \"\"\"\n",
        "    Makes predictions and calculates performance metrics.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "            # Unpack only the tensors we need (first 5 elements)\n",
        "            strain_emb, phage_emb, strain_mask, phage_mask, labels = batch[:5]\n",
        "\n",
        "            strain_emb = strain_emb.to(device)\n",
        "            phage_emb = phage_emb.to(device)\n",
        "            strain_mask = strain_mask.to(device)\n",
        "            phage_mask = phage_mask.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            logits = model(strain_emb, phage_emb, strain_mask, phage_mask)\n",
        "            preds = torch.sigmoid(logits).cpu().numpy().flatten()\n",
        "\n",
        "            all_preds.extend(preds)\n",
        "            all_labels.extend(labels.cpu().numpy().flatten())\n",
        "\n",
        "    all_preds = np.array(all_preds)\n",
        "    all_labels = np.array(all_labels)\n",
        "\n",
        "    # Calculate metrics\n",
        "    binary_preds = (all_preds > 0.5).astype(int)\n",
        "    accuracy = accuracy_score(all_labels, binary_preds)\n",
        "    precision = precision_score(all_labels, binary_preds)\n",
        "    recall = recall_score(all_labels, binary_preds)\n",
        "    f1 = f1_score(all_labels, binary_preds)\n",
        "    mcc = matthews_corrcoef(all_labels, binary_preds)\n",
        "    roc_auc = roc_auc_score(all_labels, all_preds)\n",
        "    pr_auc = average_precision_score(all_labels, all_preds)\n",
        "    conf_matrix = confusion_matrix(all_labels, binary_preds)\n",
        "\n",
        "    # Print metrics\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n",
        "    print(f\"MCC: {mcc:.4f}\")\n",
        "    print(f\"ROC AUC: {roc_auc:.4f}\")\n",
        "    print(f\"PR AUC: {pr_auc:.4f}\")\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(conf_matrix)\n",
        "\n",
        "    # Plot confusion matrix\n",
        "    plt.figure(figsize=(6, 6))\n",
        "    sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False,\n",
        "                xticklabels=['Predicted Negative', 'Predicted Positive'],\n",
        "                yticklabels=['Actual Negative', 'Actual Positive'])\n",
        "    plt.title(\"Confusion Matrix\")\n",
        "    plt.xlabel(\"Predicted Label\")\n",
        "    plt.ylabel(\"True Label\")\n",
        "    if plot_dir and fold_num is not None:\n",
        "        plt.savefig(os.path.join(plot_dir, f\"fold_{fold_num}_confusion_matrix.png\"),\n",
        "                    dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "    else:\n",
        "        plt.show()\n",
        "\n",
        "    # Plot ROC curve\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    fpr, tpr, _ = roc_curve(all_labels, all_preds)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.grid(alpha=0.3)\n",
        "    if plot_dir and fold_num is not None:\n",
        "        plt.savefig(os.path.join(plot_dir, f\"fold_{fold_num}_roc_curve.png\"),\n",
        "                    dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "    else:\n",
        "        plt.show()\n",
        "\n",
        "    # Plot precision-recall curve\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    precision_curve, recall_curve, _ = precision_recall_curve(all_labels, all_preds)\n",
        "    pr_auc = auc(recall_curve, precision_curve)\n",
        "    plt.plot(recall_curve, precision_curve, color='darkorange', lw=2,\n",
        "             label=f'PR curve (area = {pr_auc:.2f})')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('Recall')\n",
        "    plt.ylabel('Precision')\n",
        "    plt.title('Precision-Recall Curve')\n",
        "    plt.legend(loc=\"lower left\")\n",
        "    plt.grid(alpha=0.3)\n",
        "    if plot_dir and fold_num is not None:\n",
        "        plt.savefig(os.path.join(plot_dir, f\"fold_{fold_num}_pr_curve.png\"),\n",
        "                    dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "    else:\n",
        "        plt.show()\n",
        "\n",
        "    # Return metrics as dictionary\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1,\n",
        "        'mcc': mcc,\n",
        "        'roc_auc': roc_auc,\n",
        "        'pr_auc': pr_auc,\n",
        "        'conf_matrix': conf_matrix,\n",
        "        'all_preds': all_preds,\n",
        "        'all_labels': all_labels\n",
        "    }\n",
        "\n",
        "def plot_confusion_matrix(conf_matrix):\n",
        "    \"\"\"Plots the confusion matrix using seaborn.\"\"\"\n",
        "    plt.figure(figsize=(6, 6))\n",
        "    sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False,\n",
        "                xticklabels=['Predicted Negative', 'Predicted Positive'],\n",
        "                yticklabels=['Actual Negative', 'Actual Positive'])\n",
        "    plt.title(\"Confusion Matrix\")\n",
        "    plt.xlabel(\"Predicted Label\")\n",
        "    plt.ylabel(\"True Label\")\n",
        "    plt.show()\n",
        "\n",
        "def plot_roc_curve(all_labels, all_preds):\n",
        "    \"\"\"Plots the ROC curve.\"\"\"\n",
        "    fpr, tpr, _ = roc_curve(all_labels, all_preds)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.show()\n",
        "\n",
        "def plot_precision_recall_curve(all_labels, all_preds):\n",
        "    \"\"\"Plots the precision-recall curve.\"\"\"\n",
        "    precision, recall, _ = precision_recall_curve(all_labels, all_preds)\n",
        "    pr_auc = auc(recall, precision)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(recall, precision, color='darkorange', lw=2, label=f'PR curve (area = {pr_auc:.2f})')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('Recall')\n",
        "    plt.ylabel('Precision')\n",
        "    plt.title('Precision-Recall Curve')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.show()\n",
        "\n",
        "def get_attention_scores(model, strain_emb, phage_emb, device):\n",
        "    \"\"\"Gets attention scores from the model.\"\"\"\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        strain_emb = strain_emb.to(device)\n",
        "        phage_emb = phage_emb.to(device)\n",
        "        _, (strain_attn, phage_attn) = model(strain_emb, phage_emb, return_attn=True)\n",
        "    return strain_attn, phage_attn # these are still on device, need to .cpu() if needed\n",
        "\n",
        "# accuracy, precision, recall, f1, mcc, roc_auc, pr_auc, conf_matrix = predict_and_evaluate(model, test_loader, device)\n",
        "# plot_confusion_matrix(conf_matrix)\n",
        "# plot_roc_curve(all_labels, all_preds)  # Assuming you saved all_labels and all_preds in predict_and_evaluate\n",
        "# plot_precision_recall_curve(all_labels, all_preds)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39aQPVY0tRqg",
      "metadata": {
        "id": "39aQPVY0tRqg"
      },
      "source": [
        "## Main Training Script\n",
        "\n",
        "Description: This block contains a function for the full workflow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "HDRboY6CtRqg",
      "metadata": {
        "id": "HDRboY6CtRqg"
      },
      "outputs": [],
      "source": [
        "def main(debug=False):\n",
        "    # 1. Set paths for data\n",
        "    interactions_path = '/content/drive/MyDrive/Arkin/phage_public_datasets/e_coli/interaction_matrix.csv'\n",
        "\n",
        "    # If in debug mode, limit the number of genomes to load\n",
        "    if debug:\n",
        "        print(\"Running in debug mode with reduced dataset...\")\n",
        "        # First, read just the interaction file to get a subset of genomes\n",
        "        interactions_df = pd.read_csv(interactions_path)\n",
        "\n",
        "        # Sample a small number of strains and phages\n",
        "        random_strains = random.sample(list(interactions_df['strain'].unique()),\n",
        "                                     min(10, len(interactions_df['strain'].unique())))\n",
        "        random_phages = random.sample(list(interactions_df['phage'].unique()),\n",
        "                                    min(10, len(interactions_df['phage'].unique())))\n",
        "\n",
        "        # Filter interactions to only include sampled strains and phages\n",
        "        debug_interactions = interactions_df[\n",
        "            interactions_df['strain'].isin(random_strains) &\n",
        "            interactions_df['phage'].isin(random_phages)\n",
        "        ]\n",
        "\n",
        "        # Save temporary CSV with only debug genomes\n",
        "        debug_path = '/tmp/debug_interactions.csv'\n",
        "        debug_interactions.to_csv(debug_path, index=False)\n",
        "\n",
        "        # Now load only these genomes' embeddings\n",
        "        strain_embeddings = load_embeddings_with_ids(\n",
        "            '/content/drive/MyDrive/Arkin/ESM2/public_datasets/ecoli/strain/',\n",
        "            genome_ids_csv=debug_path,\n",
        "            id_column='strain'\n",
        "        )\n",
        "        phage_embeddings = load_embeddings_with_ids(\n",
        "            '/content/drive/MyDrive/Arkin/ESM2/public_datasets/ecoli/phage/',\n",
        "            genome_ids_csv=debug_path,\n",
        "            id_column='phage'\n",
        "        )\n",
        "\n",
        "        # Use the debug interactions\n",
        "        interactions_df = debug_interactions\n",
        "    else:\n",
        "        # Load all embeddings\n",
        "        strain_embeddings = load_embeddings_with_ids(\n",
        "            '/content/drive/MyDrive/Arkin/ESM2/public_datasets/ecoli/strain/',\n",
        "            genome_ids_csv=interactions_path,\n",
        "            id_column='strain'\n",
        "        )\n",
        "        phage_embeddings = load_embeddings_with_ids(\n",
        "            '/content/drive/MyDrive/Arkin/ESM2/public_datasets/ecoli/phage/',\n",
        "            genome_ids_csv=interactions_path,\n",
        "            id_column='phage'\n",
        "        )\n",
        "\n",
        "        # Load interactions\n",
        "        interactions_df = pd.read_csv(interactions_path)\n",
        "\n",
        "    # Filter interactions_df to ensure we have embeddings for all strains/phages\n",
        "    strain_keys = set(strain_embeddings.keys())\n",
        "    phage_keys = set(phage_embeddings.keys())\n",
        "    interactions_df = interactions_df[interactions_df['strain'].isin(strain_keys) &\n",
        "                                    interactions_df['phage'].isin(phage_keys)]\n",
        "\n",
        "    # 2. Split interactions\n",
        "    train_df, test_df = filter_interactions_by_strain(interactions_df)\n",
        "\n",
        "    # 2a. Compute pos_weight from train_df\n",
        "    num_pos = (train_df['interaction'] == 1).sum()\n",
        "    num_neg = (train_df['interaction'] == 0).sum()\n",
        "    pos_weight_val = num_neg / max(num_pos, 1) * 2.0 # avoid divide-by-zero\n",
        "\n",
        "    print(f\"pos_weight_val = {pos_weight_val:.3f}\")\n",
        "\n",
        "    # 3. Create data loaders\n",
        "    train_loader, test_loader = create_data_loaders(\n",
        "        train_df,\n",
        "        test_df,\n",
        "        strain_embeddings,\n",
        "        phage_embeddings,\n",
        "        batch_size=24 if not debug else 4  # Use smaller batch size for debug\n",
        "    )\n",
        "\n",
        "    # 4. Initialize model\n",
        "    model = StrainPhageTransformer(\n",
        "        embedding_dim=1280,\n",
        "        num_heads=8,\n",
        "        strain_inds=256 if not debug else 32,  # Smaller for debug\n",
        "        phage_inds=64 if not debug else 16,    # Smaller for debug\n",
        "        dropout=0.1,\n",
        "        ln=True,\n",
        "        temperature=0.05\n",
        "    ).to(device)\n",
        "\n",
        "    # Initialize with perturbations to break symmetry\n",
        "    model = init_attention_weights(model)\n",
        "\n",
        "    # 5. Train model, passing pos_weight_val\n",
        "    history = train_model(\n",
        "        model,\n",
        "        train_loader,\n",
        "        test_loader,\n",
        "        num_epochs=100 if not debug else 5,  # Fewer epochs for debug\n",
        "        learning_rate=5e-5,\n",
        "        patience=7 if not debug else 2,      # Lower patience for debug\n",
        "        device=device,\n",
        "        pos_weight_val=pos_weight_val\n",
        "    )\n",
        "\n",
        "    # 6. Save model and results\n",
        "    torch.save({\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'history': history,\n",
        "    }, 'strain_phage_transformer.pt')\n",
        "\n",
        "    # 7. Plot training history\n",
        "    plot_training_history(history)\n",
        "\n",
        "    # 6. Prediction and Evaluation within main()\n",
        "    accuracy, precision, recall, f1, mcc, roc_auc, pr_auc, conf_matrix, all_preds, all_labels = predict_and_evaluate(model, test_loader, device)\n",
        "\n",
        "    # 7. Plotting within main() (optional, but convenient)\n",
        "    plot_confusion_matrix(conf_matrix)\n",
        "    plot_roc_curve(all_labels, all_preds)\n",
        "    plot_precision_recall_curve(all_labels, all_preds)\n",
        "\n",
        "    # Return the desired values\n",
        "    return model, history, accuracy, precision, recall, f1, mcc, roc_auc, pr_auc, conf_matrix, all_preds, all_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attention Analysis"
      ],
      "metadata": {
        "id": "-WWflrmJxAYW"
      },
      "id": "-WWflrmJxAYW"
    },
    {
      "cell_type": "code",
      "source": [
        "def export_attention_data(\n",
        "    model,\n",
        "    test_loader,\n",
        "    test_df,\n",
        "    strain_embeddings,\n",
        "    phage_embeddings,\n",
        "    output_dir,\n",
        "    num_samples=50,  # Increase this for more comprehensive analysis\n",
        "    device='cuda'\n",
        "):\n",
        "    \"\"\"\n",
        "    Export attention data to CSV files for analysis.\n",
        "\n",
        "    Args:\n",
        "        model: The trained StrainPhageTransformer model\n",
        "        test_loader: DataLoader for test data\n",
        "        test_df: DataFrame with test interactions\n",
        "        strain_embeddings: Dictionary of strain embeddings\n",
        "        phage_embeddings: Dictionary of phage embeddings\n",
        "        output_dir: Directory to save output files\n",
        "        num_samples: Number of samples to analyze\n",
        "        device: Device to run model on ('cuda' or 'cpu')\n",
        "    \"\"\"\n",
        "    # Create output directory if it doesn't exist\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Initialize storage for results\n",
        "    results = []\n",
        "    strain_phage_attention_data = []\n",
        "    phage_strain_attention_data = []\n",
        "    important_genes_data = []\n",
        "\n",
        "    # Put model in evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (strain_emb, phage_emb, strain_mask, phage_mask, label, strain_id, phage_id) in enumerate(test_loader):\n",
        "            if i >= num_samples:\n",
        "                break\n",
        "\n",
        "            # Get metadata for current sample\n",
        "            sample_strain = test_df.iloc[i]['strain']\n",
        "            sample_phage = test_df.iloc[i]['phage']\n",
        "            interaction = label.item()\n",
        "\n",
        "            # Move tensors to device\n",
        "            strain_emb = strain_emb.to(device)\n",
        "            phage_emb = phage_emb.to(device)\n",
        "            strain_mask = strain_mask.to(device) if strain_mask is not None else None\n",
        "            phage_mask = phage_mask.to(device) if phage_mask is not None else None\n",
        "\n",
        "            # Get model prediction\n",
        "            logits = model(strain_emb, phage_emb, strain_mask, phage_mask)\n",
        "            pred_prob = torch.sigmoid(logits).item()\n",
        "            pred_label = 1 if pred_prob > 0.5 else 0\n",
        "\n",
        "            # Get attention maps\n",
        "            _, (strain_to_phage_attn, phage_to_strain_attn) = model(\n",
        "                strain_emb,\n",
        "                phage_emb,\n",
        "                strain_mask,\n",
        "                phage_mask,\n",
        "                return_attn=True\n",
        "            )\n",
        "\n",
        "            # Convert attention maps to numpy\n",
        "            strain_to_phage_attn = strain_to_phage_attn.cpu().numpy().squeeze(0)  # [num_heads, n_strain, n_phage]\n",
        "            phage_to_strain_attn = phage_to_strain_attn.cpu().numpy().squeeze(0)  # [num_heads, n_phage, n_strain]\n",
        "\n",
        "            # Create gene names/ids\n",
        "            n_strain_genes = len(strain_embeddings[sample_strain])\n",
        "            n_phage_genes = len(phage_embeddings[sample_phage])\n",
        "\n",
        "            # Get the gene IDs for this sample\n",
        "            strain_gene_ids = strain_embeddings[sample_strain][1]  # Get the ID list (idx 1)\n",
        "            phage_gene_ids = phage_embeddings[sample_phage][1]     # Get the ID list (idx 1)\n",
        "\n",
        "            # Use the actual gene IDs instead of generic names\n",
        "            strain_genes = strain_gene_ids\n",
        "            phage_genes = phage_gene_ids\n",
        "\n",
        "            # Add sample results\n",
        "            results.append({\n",
        "                'sample_id': i,\n",
        "                'strain': sample_strain,\n",
        "                'phage': sample_phage,\n",
        "                'true_label': interaction,\n",
        "                'pred_prob': pred_prob,\n",
        "                'pred_label': pred_label,\n",
        "                'n_strain_genes': n_strain_genes,\n",
        "                'n_phage_genes': n_phage_genes\n",
        "            })\n",
        "\n",
        "            # Calculate mean attention scores across heads\n",
        "            mean_strain_to_phage = strain_to_phage_attn.mean(axis=0)  # [n_strain, n_phage]\n",
        "            mean_phage_to_strain = phage_to_strain_attn.mean(axis=0)  # [n_phage, n_strain]\n",
        "\n",
        "            # Process strain-to-phage attention\n",
        "            for s_idx, strain_gene in enumerate(strain_genes):\n",
        "                for p_idx, phage_gene in enumerate(phage_genes):\n",
        "                    # For each attention head\n",
        "                    for head in range(strain_to_phage_attn.shape[0]):\n",
        "                        strain_phage_attention_data.append({\n",
        "                            'sample_id': i,\n",
        "                            'strain': sample_strain,\n",
        "                            'phage': sample_phage,\n",
        "                            'strain_gene': strain_gene,\n",
        "                            'phage_gene': phage_gene,\n",
        "                            'attention_head': head,\n",
        "                            'attention_score': strain_to_phage_attn[head, s_idx, p_idx],\n",
        "                            'true_label': interaction,\n",
        "                            'pred_label': pred_label\n",
        "                        })\n",
        "\n",
        "                    # Also add mean attention across heads\n",
        "                    strain_phage_attention_data.append({\n",
        "                        'sample_id': i,\n",
        "                        'strain': sample_strain,\n",
        "                        'phage': sample_phage,\n",
        "                        'strain_gene': strain_gene,\n",
        "                        'phage_gene': phage_gene,\n",
        "                        'attention_head': 'mean',\n",
        "                        'attention_score': mean_strain_to_phage[s_idx, p_idx],\n",
        "                        'true_label': interaction,\n",
        "                        'pred_label': pred_label\n",
        "                    })\n",
        "\n",
        "            # Process phage-to-strain attention\n",
        "            for p_idx, phage_gene in enumerate(phage_genes):\n",
        "                for s_idx, strain_gene in enumerate(strain_genes):\n",
        "                    # For each attention head\n",
        "                    for head in range(phage_to_strain_attn.shape[0]):\n",
        "                        phage_strain_attention_data.append({\n",
        "                            'sample_id': i,\n",
        "                            'strain': sample_strain,\n",
        "                            'phage': sample_phage,\n",
        "                            'phage_gene': phage_gene,\n",
        "                            'strain_gene': strain_gene,\n",
        "                            'attention_head': head,\n",
        "                            'attention_score': phage_to_strain_attn[head, p_idx, s_idx],\n",
        "                            'true_label': interaction,\n",
        "                            'pred_label': pred_label\n",
        "                        })\n",
        "\n",
        "                    # Also add mean attention across heads\n",
        "                    phage_strain_attention_data.append({\n",
        "                        'sample_id': i,\n",
        "                        'strain': sample_strain,\n",
        "                        'phage': sample_phage,\n",
        "                        'phage_gene': phage_gene,\n",
        "                        'strain_gene': strain_gene,\n",
        "                        'attention_head': 'mean',\n",
        "                        'attention_score': mean_phage_to_strain[p_idx, s_idx],\n",
        "                        'true_label': interaction,\n",
        "                        'pred_label': pred_label\n",
        "                    })\n",
        "\n",
        "            # Identify important genes based on attention scores\n",
        "\n",
        "            # 1. Important phage genes (from strain-to-phage attention)\n",
        "            # Sum attention across all strain genes for each phage gene\n",
        "            phage_importance = mean_strain_to_phage.sum(axis=0)  # [n_phage]\n",
        "            phage_ranks = np.argsort(phage_importance)[::-1]  # Descending order\n",
        "\n",
        "            for rank, p_idx in enumerate(phage_ranks):\n",
        "                important_genes_data.append({\n",
        "                    'sample_id': i,\n",
        "                    'strain': sample_strain,\n",
        "                    'phage': sample_phage,\n",
        "                    'gene_type': 'phage',\n",
        "                    'gene_id': phage_genes[p_idx],\n",
        "                    'attention_sum': phage_importance[p_idx],\n",
        "                    'rank': rank + 1,\n",
        "                    'percentile': 100 * (1 - rank / len(phage_genes)),\n",
        "                    'direction': 'strain_to_phage',\n",
        "                    'true_label': interaction,\n",
        "                    'pred_label': pred_label\n",
        "                })\n",
        "\n",
        "            # 2. Important strain genes (from phage-to-strain attention)\n",
        "            # Sum attention across all phage genes for each strain gene\n",
        "            strain_importance = mean_phage_to_strain.sum(axis=0)  # [n_strain]\n",
        "            strain_ranks = np.argsort(strain_importance)[::-1]  # Descending order\n",
        "\n",
        "            for rank, s_idx in enumerate(strain_ranks):\n",
        "                important_genes_data.append({\n",
        "                    'sample_id': i,\n",
        "                    'strain': sample_strain,\n",
        "                    'phage': sample_phage,\n",
        "                    'gene_type': 'strain',\n",
        "                    'gene_id': strain_genes[s_idx],\n",
        "                    'attention_sum': strain_importance[s_idx],\n",
        "                    'rank': rank + 1,\n",
        "                    'percentile': 100 * (1 - rank / len(strain_genes)),\n",
        "                    'direction': 'phage_to_strain',\n",
        "                    'true_label': interaction,\n",
        "                    'pred_label': pred_label\n",
        "                })\n",
        "\n",
        "    # Convert results to DataFrames\n",
        "    results_df = pd.DataFrame(results)\n",
        "    strain_phage_attn_df = pd.DataFrame(strain_phage_attention_data)\n",
        "    phage_strain_attn_df = pd.DataFrame(phage_strain_attention_data)\n",
        "    important_genes_df = pd.DataFrame(important_genes_data)\n",
        "\n",
        "    # Save to CSV\n",
        "    results_df.to_csv(os.path.join(output_dir, 'sample_results.csv'), index=False)\n",
        "    strain_phage_attn_df.to_csv(os.path.join(output_dir, 'strain_to_phage_attention.csv'), index=False)\n",
        "    phage_strain_attn_df.to_csv(os.path.join(output_dir, 'phage_to_strain_attention.csv'), index=False)\n",
        "    important_genes_df.to_csv(os.path.join(output_dir, 'important_genes.csv'), index=False)\n",
        "\n",
        "    # Create summary DataFrames for important genes\n",
        "\n",
        "    # Top phage genes across all samples\n",
        "    top_phage_genes = important_genes_df[important_genes_df['gene_type'] == 'phage'].copy()\n",
        "    top_phage_summary = top_phage_genes.groupby('gene_id')['attention_sum'].mean().reset_index()\n",
        "    top_phage_summary = top_phage_summary.sort_values('attention_sum', ascending=False)\n",
        "    top_phage_summary['global_rank'] = np.arange(1, len(top_phage_summary) + 1)\n",
        "\n",
        "    # Top strain genes across all samples\n",
        "    top_strain_genes = important_genes_df[important_genes_df['gene_type'] == 'strain'].copy()\n",
        "    top_strain_summary = top_strain_genes.groupby('gene_id')['attention_sum'].mean().reset_index()\n",
        "    top_strain_summary = top_strain_summary.sort_values('attention_sum', ascending=False)\n",
        "    top_strain_summary['global_rank'] = np.arange(1, len(top_strain_summary) + 1)\n",
        "\n",
        "    # Save summaries\n",
        "    top_phage_summary.to_csv(os.path.join(output_dir, 'top_phage_genes_summary.csv'), index=False)\n",
        "    top_strain_summary.to_csv(os.path.join(output_dir, 'top_strain_genes_summary.csv'), index=False)\n",
        "\n",
        "    return {\n",
        "        'results_df': results_df,\n",
        "        'strain_phage_attn_df': strain_phage_attn_df,\n",
        "        'phage_strain_attn_df': phage_strain_attn_df,\n",
        "        'important_genes_df': important_genes_df,\n",
        "        'top_phage_summary': top_phage_summary,\n",
        "        'top_strain_summary': top_strain_summary\n",
        "    }\n",
        "\n",
        "# Additional function to analyze gene patterns across interaction outcomes\n",
        "def analyze_attention_patterns(important_genes_df, output_dir):\n",
        "    \"\"\"\n",
        "    Analyze patterns in gene attention across different interaction outcomes.\n",
        "\n",
        "    Args:\n",
        "        important_genes_df: DataFrame with important genes data\n",
        "        output_dir: Directory to save output files\n",
        "    \"\"\"\n",
        "    # Create separate dataframes for true positives, true negatives, false positives, and false negatives\n",
        "    tp_genes = important_genes_df[(important_genes_df['true_label'] == 1) &\n",
        "                                  (important_genes_df['pred_label'] == 1)]\n",
        "    tn_genes = important_genes_df[(important_genes_df['true_label'] == 0) &\n",
        "                                  (important_genes_df['pred_label'] == 0)]\n",
        "    fp_genes = important_genes_df[(important_genes_df['true_label'] == 0) &\n",
        "                                  (important_genes_df['pred_label'] == 1)]\n",
        "    fn_genes = important_genes_df[(important_genes_df['true_label'] == 1) &\n",
        "                                  (important_genes_df['pred_label'] == 0)]\n",
        "\n",
        "    # Function to get top genes by outcome\n",
        "    def get_top_genes(df, gene_type, n=20):\n",
        "        if df.empty:\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        subset = df[df['gene_type'] == gene_type]\n",
        "        summary = subset.groupby('gene_id')['attention_sum'].mean().reset_index()\n",
        "        summary = summary.sort_values('attention_sum', ascending=False).head(n)\n",
        "        summary['rank'] = np.arange(1, len(summary) + 1)\n",
        "        return summary\n",
        "\n",
        "    # Get top genes for each outcome category\n",
        "    tp_phage_top = get_top_genes(tp_genes, 'phage')\n",
        "    tp_strain_top = get_top_genes(tp_genes, 'strain')\n",
        "    tn_phage_top = get_top_genes(tn_genes, 'phage')\n",
        "    tn_strain_top = get_top_genes(tn_genes, 'strain')\n",
        "    fp_phage_top = get_top_genes(fp_genes, 'phage')\n",
        "    fp_strain_top = get_top_genes(fp_genes, 'strain')\n",
        "    fn_phage_top = get_top_genes(fn_genes, 'phage')\n",
        "    fn_strain_top = get_top_genes(fn_genes, 'strain')\n",
        "\n",
        "    # Save to CSV\n",
        "    if not tp_phage_top.empty:\n",
        "        tp_phage_top.to_csv(os.path.join(output_dir, 'tp_phage_top.csv'), index=False)\n",
        "    if not tp_strain_top.empty:\n",
        "        tp_strain_top.to_csv(os.path.join(output_dir, 'tp_strain_top.csv'), index=False)\n",
        "    if not tn_phage_top.empty:\n",
        "        tn_phage_top.to_csv(os.path.join(output_dir, 'tn_phage_top.csv'), index=False)\n",
        "    if not tn_strain_top.empty:\n",
        "        tn_strain_top.to_csv(os.path.join(output_dir, 'tn_strain_top.csv'), index=False)\n",
        "    if not fp_phage_top.empty:\n",
        "        fp_phage_top.to_csv(os.path.join(output_dir, 'fp_phage_top.csv'), index=False)\n",
        "    if not fp_strain_top.empty:\n",
        "        fp_strain_top.to_csv(os.path.join(output_dir, 'fp_strain_top.csv'), index=False)\n",
        "    if not fn_phage_top.empty:\n",
        "        fn_phage_top.to_csv(os.path.join(output_dir, 'fn_phage_top.csv'), index=False)\n",
        "    if not fn_strain_top.empty:\n",
        "        fn_strain_top.to_csv(os.path.join(output_dir, 'fn_strain_top.csv'), index=False)\n",
        "\n",
        "    # Create a comparison of gene attention patterns between correct and incorrect predictions\n",
        "    if not (tp_genes.empty or fp_genes.empty):\n",
        "        phage_correct_vs_incorrect = pd.merge(\n",
        "            tp_phage_top.rename(columns={'attention_sum': 'positive_correct_attention'})[['gene_id', 'positive_correct_attention']],\n",
        "            fp_phage_top.rename(columns={'attention_sum': 'positive_incorrect_attention'})[['gene_id', 'positive_incorrect_attention']],\n",
        "            on='gene_id',\n",
        "            how='outer'\n",
        "        ).fillna(0)\n",
        "\n",
        "        if not phage_correct_vs_incorrect.empty:\n",
        "            phage_correct_vs_incorrect['attention_difference'] = (\n",
        "                phage_correct_vs_incorrect['positive_correct_attention'] -\n",
        "                phage_correct_vs_incorrect['positive_incorrect_attention']\n",
        "            )\n",
        "            phage_correct_vs_incorrect = phage_correct_vs_incorrect.sort_values('attention_difference', ascending=False)\n",
        "            phage_correct_vs_incorrect.to_csv(os.path.join(output_dir, 'phage_correct_vs_incorrect.csv'), index=False)\n",
        "\n",
        "    if not (tp_genes.empty or fp_genes.empty):\n",
        "        strain_correct_vs_incorrect = pd.merge(\n",
        "            tp_strain_top.rename(columns={'attention_sum': 'positive_correct_attention'})[['gene_id', 'positive_correct_attention']],\n",
        "            fp_strain_top.rename(columns={'attention_sum': 'positive_incorrect_attention'})[['gene_id', 'positive_incorrect_attention']],\n",
        "            on='gene_id',\n",
        "            how='outer'\n",
        "        ).fillna(0)\n",
        "\n",
        "        if not strain_correct_vs_incorrect.empty:\n",
        "            strain_correct_vs_incorrect['attention_difference'] = (\n",
        "                strain_correct_vs_incorrect['positive_correct_attention'] -\n",
        "                strain_correct_vs_incorrect['positive_incorrect_attention']\n",
        "            )\n",
        "            strain_correct_vs_incorrect = strain_correct_vs_incorrect.sort_values('attention_difference', ascending=False)\n",
        "            strain_correct_vs_incorrect.to_csv(os.path.join(output_dir, 'strain_correct_vs_incorrect.csv'), index=False)\n",
        "\n",
        "    return {\n",
        "        'tp_phage_top': tp_phage_top,\n",
        "        'tp_strain_top': tp_strain_top,\n",
        "        'tn_phage_top': tn_phage_top,\n",
        "        'tn_strain_top': tn_strain_top,\n",
        "        'fp_phage_top': fp_phage_top,\n",
        "        'fp_strain_top': fp_strain_top,\n",
        "        'fn_phage_top': fn_phage_top,\n",
        "        'fn_strain_top': fn_strain_top\n",
        "    }"
      ],
      "metadata": {
        "id": "A4dR5RbqxDTP"
      },
      "id": "A4dR5RbqxDTP",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    model, history, accuracy, precision, recall, f1, mcc, roc_auc, pr_auc, conf_matrix, all_preds, all_labels = main(debug=False)\n",
        "\n",
        "    attention_analysis = False\n",
        "\n",
        "    # Create a dictionary to store the outputs\n",
        "    outputs = {\n",
        "         'model': model,\n",
        "         'history': history,\n",
        "         'accuracy': accuracy,\n",
        "         'precision': precision,\n",
        "         'recall': recall,\n",
        "         'f1': f1,\n",
        "         'mcc': mcc,\n",
        "         'roc_auc': roc_auc,\n",
        "         'pr_auc': pr_auc,\n",
        "         'conf_matrix': conf_matrix,\n",
        "         'all_preds': all_preds,\n",
        "         'all_labels': all_labels\n",
        "    }\n",
        "\n",
        "    # Get the directory of the interaction matrix\n",
        "    output_dir = '/content/drive/MyDrive/Arkin/set_transformer_data/outputs'\n",
        "\n",
        "    # Create a subdirectory for outputs if it doesn't exist\n",
        "    output_subdir = os.path.join(output_dir, 'outputs')\n",
        "    os.makedirs(output_subdir, exist_ok=True)\n",
        "\n",
        "    # Construct the full file path for the output file\n",
        "    output_file_path = os.path.join(output_subdir, 'main_function_outputs_pseudomonas_attention.pkl')  # or .joblib\n",
        "\n",
        "    # Save the outputs\n",
        "    with open(output_file_path, 'wb') as f:  # or 'wb' for joblib\n",
        "        pickle.dump(outputs, f)  # or joblib.dump(outputs, f)\n",
        "\n",
        "    if attention_analysis:\n",
        "        print(\"Starting attention analysis...\")\n",
        "\n",
        "        # Need to make sure we also have access to these variables for attention analysis\n",
        "        # strain_embeddings = load_embeddings('/content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/strains')\n",
        "        # phage_embeddings  = load_embeddings('/content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/phages')\n",
        "        strain_embeddings = load_embeddings_with_ids('/content/drive/MyDrive/Arkin/ESM2/public_datasets/pseudomonas/strain_mod/')\n",
        "        phage_embeddings  = load_embeddings_with_ids('/content/drive/MyDrive/Arkin/ESM2/public_datasets/pseudomonas/phage_mod/')\n",
        "\n",
        "        # 2. Load and split interactions\n",
        "        interactions_df = pd.read_csv('/content/drive/MyDrive/Arkin/phage_public_datasets/pseudomonas/interaction_matrix.csv')\n",
        "        # interactions_df = pd.read_csv('/content/drive/MyDrive/Arkin/set_transformer_data/EDGE_interaction_long_172_no2.csv')\n",
        "\n",
        "        # Filter for available embeddings\n",
        "        strain_keys = set(strain_embeddings.keys())\n",
        "        phage_keys = set(phage_embeddings.keys())\n",
        "        interactions_df = interactions_df[interactions_df['strain'].isin(strain_keys) &\n",
        "                                        interactions_df['phage'].isin(phage_keys)]\n",
        "\n",
        "        # Split data to get test_df\n",
        "        _, test_df = filter_interactions_by_strain(interactions_df)\n",
        "\n",
        "        # Create test_loader for attention analysis\n",
        "        _, test_loader = create_data_loaders(\n",
        "            None,\n",
        "            test_df,\n",
        "            strain_embeddings,\n",
        "            phage_embeddings,\n",
        "            batch_size=1  # Note: batch_size must be 1 for attention analysis\n",
        "        )\n",
        "\n",
        "        # Set output directory for attention analysis files\n",
        "        attention_output_dir = os.path.join(output_dir, 'attention_analysis_pseudo')\n",
        "        os.makedirs(attention_output_dir, exist_ok=True)\n",
        "\n",
        "        print(\"Starting attention analysis...\")\n",
        "\n",
        "        # Run the attention analysis\n",
        "        attention_results = export_attention_data(\n",
        "            model=model,\n",
        "            test_loader=test_loader,\n",
        "            test_df=test_df,\n",
        "            strain_embeddings=strain_embeddings,\n",
        "            phage_embeddings=phage_embeddings,\n",
        "            output_dir=attention_output_dir,\n",
        "            num_samples=100,  # Adjust as needed\n",
        "            device=device\n",
        "        )\n",
        "\n",
        "        print(\"Completed basic attention analysis. Starting pattern analysis...\")\n",
        "\n",
        "        # Run additional pattern analysis\n",
        "        pattern_results = analyze_attention_patterns(\n",
        "            important_genes_df=attention_results['important_genes_df'],\n",
        "            output_dir=attention_output_dir\n",
        "        )\n",
        "\n",
        "        print(f\"Attention analysis complete. Results saved to {attention_output_dir}\")\n",
        "\n",
        "        # Save attention results\n",
        "        with open(os.path.join(attention_output_dir, 'attention_analysis_results_pseudo.pkl'), 'wb') as f:\n",
        "            pickle.dump({\n",
        "                'attention_results': attention_results,\n",
        "                'pattern_results': pattern_results\n",
        "            }, f)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dVFW6RTDxjyU",
        "outputId": "359a3922-bea6-49df-c667-b2921d6ef7df"
      },
      "id": "dVFW6RTDxjyU",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 402 genome IDs from /content/drive/MyDrive/Arkin/phage_public_datasets/e_coli/interaction_matrix.csv\n",
            "Loading: 536\n",
            "Loading: ECOR26\n",
            "Loading: ECOR47\n",
            "Loading: ECOR32\n",
            "Loading: ECOR34\n",
            "Loading: ECOR27\n",
            "Loading: ECOR19\n",
            "Loading: ECOR14\n",
            "Loading: ECOR15\n",
            "Loading: ECOR28\n",
            "Loading: ECOR59\n",
            "Loading: BL21\n",
            "Loading: ECOR65\n",
            "Loading: ECOR22\n",
            "Loading: ECOR16\n",
            "Loading: ECOR66\n",
            "Loading: ECOR25\n",
            "Loading: ECOR13\n",
            "Loading: ECOR17\n",
            "Loading: ECOR71\n",
            "Loading: ECOR33\n",
            "Loading: ECOR20\n",
            "Loading: ECOR29\n",
            "Loading: ECOR72\n",
            "Loading: ECOR10\n",
            "Loading: ECOR45\n",
            "Loading: ECOR61\n",
            "Loading: ECOR67\n",
            "Loading: ECOR56\n",
            "Loading: ECOR69\n",
            "Loading: ECOR58\n",
            "Loading: ECOR30\n",
            "Loading: ECOR21\n",
            "Loading: ECOR18\n",
            "Loading: H48\n",
            "Loading: NILS08\n",
            "Loading: NILS53\n",
            "Loading: 55989\n",
            "Loading: H14\n",
            "Loading: NILS06\n",
            "Loading: NILS35\n",
            "Loading: NILS24\n",
            "Loading: NILS17\n",
            "Loading: NILS41\n",
            "Loading: NILS10\n",
            "Loading: NILS04\n",
            "Loading: NILS22\n",
            "Loading: NILS02\n",
            "Loading: NILS43\n",
            "Loading: 381A\n",
            "Loading: LF82\n",
            "Loading: T145\n",
            "Loading: NILS14\n",
            "Loading: 370D\n",
            "Loading: NILS45\n",
            "Loading: ECOR23\n",
            "Loading: ECOR42\n",
            "Loading: ECOR53\n",
            "Loading: H4\n",
            "Loading: NILS20\n",
            "Loading: NILS34\n",
            "Loading: NILS58\n",
            "Loading: NILS38\n",
            "Loading: NILS27\n",
            "Loading: ECOR57\n",
            "Loading: NILS13\n",
            "Loading: NILS59\n",
            "Loading: NILS50\n",
            "Loading: LM33\n",
            "Loading: NILS32\n",
            "Loading: NILS52\n",
            "Loading: NILS65\n",
            "Loading: B6A1\n",
            "Loading: NILS67\n",
            "Loading: NILS05\n",
            "Loading: ECOR37\n",
            "Loading: NILS75\n",
            "Loading: ROAR274\n",
            "Loading: NILS40\n",
            "Loading: MT1B1\n",
            "Loading: ECOR39\n",
            "Loading: ECOR63\n",
            "Loading: NILS57\n",
            "Loading: NILS42\n",
            "Loading: NILS62\n",
            "Loading: NILS21\n",
            "Loading: ROAR400\n",
            "Loading: NILS76\n",
            "Loading: NILS28\n",
            "Loading: NILS47\n",
            "Loading: NILS33\n",
            "Loading: ECOR40\n",
            "Loading: ECOR48\n",
            "Loading: ROAR012\n",
            "Loading: ECOR60\n",
            "Loading: NILS29\n",
            "Loading: NILS16\n",
            "Loading: ED1A\n",
            "Loading: NILS77\n",
            "Loading: ROAR029\n",
            "Loading: NILS15\n",
            "Loading: NILS36\n",
            "Loading: NILS63\n",
            "Loading: M863\n",
            "Loading: TW10509\n",
            "Loading: ECOR70\n",
            "Loading: EDL933\n",
            "Loading: DEC2A\n",
            "Loading: NILS71\n",
            "Loading: NILS48\n",
            "Loading: ECOR41\n",
            "Loading: NILS19\n",
            "Loading: NILS09\n",
            "Loading: ROAR059\n",
            "Loading: NILS55\n",
            "Loading: LMR3158\n",
            "Loading: NILS72\n",
            "Loading: NILS12\n",
            "Loading: ECOR38\n",
            "Loading: ECOR24\n",
            "Loading: NILS49\n",
            "Loading: NILS11\n",
            "Loading: ECOR11\n",
            "Loading: NILS82\n",
            "Loading: NILS74\n",
            "Loading: ECOR55\n",
            "Loading: ECOR46\n",
            "Loading: NILS61\n",
            "Loading: ECOR50\n",
            "Loading: NILS66\n",
            "Loading: ROAR131\n",
            "Loading: ECOR36\n",
            "Loading: NILS56\n",
            "Loading: IAI39\n",
            "Loading: NILS73\n",
            "Loading: ECOR49\n",
            "Loading: NILS54\n",
            "Loading: ECOR44\n",
            "Loading: NILS70\n",
            "Loading: AL505\n",
            "Loading: ROAR434\n",
            "Loading: NILS25\n",
            "Loading: ECOR64\n",
            "Loading: NILS39\n",
            "Loading: NILS44\n",
            "Loading: NILS64\n",
            "Loading: ECOR12\n",
            "Loading: NILS03\n",
            "Loading: NILS26\n",
            "Loading: NILS07\n",
            "Loading: NILS79\n",
            "Loading: ECOR31\n",
            "Loading: NILS81\n",
            "Loading: NILS68\n",
            "Loading: ROAR387\n",
            "Loading: NILS30\n",
            "Loading: 921A\n",
            "Loading: NILS69\n",
            "Loading: ECOR52\n",
            "Loading: NILS23\n",
            "Loading: NILS51\n",
            "Loading: NILS37\n",
            "Loading: 916A\n",
            "Loading: NILS31\n",
            "Loading: DEC1A\n",
            "Loading: ECOR35\n",
            "Loading: B49\n",
            "Loading: NILS60\n",
            "Loading: NILS78\n",
            "Loading: ECOR54\n",
            "Loading: ROAR036\n",
            "Loading: NILS01\n",
            "Loading: NILS46\n",
            "Loading: NILS80\n",
            "Loading: ECOR51\n",
            "Loading: ECOR62\n",
            "Loading: NILS18\n",
            "Successfully loaded 177 embeddings from /content/drive/MyDrive/Arkin/ESM2/public_datasets/ecoli/strain\n",
            "Warning: 225 genomes from the CSV were not found:\n",
            "  - AN17\n",
            "  - ECOR01\n",
            "  - EFERGUSONIIIVAN\n",
            "  - H10060022LR\n",
            "  - IAI11\n",
            "  - IAI40\n",
            "  - IAI41\n",
            "  - IAI67\n",
            "  - S21\n",
            "  - T205\n",
            "  ...and 215 more\n",
            "Loaded 96 genome IDs from /content/drive/MyDrive/Arkin/phage_public_datasets/e_coli/interaction_matrix.csv\n",
            "Loading: 412_P5\n",
            "Loading: 409_P8\n",
            "Loading: BCH953_P3\n",
            "Loading: 409_P6\n",
            "Loading: LF82_P6\n",
            "Loading: LF73_P4\n",
            "Loading: LI10_P3\n",
            "Loading: AN17_P8\n",
            "Loading: AL505_Ev3\n",
            "Loading: BDX03_P1\n",
            "Loading: LF50_P3\n",
            "Loading: LF110_P2\n",
            "Loading: 411_P1\n",
            "Loading: NAN33_P2\n",
            "Loading: LF31_P3\n",
            "Loading: LM08_P1\n",
            "Loading: LI10_P1\n",
            "Loading: LF73_P1\n",
            "Loading: LF82_P9\n",
            "Loading: LF7074_P1\n",
            "Loading: 536_P1\n",
            "Loading: AN24_P2\n",
            "Loading: 423_P5\n",
            "Loading: 409_P3\n",
            "Loading: NIC06_P2\n",
            "Loading: LM08_P2\n",
            "Loading: 427_P3\n",
            "Loading: LM40_P3\n",
            "Loading: 427_P2\n",
            "Loading: 409_P1\n",
            "Loading: 427_P4\n",
            "Loading: 423_P9\n",
            "Loading: DIJ07_P2\n",
            "Loading: 536_P7\n",
            "Loading: BCH953_P4\n",
            "Loading: NRG_12A1B\n",
            "Loading: AN24_P4\n",
            "Loading: LI10_P5\n",
            "Loading: 536_P6\n",
            "Loading: MT1B1_3A1\n",
            "Loading: LI10_P4\n",
            "Loading: DIJ06_P1\n",
            "Loading: LF110_P3\n",
            "Loading: 536_P11\n",
            "Loading: LM33_P1\n",
            "Loading: BCH953_P5\n",
            "Loading: LF82_P3\n",
            "Loading: NAN33_P1\n",
            "Loading: 416_P4\n",
            "Loading: LF110_P4\n",
            "Loading: AL505_Sd2\n",
            "Loading: BDX03_P2\n",
            "Loading: LF110_P1\n",
            "Loading: 423_P7\n",
            "Loading: 536_P12\n",
            "Loading: AN24_P3\n",
            "Loading: 55989_P2\n",
            "Loading: 412_P3\n",
            "Loading: 412_P1\n",
            "Loading: 412_P2\n",
            "Loading: 536_P9\n",
            "Loading: LF82_P1\n",
            "Loading: AN17_P1\n",
            "Loading: 412_P4\n",
            "Loading: LF7074_P3\n",
            "Loading: 411_P2\n",
            "Loading: NRG_11A2\n",
            "Loading: LF82_P4\n",
            "Loading: BDX09_P1\n",
            "Loading: LI10_P2\n",
            "Loading: 55989_P1\n",
            "Loading: LM40_P1\n",
            "Loading: NAN33_P5\n",
            "Loading: BCH953_P2\n",
            "Loading: NIC06_P3\n",
            "Loading: 409_P5\n",
            "Loading: 416_P5\n",
            "Loading: DIJ07_P1\n",
            "Loading: LF73_P3\n",
            "Loading: LM40_P2\n",
            "Loading: NRG_11B1\n",
            "Loading: LF7074_P2\n",
            "Loading: NAN33_P4\n",
            "Loading: 423_P1\n",
            "Loading: LI10_P6\n",
            "Loading: 423_P10\n",
            "Loading: NAN33_P6\n",
            "Loading: LF82_P2\n",
            "Loading: LM07_P1\n",
            "Loading: LF82_P5\n",
            "Loading: LF31_P1\n",
            "Loading: BCH953_P1\n",
            "Loading: LF82_P8\n",
            "Loading: LM02_P1\n",
            "Successfully loaded 94 embeddings from /content/drive/MyDrive/Arkin/ESM2/public_datasets/ecoli/phage\n",
            "Warning: 2 genomes from the CSV were not found:\n",
            "  - T4LD\n",
            "  - T7_Portugal\n",
            "Train set: 13254 interactions, 141 strains\n",
            "Test set:  3384 interactions, 36 strains\n",
            "pos_weight_val = 6.593\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   0%|          | 0/100 [00:00<?, ?it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cross-Validation"
      ],
      "metadata": {
        "id": "ajzGBjyN7Wpj"
      },
      "id": "ajzGBjyN7Wpj"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import KFold\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "import torch\n",
        "\n",
        "def strain_k_fold_cv(\n",
        "        interaction_matrix_path,  # Path to the interaction matrix CSV\n",
        "        strain_embeddings_path,   # Path to strain embeddings directory\n",
        "        phage_embeddings_path,    # Path to phage embeddings directory\n",
        "        k=10,\n",
        "        random_state=42,\n",
        "        output_dir=None,\n",
        "        batch_size=32,\n",
        "        num_epochs=100,\n",
        "        learning_rate=5e-5,\n",
        "        patience=7,\n",
        "        debug=False  # Option to use a smaller subset for debugging\n",
        "    ):\n",
        "    \"\"\"\n",
        "    Perform k-fold cross-validation based on strains.\n",
        "\n",
        "    Args:\n",
        "        interaction_matrix_path: Path to CSV with columns ['strain', 'phage', 'interaction']\n",
        "        strain_embeddings_path: Path to directory containing strain embeddings\n",
        "        phage_embeddings_path: Path to directory containing phage embeddings\n",
        "        k: Number of folds (default: 10)\n",
        "        random_state: Random seed for reproducibility\n",
        "        output_dir: Directory to save results\n",
        "        batch_size: Batch size for training\n",
        "        num_epochs: Maximum number of epochs for training\n",
        "        learning_rate: Learning rate for optimizer\n",
        "        patience: Patience for early stopping\n",
        "        debug: If True, use a subset of the data for faster debugging\n",
        "\n",
        "    Returns:\n",
        "        DataFrame with performance metrics for each fold\n",
        "    \"\"\"\n",
        "    # Create output directory if it doesn't exist\n",
        "    if output_dir is None:\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        output_dir = f\"cv_results_{timestamp}\"\n",
        "\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    os.makedirs(os.path.join(output_dir, \"models\"), exist_ok=True)\n",
        "    os.makedirs(os.path.join(output_dir, \"plots\"), exist_ok=True)\n",
        "\n",
        "    # Log the configuration\n",
        "    config = {\n",
        "        'interaction_matrix_path': interaction_matrix_path,\n",
        "        'strain_embeddings_path': strain_embeddings_path,\n",
        "        'phage_embeddings_path': phage_embeddings_path,\n",
        "        'k': k,\n",
        "        'random_state': random_state,\n",
        "        'batch_size': batch_size,\n",
        "        'num_epochs': num_epochs,\n",
        "        'learning_rate': learning_rate,\n",
        "        'patience': patience,\n",
        "        'debug': debug\n",
        "    }\n",
        "\n",
        "    pd.DataFrame([config]).to_csv(os.path.join(output_dir, \"config.csv\"), index=False)\n",
        "\n",
        "    print(\"Loading embeddings...\")\n",
        "    strain_embeddings = load_embeddings_with_ids(strain_embeddings_path)\n",
        "    phage_embeddings = load_embeddings_with_ids(phage_embeddings_path)\n",
        "\n",
        "    print(\"Loading interaction data...\")\n",
        "    interactions_df = pd.read_csv(interaction_matrix_path)\n",
        "\n",
        "    # Filter interactions_df before splitting\n",
        "    strain_keys = set(strain_embeddings.keys())\n",
        "    phage_keys = set(phage_embeddings.keys())\n",
        "    interactions_df = interactions_df[\n",
        "        interactions_df['strain'].isin(strain_keys) &\n",
        "        interactions_df['phage'].isin(phage_keys)\n",
        "    ]\n",
        "\n",
        "    print(f\"Filtered interactions: {len(interactions_df)} rows\")\n",
        "\n",
        "    # Debug mode filtering\n",
        "    if debug:\n",
        "        print(\"Running in debug mode with reduced dataset...\")\n",
        "        random_strains = random.sample(list(strain_keys), min(50, len(strain_keys)))\n",
        "        random_phages = random.sample(list(phage_keys), min(50, len(phage_keys)))\n",
        "        interactions_df = interactions_df[\n",
        "            interactions_df['strain'].isin(random_strains) &\n",
        "            interactions_df['phage'].isin(random_phages)\n",
        "        ]\n",
        "        print(f\"Debug dataset: {len(interactions_df)} interactions\")\n",
        "\n",
        "    # Get unique strains\n",
        "    unique_strains = interactions_df['strain'].unique()\n",
        "    print(f\"Total unique strains: {len(unique_strains)}\")\n",
        "\n",
        "    # Create k-fold splitter\n",
        "    kf = KFold(n_splits=k, shuffle=True, random_state=random_state)\n",
        "\n",
        "    # Store results for each fold\n",
        "    results = []\n",
        "\n",
        "    # Perform k-fold cross-validation\n",
        "    for fold, (train_idx, test_idx) in enumerate(kf.split(unique_strains)):\n",
        "        fold_num = fold + 1\n",
        "        print(f\"\\n{'='*50}\")\n",
        "        print(f\"Processing fold {fold_num}/{k}\")\n",
        "        print(f\"{'='*50}\")\n",
        "\n",
        "        # Get train/test strains\n",
        "        train_strains = unique_strains[train_idx]\n",
        "        test_strains = unique_strains[test_idx]\n",
        "\n",
        "        # Filter interactions\n",
        "        train_df = interactions_df[interactions_df['strain'].isin(train_strains)]\n",
        "        test_df = interactions_df[interactions_df['strain'].isin(test_strains)]\n",
        "\n",
        "        print(f\"Train set: {len(train_df)} interactions, {len(train_strains)} strains\")\n",
        "        print(f\"Test set:  {len(test_df)} interactions, {len(test_strains)} strains\")\n",
        "\n",
        "        # Create data loaders\n",
        "        train_loader, test_loader = create_data_loaders(\n",
        "            train_df, test_df, strain_embeddings, phage_embeddings, batch_size=batch_size\n",
        "        )\n",
        "\n",
        "        # Initialize model\n",
        "        model = StrainPhageTransformer(\n",
        "            embedding_dim=1280,\n",
        "            num_heads=8,\n",
        "            strain_inds=128,\n",
        "            phage_inds=64,\n",
        "            dropout=0.1,\n",
        "            ln=True,\n",
        "            temperature=0.01\n",
        "        ).to(device)\n",
        "\n",
        "        # Initialize attention weights\n",
        "        model = init_attention_weights(model)\n",
        "\n",
        "        # Compute pos_weight for handling class imbalance\n",
        "        num_pos = (train_df['interaction'] == 1).sum()\n",
        "        num_neg = (train_df['interaction'] == 0).sum()\n",
        "        pos_weight_val = num_neg / max(num_pos, 1)\n",
        "        print(f\"Positive class weight: {pos_weight_val:.4f}\")\n",
        "\n",
        "        # Train model\n",
        "        history = train_model(\n",
        "            model,\n",
        "            train_loader,\n",
        "            test_loader,\n",
        "            num_epochs=num_epochs,\n",
        "            learning_rate=learning_rate,\n",
        "            patience=patience,\n",
        "            device=device,\n",
        "            pos_weight_val=pos_weight_val\n",
        "        )\n",
        "\n",
        "        # Plot training history\n",
        "        fold_plot_path = os.path.join(output_dir, \"plots\", f\"fold_{fold_num}_history.png\")\n",
        "        plot_training_history(history, save_path=fold_plot_path)\n",
        "\n",
        "        # Evaluate model\n",
        "        print(f\"\\nEvaluating model for fold {fold_num}...\")\n",
        "        metrics = predict_and_evaluate(\n",
        "            model, test_loader, device,\n",
        "            plot_dir=os.path.join(output_dir, \"plots\"),\n",
        "            fold_num=fold_num\n",
        "        )\n",
        "\n",
        "        # Store results\n",
        "        fold_results = {\n",
        "            'fold': fold_num,\n",
        "            'accuracy': metrics['accuracy'],\n",
        "            'precision': metrics['precision'],\n",
        "            'recall': metrics['recall'],\n",
        "            'f1': metrics['f1'],\n",
        "            'mcc': metrics['mcc'],\n",
        "            'roc_auc': metrics['roc_auc'],\n",
        "            'pr_auc': metrics['pr_auc'],\n",
        "            'train_size': len(train_df),\n",
        "            'test_size': len(test_df),\n",
        "            'num_train_strains': len(train_strains),\n",
        "            'num_test_strains': len(test_strains),\n",
        "            'pos_weight': pos_weight_val,\n",
        "            'early_stop_epoch': len(history['train_loss'])\n",
        "        }\n",
        "        results.append(fold_results)\n",
        "\n",
        "        # Save detailed fold results to CSV\n",
        "        fold_df = pd.DataFrame([fold_results])\n",
        "        fold_df.to_csv(os.path.join(output_dir, f\"fold_{fold_num}_metrics.csv\"), index=False)\n",
        "\n",
        "        # Save model for this fold\n",
        "        torch.save({\n",
        "            'fold': fold_num,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'history': history,\n",
        "            'metrics': metrics,\n",
        "            'test_strains': test_strains.tolist()\n",
        "        }, os.path.join(output_dir, \"models\", f\"model_fold_{fold_num}.pt\"))\n",
        "\n",
        "    # Convert results to DataFrame\n",
        "    results_df = pd.DataFrame(results)\n",
        "\n",
        "    # Calculate summary statistics\n",
        "    summary = results_df.describe()\n",
        "\n",
        "    # Save results\n",
        "    results_df.to_csv(os.path.join(output_dir, \"all_folds_metrics.csv\"), index=False)\n",
        "    summary.to_csv(os.path.join(output_dir, \"summary_statistics.csv\"))\n",
        "\n",
        "    # Plot summary of key metrics across folds\n",
        "    plot_cv_summary(results_df, os.path.join(output_dir, \"summary_boxplot.png\"))\n",
        "\n",
        "    return results_df, summary"
      ],
      "metadata": {
        "id": "oTsH7cOF7aRu"
      },
      "id": "oTsH7cOF7aRu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_training_history(history, save_path=None):\n",
        "    \"\"\"\n",
        "    Plot training and validation loss and MCC over epochs.\n",
        "\n",
        "    Args:\n",
        "        history: Dictionary with training history\n",
        "        save_path: Path to save the plot (if None, plot is displayed)\n",
        "    \"\"\"\n",
        "    epochs = len(history['train_loss'])\n",
        "    plt.figure(figsize=(15, 6))\n",
        "\n",
        "    # Loss\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(range(1, epochs+1), history['train_loss'], label='Train Loss')\n",
        "    plt.plot(range(1, epochs+1), history['val_loss'], label='Val Loss')\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.title(\"Loss vs. Epochs\")\n",
        "    plt.legend()\n",
        "    plt.grid(alpha=0.3)\n",
        "\n",
        "    # MCC\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(range(1, epochs+1), history['train_mcc'], label='Train MCC')\n",
        "    plt.plot(range(1, epochs+1), history['val_mcc'], label='Val MCC')\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"MCC\")\n",
        "    plt.title(\"MCC vs. Epochs\")\n",
        "    plt.legend()\n",
        "    plt.grid(alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "    else:\n",
        "        plt.show()\n",
        "\n",
        "def predict_and_evaluate(model, data_loader, device, plot_dir=None, fold_num=None):\n",
        "    \"\"\"\n",
        "    Makes predictions and calculates performance metrics.\n",
        "\n",
        "    Args:\n",
        "        model: Trained model\n",
        "        data_loader: DataLoader for evaluation\n",
        "        device: Device to run model on\n",
        "        plot_dir: Directory to save plots (if None, plots are displayed)\n",
        "        fold_num: Current fold number for naming plots\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with performance metrics\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for strain_emb, phage_emb, strain_mask, phage_mask, labels in data_loader:\n",
        "            strain_emb = strain_emb.to(device)\n",
        "            phage_emb = phage_emb.to(device)\n",
        "            strain_mask = strain_mask.to(device)\n",
        "            phage_mask = phage_mask.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            logits = model(strain_emb, phage_emb, strain_mask, phage_mask)\n",
        "            preds = torch.sigmoid(logits).cpu().numpy().flatten()\n",
        "\n",
        "            all_preds.extend(preds)\n",
        "            all_labels.extend(labels.cpu().numpy().flatten())\n",
        "\n",
        "    all_preds = np.array(all_preds)\n",
        "    all_labels = np.array(all_labels)\n",
        "\n",
        "    # Calculate metrics\n",
        "    binary_preds = (all_preds > 0.5).astype(int)\n",
        "    accuracy = accuracy_score(all_labels, binary_preds)\n",
        "    precision = precision_score(all_labels, binary_preds)\n",
        "    recall = recall_score(all_labels, binary_preds)\n",
        "    f1 = f1_score(all_labels, binary_preds)\n",
        "    mcc = matthews_corrcoef(all_labels, binary_preds)\n",
        "    roc_auc = roc_auc_score(all_labels, all_preds)\n",
        "    pr_auc = average_precision_score(all_labels, all_preds)\n",
        "    conf_matrix = confusion_matrix(all_labels, binary_preds)\n",
        "\n",
        "    # Print metrics\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n",
        "    print(f\"MCC: {mcc:.4f}\")\n",
        "    print(f\"ROC AUC: {roc_auc:.4f}\")\n",
        "    print(f\"PR AUC: {pr_auc:.4f}\")\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(conf_matrix)\n",
        "\n",
        "    # Plot confusion matrix\n",
        "    plt.figure(figsize=(8, 7))\n",
        "    sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False,\n",
        "                xticklabels=['Predicted Negative', 'Predicted Positive'],\n",
        "                yticklabels=['Actual Negative', 'Actual Positive'])\n",
        "    plt.title(\"Confusion Matrix\")\n",
        "    plt.xlabel(\"Predicted Label\")\n",
        "    plt.ylabel(\"True Label\")\n",
        "\n",
        "    if plot_dir and fold_num is not None:\n",
        "        plt.savefig(os.path.join(plot_dir, f\"fold_{fold_num}_confusion_matrix.png\"),\n",
        "                    dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "    else:\n",
        "        plt.show()\n",
        "\n",
        "    # Plot ROC curve\n",
        "    plt.figure(figsize=(8, 7))\n",
        "    fpr, tpr, _ = roc_curve(all_labels, all_preds)\n",
        "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.3f})')\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.grid(alpha=0.3)\n",
        "\n",
        "    if plot_dir and fold_num is not None:\n",
        "        plt.savefig(os.path.join(plot_dir, f\"fold_{fold_num}_roc_curve.png\"),\n",
        "                    dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "    else:\n",
        "        plt.show()\n",
        "\n",
        "    # Plot Precision-Recall curve\n",
        "    plt.figure(figsize=(8, 7))\n",
        "    precision_curve, recall_curve, _ = precision_recall_curve(all_labels, all_preds)\n",
        "    plt.plot(recall_curve, precision_curve, color='darkorange', lw=2,\n",
        "             label=f'PR curve (area = {pr_auc:.3f})')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('Recall')\n",
        "    plt.ylabel('Precision')\n",
        "    plt.title('Precision-Recall Curve')\n",
        "    plt.legend(loc=\"lower left\")\n",
        "    plt.grid(alpha=0.3)\n",
        "\n",
        "    if plot_dir and fold_num is not None:\n",
        "        plt.savefig(os.path.join(plot_dir, f\"fold_{fold_num}_pr_curve.png\"),\n",
        "                    dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "    else:\n",
        "        plt.show()\n",
        "\n",
        "    # Return metrics as dictionary\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1,\n",
        "        'mcc': mcc,\n",
        "        'roc_auc': roc_auc,\n",
        "        'pr_auc': pr_auc,\n",
        "        'conf_matrix': conf_matrix,\n",
        "        'all_preds': all_preds,\n",
        "        'all_labels': all_labels\n",
        "    }\n",
        "\n",
        "def plot_cv_summary(results_df, save_path=None):\n",
        "    \"\"\"\n",
        "    Plot summary of cross-validation results.\n",
        "\n",
        "    Args:\n",
        "        results_df: DataFrame with results from cross-validation\n",
        "        save_path: Path to save the plot (if None, plot is displayed)\n",
        "    \"\"\"\n",
        "    # Select key metrics\n",
        "    metrics = ['accuracy', 'precision', 'recall', 'f1', 'mcc', 'roc_auc', 'pr_auc']\n",
        "\n",
        "    # Melt the DataFrame for plotting\n",
        "    plot_df = pd.melt(results_df, id_vars=['fold'], value_vars=metrics,\n",
        "                       var_name='Metric', value_name='Value')\n",
        "\n",
        "    # Create boxplot\n",
        "    plt.figure(figsize=(14, 8))\n",
        "    sns.boxplot(x='Metric', y='Value', data=plot_df)\n",
        "    sns.swarmplot(x='Metric', y='Value', data=plot_df, color='black', size=8)\n",
        "\n",
        "    plt.title('Cross-Validation Performance Metrics', fontsize=16)\n",
        "    plt.ylabel('Score', fontsize=14)\n",
        "    plt.xlabel('Metric', fontsize=14)\n",
        "    plt.grid(axis='y', alpha=0.3)\n",
        "    plt.xticks(rotation=45)\n",
        "\n",
        "    # Add median values on top of each box\n",
        "    medians = results_df[metrics].median().values\n",
        "    pos = range(len(metrics))\n",
        "    for i, metric in enumerate(metrics):\n",
        "        plt.text(i, medians[i] + 0.02, f'Median: {medians[i]:.3f}',\n",
        "                 horizontalalignment='center', size='medium', color='black', weight='bold')\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "    else:\n",
        "        plt.show()\n",
        "\n",
        "    # Also create a table with median values for quick reference\n",
        "    median_df = results_df[metrics].median().reset_index()\n",
        "    median_df.columns = ['Metric', 'Median']\n",
        "\n",
        "    if save_path:\n",
        "        median_csv_path = save_path.replace('.png', '_medians.csv')\n",
        "        median_df.to_csv(median_csv_path, index=False)"
      ],
      "metadata": {
        "id": "3_FP3zvQ7efk"
      },
      "id": "3_FP3zvQ7efk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries for random sampling\n",
        "import random\n",
        "\n",
        "# Set up your paths\n",
        "interaction_matrix_path = '/content/drive/MyDrive/Arkin/phage_public_datasets/pseudomonas/interaction_matrix.csv'\n",
        "strain_embeddings_path = '/content/drive/MyDrive/Arkin/ESM2/public_datasets/pseudomonas/strain_mod/'\n",
        "phage_embeddings_path = '/content/drive/MyDrive/Arkin/ESM2/public_datasets/pseudomonas/phage_mod/'\n",
        "\n",
        "# Set parameters\n",
        "k_folds = 10\n",
        "random_state = 42\n",
        "output_dir = \"/content/drive/MyDrive/Arkin/set_transformer_data/cv_results_pseudomonas\"\n",
        "\n",
        "# Run cross-validation\n",
        "results_df, summary = strain_k_fold_cv(\n",
        "    interaction_matrix_path=interaction_matrix_path,\n",
        "    strain_embeddings_path=strain_embeddings_path,\n",
        "    phage_embeddings_path=phage_embeddings_path,\n",
        "    k=k_folds,\n",
        "    random_state=random_state,\n",
        "    output_dir=output_dir,\n",
        "    batch_size=32,\n",
        "    num_epochs=100,\n",
        "    learning_rate=5e-5,\n",
        "    patience=7,\n",
        "    debug=False  # Set to True for a quick test run with a subset of data\n",
        ")\n",
        "\n",
        "# Display summary statistics with focus on median (50%)\n",
        "print(\"\\nSummary Statistics (focus on median):\")\n",
        "print(summary.loc[['50%'], :])"
      ],
      "metadata": {
        "id": "ACXUiTNa7hMk"
      },
      "id": "ACXUiTNa7hMk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attention Analysis"
      ],
      "metadata": {
        "id": "Tgee6yse4YPO"
      },
      "id": "Tgee6yse4YPO"
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_attention_variance(model, test_loader, device, num_samples=10):\n",
        "    \"\"\"Analyze the variance in attention weights to debug uniform attention issues\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    head_variances = []\n",
        "    sample_variances = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (strain_emb, phage_emb, strain_mask, phage_mask, _) in enumerate(test_loader):\n",
        "            if i >= num_samples:\n",
        "                break\n",
        "\n",
        "            strain_emb = strain_emb.to(device)\n",
        "            phage_emb = phage_emb.to(device)\n",
        "            strain_mask = strain_mask.to(device) if strain_mask is not None else None\n",
        "            phage_mask = phage_mask.to(device) if phage_mask is not None else None\n",
        "\n",
        "            # Get attention maps\n",
        "            _, (strain_to_phage_attn, phage_to_strain_attn) = model(\n",
        "                strain_emb, phage_emb, strain_mask, phage_mask, return_attn=True\n",
        "            )\n",
        "\n",
        "            # Convert to numpy for analysis\n",
        "            strain_to_phage_attn = strain_to_phage_attn.cpu().numpy().squeeze(0)  # [num_heads, n_strain, n_phage]\n",
        "            phage_to_strain_attn = phage_to_strain_attn.cpu().numpy().squeeze(0)  # [num_heads, n_phage, n_strain]\n",
        "\n",
        "            # Analyze variance across different heads\n",
        "            for head in range(strain_to_phage_attn.shape[0]):\n",
        "                head_strain_variance = np.var(strain_to_phage_attn[head])\n",
        "                head_phage_variance = np.var(phage_to_strain_attn[head])\n",
        "                head_variances.append({\n",
        "                    'sample': i,\n",
        "                    'head': head,\n",
        "                    'strain_to_phage_variance': head_strain_variance,\n",
        "                    'phage_to_strain_variance': head_phage_variance\n",
        "                })\n",
        "\n",
        "            # Overall sample variance\n",
        "            sample_strain_variance = np.var(strain_to_phage_attn)\n",
        "            sample_phage_variance = np.var(phage_to_strain_attn)\n",
        "            sample_variances.append({\n",
        "                'sample': i,\n",
        "                'strain_to_phage_variance': sample_strain_variance,\n",
        "                'phage_to_strain_variance': sample_phage_variance\n",
        "            })\n",
        "\n",
        "    # Convert to DataFrames\n",
        "    head_variance_df = pd.DataFrame(head_variances)\n",
        "    sample_variance_df = pd.DataFrame(sample_variances)\n",
        "\n",
        "    print(\"Average head variance:\")\n",
        "    print(head_variance_df[['strain_to_phage_variance', 'phage_to_strain_variance']].mean())\n",
        "\n",
        "    print(\"\\nAverage sample variance:\")\n",
        "    print(sample_variance_df[['strain_to_phage_variance', 'phage_to_strain_variance']].mean())\n",
        "\n",
        "    return head_variance_df, sample_variance_df\n",
        "\n",
        "def plot_variance_analysis(head_variance_df, sample_variance_df, output_dir):\n",
        "    \"\"\"Plot the variance analysis results\"\"\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Plot head variance\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    sns.boxplot(x='head', y='strain_to_phage_variance', data=head_variance_df)\n",
        "    plt.title('Strain-to-Phage Attention Variance by Head')\n",
        "    plt.xlabel('Attention Head')\n",
        "    plt.ylabel('Variance')\n",
        "    plt.savefig(os.path.join(output_dir, 'strain_to_phage_head_variance.png'))\n",
        "    plt.close()\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    sns.boxplot(x='head', y='phage_to_strain_variance', data=head_variance_df)\n",
        "    plt.title('Phage-to-Strain Attention Variance by Head')\n",
        "    plt.xlabel('Attention Head')\n",
        "    plt.ylabel('Variance')\n",
        "    plt.savefig(os.path.join(output_dir, 'phage_to_strain_head_variance.png'))\n",
        "    plt.close()\n",
        "\n",
        "    # Plot sample variance\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.scatter(\n",
        "        sample_variance_df['strain_to_phage_variance'],\n",
        "        sample_variance_df['phage_to_strain_variance'],\n",
        "        alpha=0.7\n",
        "    )\n",
        "    plt.xlabel('Strain-to-Phage Variance')\n",
        "    plt.ylabel('Phage-to-Strain Variance')\n",
        "    plt.title('Attention Variance by Sample')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.savefig(os.path.join(output_dir, 'sample_variance.png'))\n",
        "    plt.close()\n",
        "\n",
        "output_dir = '/content/drive/MyDrive/Arkin/set_transformer_data/outputs/attention_analysis/variance_analysis2'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Create a test loader with batch_size=1 for attention analysis\n",
        "_, test_loader = create_data_loaders(\n",
        "    None,\n",
        "    test_df,\n",
        "    strain_embeddings,\n",
        "    phage_embeddings,\n",
        "    batch_size=1  # Must be 1 for attention analysis\n",
        ")\n",
        "\n",
        "# Run the variance analysis\n",
        "head_variance_df, sample_variance_df = analyze_attention_variance(\n",
        "    model, test_loader, device, num_samples=20\n",
        ")\n",
        "\n",
        "# Plot and save the results\n",
        "plot_variance_analysis(head_variance_df, sample_variance_df, output_dir)"
      ],
      "metadata": {
        "id": "E8Ua1mRN4bHp"
      },
      "id": "E8Ua1mRN4bHp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "9bNqQJacqrcJ",
      "metadata": {
        "id": "9bNqQJacqrcJ"
      },
      "source": [
        "## Model Performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "S3ATjCfRrAqH",
      "metadata": {
        "id": "S3ATjCfRrAqH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "907d17c7-2d6a-4126-c96f-dc0b4a7a9a8f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Loading outputs from /content/drive/MyDrive/Arkin/set_transformer_data/outputs/outputs/main_function_outputs_ecoli_1024_5_2layer.pkl\n",
            "Loading embeddings...\n",
            "Loading: ECOR17\n",
            "Error loading /content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/strains/ECOR17.npy: can only convert an array of size 1 to a Python scalar\n",
            "Loading: ECOR71\n",
            "Error loading /content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/strains/ECOR71.npy: can only convert an array of size 1 to a Python scalar\n",
            "Loading: ECRC22\n",
            "Error loading /content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/strains/ECRC22.npy: can only convert an array of size 1 to a Python scalar\n",
            "Loading: ECRC46\n",
            "Error loading /content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/strains/ECRC46.npy: can only convert an array of size 1 to a Python scalar\n",
            "Loading: ECRC21\n",
            "Error loading /content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/strains/ECRC21.npy: can only convert an array of size 1 to a Python scalar\n",
            "Loading: ECRC35\n",
            "Error loading /content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/strains/ECRC35.npy: can only convert an array of size 1 to a Python scalar\n",
            "Loading: NILS34\n",
            "Error loading /content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/strains/NILS34.npy: can only convert an array of size 1 to a Python scalar\n",
            "Loading: NILS33\n",
            "Error loading /content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/strains/NILS33.npy: can only convert an array of size 1 to a Python scalar\n",
            "Loading: ECOR62\n",
            "Error loading /content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/strains/ECOR62.npy: can only convert an array of size 1 to a Python scalar\n",
            "Loading: DEC1A\n",
            "Error loading /content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/strains/DEC1A.npy: can only convert an array of size 1 to a Python scalar\n",
            "Loading: ECOR20\n",
            "Error loading /content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/strains/ECOR20.npy: can only convert an array of size 1 to a Python scalar\n",
            "Loading: NILS40\n",
            "Error loading /content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/strains/NILS40.npy: can only convert an array of size 1 to a Python scalar\n",
            "Loading: ECOR5\n",
            "Error loading /content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/strains/ECOR5.npy: can only convert an array of size 1 to a Python scalar\n",
            "Loading: ECOR50\n",
            "Error loading /content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/strains/ECOR50.npy: can only convert an array of size 1 to a Python scalar\n",
            "Loading: ECOR7\n",
            "Error loading /content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/strains/ECOR7.npy: can only convert an array of size 1 to a Python scalar\n",
            "Loading: ECOR37\n",
            "Error loading /content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/strains/ECOR37.npy: can only convert an array of size 1 to a Python scalar\n",
            "Loading: ECRC13\n",
            "Error loading /content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/strains/ECRC13.npy: can only convert an array of size 1 to a Python scalar\n",
            "Loading: B6A1\n",
            "Error loading /content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/strains/B6A1.npy: can only convert an array of size 1 to a Python scalar\n",
            "Loading: ECOR10\n",
            "Error loading /content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/strains/ECOR10.npy: can only convert an array of size 1 to a Python scalar\n",
            "Loading: ECOR47\n",
            "Error loading /content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/strains/ECOR47.npy: can only convert an array of size 1 to a Python scalar\n",
            "Loading: ECOR29\n",
            "Error loading /content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/strains/ECOR29.npy: can only convert an array of size 1 to a Python scalar\n",
            "Loading: ECOR36\n",
            "Error loading /content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/strains/ECOR36.npy: can only convert an array of size 1 to a Python scalar\n",
            "Loading: 916A\n",
            "Error loading /content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/strains/916A.npy: can only convert an array of size 1 to a Python scalar\n",
            "Loading: ECOR38\n",
            "Error loading /content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/strains/ECOR38.npy: can only convert an array of size 1 to a Python scalar\n",
            "Loading: ECOR59\n",
            "Error loading /content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/strains/ECOR59.npy: can only convert an array of size 1 to a Python scalar\n",
            "Loading: ECOR61\n",
            "Error loading /content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/strains/ECOR61.npy: can only convert an array of size 1 to a Python scalar\n",
            "Loading: ECRC17\n",
            "Error loading /content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/strains/ECRC17.npy: can only convert an array of size 1 to a Python scalar\n",
            "Loading: ECRC12\n",
            "Error loading /content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/strains/ECRC12.npy: can only convert an array of size 1 to a Python scalar\n",
            "Loading: ECOR40\n",
            "Error loading /content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/strains/ECOR40.npy: can only convert an array of size 1 to a Python scalar\n",
            "Loading: ECOR70\n",
            "Error loading /content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/strains/ECOR70.npy: can only convert an array of size 1 to a Python scalar\n",
            "Loading: ECOR9\n",
            "Error loading /content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/strains/ECOR9.npy: can only convert an array of size 1 to a Python scalar\n",
            "Loading: ECOR43\n",
            "Error loading /content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/strains/ECOR43.npy: can only convert an array of size 1 to a Python scalar\n",
            "Loading: ECOR41\n",
            "Error loading /content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/strains/ECOR41.npy: can only convert an array of size 1 to a Python scalar\n",
            "Loading: ECOR14\n",
            "Error loading /content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/strains/ECOR14.npy: can only convert an array of size 1 to a Python scalar\n",
            "Loading: ECOR39\n",
            "Error loading /content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/strains/ECOR39.npy: can only convert an array of size 1 to a Python scalar\n",
            "Loading: ECOR6\n",
            "Error loading /content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/strains/ECOR6.npy: can only convert an array of size 1 to a Python scalar\n",
            "Loading: ECOR52\n",
            "Error loading /content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/strains/ECOR52.npy: can only convert an array of size 1 to a Python scalar\n",
            "Loading: ECOR44\n",
            "Error loading /content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/strains/ECOR44.npy: can only convert an array of size 1 to a Python scalar\n",
            "Loading: ECOR56\n",
            "Error loading /content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/strains/ECOR56.npy: can only convert an array of size 1 to a Python scalar\n",
            "Loading: BW25113\n",
            "Error loading /content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/strains/BW25113.npy: can only convert an array of size 1 to a Python scalar\n",
            "Loading: ECOR58\n",
            "Error loading /content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/strains/ECOR58.npy: can only convert an array of size 1 to a Python scalar\n",
            "Loading: ECOR69\n",
            "Error loading /content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/strains/ECOR69.npy: can only convert an array of size 1 to a Python scalar\n",
            "Loading: ECOR15\n",
            "Error loading /content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/strains/ECOR15.npy: can only convert an array of size 1 to a Python scalar\n",
            "Loading: ECOR55\n",
            "Error loading /content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/strains/ECOR55.npy: can only convert an array of size 1 to a Python scalar\n",
            "Loading: ECOR16\n",
            "Error loading /content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/strains/ECOR16.npy: can only convert an array of size 1 to a Python scalar\n",
            "Loading: ECOR31\n",
            "Error loading /content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/strains/ECOR31.npy: can only convert an array of size 1 to a Python scalar\n",
            "Loading: ECOR54\n",
            "Error loading /content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/strains/ECOR54.npy: can only convert an array of size 1 to a Python scalar\n",
            "Loading: 55989\n",
            "Error loading /content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/strains/55989.npy: can only convert an array of size 1 to a Python scalar\n",
            "Loading: ECOR63\n",
            "Error loading /content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/strains/ECOR63.npy: can only convert an array of size 1 to a Python scalar\n",
            "Loading: ECOR24\n",
            "Error loading /content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/strains/ECOR24.npy: can only convert an array of size 1 to a Python scalar\n",
            "Loading: ECOR67\n",
            "Error loading /content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/strains/ECOR67.npy: can only convert an array of size 1 to a Python scalar\n",
            "Loading: ECOR53\n",
            "Error loading /content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/strains/ECOR53.npy: can only convert an array of size 1 to a Python scalar\n",
            "Loading: ECOR2\n",
            "Error loading /content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/strains/ECOR2.npy: can only convert an array of size 1 to a Python scalar\n",
            "Loading: ECOR49\n",
            "Error loading /content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/strains/ECOR49.npy: can only convert an array of size 1 to a Python scalar\n",
            "Loading: ECOR35\n",
            "Error loading /content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/strains/ECOR35.npy: can only convert an array of size 1 to a Python scalar\n",
            "Loading: ECRC14\n",
            "Error loading /content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/strains/ECRC14.npy: can only convert an array of size 1 to a Python scalar\n",
            "Loading: ECOR23\n",
            "Error loading /content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/strains/ECOR23.npy: can only convert an array of size 1 to a Python scalar\n",
            "Loading: ECRC2\n",
            "Error loading /content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/strains/ECRC2.npy: can only convert an array of size 1 to a Python scalar\n",
            "Loading: ECRC1\n",
            "Error loading /content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/strains/ECRC1.npy: can only convert an array of size 1 to a Python scalar\n",
            "Loading: ECOR42\n",
            "Error loading /content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/strains/ECOR42.npy: can only convert an array of size 1 to a Python scalar\n",
            "Loading: ECOR18\n",
            "Error loading /content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/strains/ECOR18.npy: can only convert an array of size 1 to a Python scalar\n",
            "Loading: AL505\n",
            "Error loading /content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/strains/AL505.npy: can only convert an array of size 1 to a Python scalar\n",
            "Loading: ECOR26\n",
            "Error loading /content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/strains/ECOR26.npy: can only convert an array of size 1 to a Python scalar\n",
            "Loading: ECOR25\n",
            "Error loading /content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/strains/ECOR25.npy: can only convert an array of size 1 to a Python scalar\n",
            "Loading: ECOR13\n",
            "Error loading /content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/strains/ECOR13.npy: can only convert an array of size 1 to a Python scalar\n",
            "Loading: B49\n",
            "Error loading /content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/strains/B49.npy: can only convert an array of size 1 to a Python scalar\n",
            "Loading: ECOR34\n",
            "Error loading /content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/strains/ECOR34.npy: can only convert an array of size 1 to a Python scalar\n",
            "Loading: 381A\n",
            "Error loading /content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/strains/381A.npy: can only convert an array of size 1 to a Python scalar\n",
            "Loading: ECOR60\n",
            "Error loading /content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/strains/ECOR60.npy: can only convert an array of size 1 to a Python scalar\n",
            "Loading: ECOR12\n",
            "Error loading /content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/strains/ECOR12.npy: can only convert an array of size 1 to a Python scalar\n",
            "Loading: ECOR68\n",
            "Error loading /content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/strains/ECOR68.npy: can only convert an array of size 1 to a Python scalar\n",
            "Loading: ECOR27\n",
            "Error loading /content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/strains/ECOR27.npy: can only convert an array of size 1 to a Python scalar\n",
            "Loading: ECOR8\n",
            "Error loading /content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/strains/ECOR8.npy: can only convert an array of size 1 to a Python scalar\n",
            "Loading: ECRC15\n",
            "Error loading /content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/strains/ECRC15.npy: can only convert an array of size 1 to a Python scalar\n",
            "Loading: ECRC23\n",
            "Error loading /content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/strains/ECRC23.npy: can only convert an array of size 1 to a Python scalar\n",
            "Loading: ECOR19\n",
            "Error loading /content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/strains/ECOR19.npy: can only convert an array of size 1 to a Python scalar\n",
            "Loading: ECOR46\n",
            "Error loading /content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/strains/ECOR46.npy: can only convert an array of size 1 to a Python scalar\n",
            "Loading: ECRC19\n",
            "Error loading /content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/strains/ECRC19.npy: can only convert an array of size 1 to a Python scalar\n",
            "Loading: ECOR66\n",
            "Error loading /content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/strains/ECOR66.npy: can only convert an array of size 1 to a Python scalar\n",
            "Loading: ECOR64\n",
            "Error loading /content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/strains/ECOR64.npy: can only convert an array of size 1 to a Python scalar\n",
            "Loading: ECOR57\n",
            "Error loading /content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/strains/ECOR57.npy: can only convert an array of size 1 to a Python scalar\n",
            "Loading: ECOR21\n",
            "Error loading /content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/strains/ECOR21.npy: can only convert an array of size 1 to a Python scalar\n",
            "Loading: ECRC25\n",
            "Error loading /content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/strains/ECRC25.npy: can only convert an array of size 1 to a Python scalar\n",
            "Loading: DEC2A\n",
            "Error loading /content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/strains/DEC2A.npy: can only convert an array of size 1 to a Python scalar\n",
            "Loading: ECRC24\n",
            "Error loading /content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/strains/ECRC24.npy: can only convert an array of size 1 to a Python scalar\n",
            "Loading: ECOR32\n",
            "Error loading /content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/strains/ECOR32.npy: can only convert an array of size 1 to a Python scalar\n",
            "Loading: 536\n",
            "Error loading /content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/strains/536.npy: can only convert an array of size 1 to a Python scalar\n",
            "Loading: ECOR30\n",
            "Error loading /content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/strains/ECOR30.npy: can only convert an array of size 1 to a Python scalar\n",
            "Loading: ECOR33\n",
            "Error loading /content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/strains/ECOR33.npy: can only convert an array of size 1 to a Python scalar\n",
            "Loading: 370D\n",
            "Error loading /content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/strains/370D.npy: can only convert an array of size 1 to a Python scalar\n",
            "Loading: BL21\n",
            "Error loading /content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/strains/BL21.npy: can only convert an array of size 1 to a Python scalar\n",
            "Loading: ECOR22\n",
            "Error loading /content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/strains/ECOR22.npy: can only convert an array of size 1 to a Python scalar\n",
            "Loading: ECOR3\n",
            "Error loading /content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/strains/ECOR3.npy: can only convert an array of size 1 to a Python scalar\n",
            "Loading: ECOR1\n",
            "Error loading /content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/strains/ECOR1.npy: can only convert an array of size 1 to a Python scalar\n",
            "Loading: ECOR65\n",
            "Error loading /content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/strains/ECOR65.npy: can only convert an array of size 1 to a Python scalar\n",
            "Loading: ECOR51\n",
            "Error loading /content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/strains/ECOR51.npy: can only convert an array of size 1 to a Python scalar\n",
            "Loading: ECRC11\n",
            "Error loading /content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/strains/ECRC11.npy: can only convert an array of size 1 to a Python scalar\n",
            "Loading: ECOR28\n",
            "Error loading /content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/strains/ECOR28.npy: can only convert an array of size 1 to a Python scalar\n",
            "Loading: ECRC16\n",
            "Error loading /content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/strains/ECRC16.npy: can only convert an array of size 1 to a Python scalar\n",
            "Loading: ECOR45\n",
            "Error loading /content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/strains/ECOR45.npy: can only convert an array of size 1 to a Python scalar\n",
            "Loading: ECOR48\n",
            "Error loading /content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/strains/ECOR48.npy: can only convert an array of size 1 to a Python scalar\n",
            "Loading: ECOR11\n",
            "Error loading /content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/strains/ECOR11.npy: can only convert an array of size 1 to a Python scalar\n",
            "Loading: 921A\n",
            "Error loading /content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/strains/921A.npy: can only convert an array of size 1 to a Python scalar\n",
            "Loading: ECOR72\n",
            "Error loading /content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/strains/ECOR72.npy: can only convert an array of size 1 to a Python scalar\n",
            "Loading: ECRC39\n",
            "Error loading /content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/strains/ECRC39.npy: can only convert an array of size 1 to a Python scalar\n",
            "Loading: ECRC3\n",
            "Error loading /content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/strains/ECRC3.npy: can only convert an array of size 1 to a Python scalar\n",
            "Loading: LMR3158\n",
            "Error loading /content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/strains/LMR3158.npy: can only convert an array of size 1 to a Python scalar\n",
            "Loading: NILS82\n",
            "Error loading /content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/strains/NILS82.npy: can only convert an array of size 1 to a Python scalar\n",
            "Loading: NILS44\n",
            "Error loading /content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/strains/NILS44.npy: can only convert an array of size 1 to a Python scalar\n",
            "Loading: NILS55\n",
            "Error loading /content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/strains/NILS55.npy: can only convert an array of size 1 to a Python scalar\n",
            "Loading: NILS57\n",
            "Error loading /content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/strains/NILS57.npy: can only convert an array of size 1 to a Python scalar\n",
            "Loading: NILS14\n",
            "Error loading /content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/strains/NILS14.npy: can only convert an array of size 1 to a Python scalar\n",
            "Loading: H48\n",
            "Error loading /content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/strains/H48.npy: can only convert an array of size 1 to a Python scalar\n",
            "Loading: ECRC7\n",
            "Error loading /content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/strains/ECRC7.npy: can only convert an array of size 1 to a Python scalar\n",
            "Loading: NILS68\n",
            "Error loading /content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/strains/NILS68.npy: can only convert an array of size 1 to a Python scalar\n",
            "Loading: NILS36\n",
            "Error loading /content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/strains/NILS36.npy: can only convert an array of size 1 to a Python scalar\n",
            "Loading: LF82\n",
            "Error loading /content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/strains/LF82.npy: can only convert an array of size 1 to a Python scalar\n",
            "Loading: IAI39\n",
            "Error loading /content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/strains/IAI39.npy: can only convert an array of size 1 to a Python scalar\n",
            "Loading: NILS43\n",
            "Error loading /content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/strains/NILS43.npy: can only convert an array of size 1 to a Python scalar\n",
            "Loading: NILS76\n",
            "Error loading /content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/strains/NILS76.npy: can only convert an array of size 1 to a Python scalar\n",
            "Loading: NILS22\n",
            "Error loading /content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/strains/NILS22.npy: can only convert an array of size 1 to a Python scalar\n",
            "Loading: MT1B1\n",
            "Error loading /content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/strains/MT1B1.npy: can only convert an array of size 1 to a Python scalar\n",
            "Loading: ECRC43\n",
            "Error loading /content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/strains/ECRC43.npy: can only convert an array of size 1 to a Python scalar\n",
            "Loading: NILS16\n",
            "Error loading /content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/strains/NILS16.npy: can only convert an array of size 1 to a Python scalar\n",
            "Loading: H4\n",
            "Error loading /content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/strains/H4.npy: can only convert an array of size 1 to a Python scalar\n",
            "Loading: NILS48\n",
            "Error loading /content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/strains/NILS48.npy: can only convert an array of size 1 to a Python scalar\n",
            "Loading: EDL933\n",
            "Error loading /content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/strains/EDL933.npy: can only convert an array of size 1 to a Python scalar\n",
            "Loading: ECRC18\n",
            "Error loading /content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/strains/ECRC18.npy: can only convert an array of size 1 to a Python scalar\n",
            "Loading: NILS41\n",
            "Error loading /content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/strains/NILS41.npy: can only convert an array of size 1 to a Python scalar\n",
            "Loading: ECRC34\n",
            "Error loading /content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/strains/ECRC34.npy: can only convert an array of size 1 to a Python scalar\n",
            "Loading: ECRC38\n",
            "Error loading /content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/strains/ECRC38.npy: can only convert an array of size 1 to a Python scalar\n",
            "Loading: NILS78\n",
            "Error loading /content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/strains/NILS78.npy: can only convert an array of size 1 to a Python scalar\n",
            "Loading: NILS09\n",
            "Error loading /content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/strains/NILS09.npy: can only convert an array of size 1 to a Python scalar\n",
            "Loading: IAI1\n",
            "Error loading /content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/strains/IAI1.npy: can only convert an array of size 1 to a Python scalar\n",
            "Loading: NILS13\n",
            "Error loading /content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/strains/NILS13.npy: can only convert an array of size 1 to a Python scalar\n",
            "Loading: M863\n",
            "Error loading /content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/strains/M863.npy: can only convert an array of size 1 to a Python scalar\n",
            "Loading: ROAR059\n",
            "Error loading /content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/strains/ROAR059.npy: can only convert an array of size 1 to a Python scalar\n",
            "Loading: NILS02\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pickle\n",
        "import os\n",
        "import random\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Tuple, Optional\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "def load_embeddings(embedding_dir: str) -> Dict[str, np.ndarray]:\n",
        "    \"\"\"Load all embeddings from a directory (.npy files).\"\"\"\n",
        "    embedding_dir = Path(embedding_dir)\n",
        "    embeddings = {}\n",
        "\n",
        "    for file_path in embedding_dir.glob('*.npy'):\n",
        "        identifier = file_path.stem  # filename without extension\n",
        "        print('Parsing: ', identifier)\n",
        "        embedding = np.load(file_path)\n",
        "        embeddings[identifier] = embedding\n",
        "\n",
        "    print(f\"Loaded {len(embeddings)} embeddings from {embedding_dir}\")\n",
        "    return embeddings\n",
        "\n",
        "def filter_interactions_by_strain(interactions_df: pd.DataFrame,\n",
        "                                random_state: int = 42) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "    \"\"\"Split interactions into train/test by strain.\"\"\"\n",
        "    unique_strains = interactions_df['strain'].unique()\n",
        "    train_strains, test_strains = train_test_split(\n",
        "        unique_strains,\n",
        "        test_size=0.2,\n",
        "        random_state=random_state\n",
        "    )\n",
        "\n",
        "    train_df = interactions_df[interactions_df['strain'].isin(train_strains)]\n",
        "    test_df = interactions_df[interactions_df['strain'].isin(test_strains)]\n",
        "\n",
        "    print(f\"Train set: {len(train_df)} interactions, {len(train_strains)} strains\")\n",
        "    print(f\"Test set:  {len(test_df)} interactions, {len(test_strains)} strains\")\n",
        "    return train_df, test_df\n",
        "\n",
        "class StrainPhageDataset(Dataset):\n",
        "    def __init__(self,\n",
        "                 interactions_df: pd.DataFrame,\n",
        "                 strain_embeddings: Dict[str, np.ndarray],\n",
        "                 phage_embeddings: Dict[str, np.ndarray]):\n",
        "        self.interactions = interactions_df.reset_index(drop=True)\n",
        "        self.strain_embeddings = strain_embeddings\n",
        "        self.phage_embeddings = phage_embeddings\n",
        "\n",
        "        # Check for missing embeddings\n",
        "        missing_strains = set(self.interactions['strain']) - set(strain_embeddings.keys())\n",
        "        missing_phages = set(self.interactions['phage']) - set(phage_embeddings.keys())\n",
        "\n",
        "        if missing_strains or missing_phages:\n",
        "            raise ValueError(\n",
        "                f\"Missing embeddings for {len(missing_strains)} strains \"\n",
        "                f\"and {len(missing_phages)} phages.\"\n",
        "            )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.interactions)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.interactions.iloc[idx]\n",
        "        # Get the array part of the tuple (idx 0)\n",
        "        strain_emb = torch.tensor(\n",
        "            self.strain_embeddings[row['strain']][0], dtype=torch.float32\n",
        "        )  # shape [n_s, 1280]\n",
        "        phage_emb = torch.tensor(\n",
        "            self.phage_embeddings[row['phage']][0], dtype=torch.float32\n",
        "        )   # shape [n_p, 1280]\n",
        "        label = torch.tensor(row['interaction'], dtype=torch.float32)\n",
        "        # Commenting out the strain/phage ID return values to match collate function\n",
        "        # return strain_emb, phage_emb, label, row['strain'], row['phage']\n",
        "        return strain_emb, phage_emb, label\n",
        "\n",
        "def collate_variable_sets(batch):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      batch: list of (strain_emb, phage_emb, label, strain_id, phage_id)\n",
        "        - strain_emb: shape [S, emb_dim]\n",
        "        - phage_emb:  shape [P, emb_dim]\n",
        "        - label: single scalar or shape (1,)\n",
        "        - strain_id: string identifier for strain\n",
        "        - phage_id: string identifier for phage\n",
        "\n",
        "    Returns:\n",
        "      strain_padded: (B, S_max, emb_dim)\n",
        "      phage_padded : (B, P_max, emb_dim)\n",
        "      strain_mask  : (B, S_max) boolean\n",
        "      phage_mask   : (B, P_max) boolean\n",
        "      label_batch  : (B, 1)\n",
        "      strain_ids   : list of strain identifiers\n",
        "      phage_ids    : list of phage identifiers\n",
        "    \"\"\"\n",
        "    # Unpack the batch\n",
        "    strains, phages, labels, strain_ids, phage_ids = zip(*batch)\n",
        "\n",
        "    # 1) Find max set sizes\n",
        "    max_strain_len = max(s.shape[0] for s in strains)\n",
        "    max_phage_len  = max(p.shape[0] for p in phages)\n",
        "\n",
        "    # 2) Emb dim (assume consistent)\n",
        "    emb_dim = strains[0].shape[1]\n",
        "    batch_size = len(batch)\n",
        "\n",
        "    # 3) Allocate zero-padded Tensors + boolean masks\n",
        "    strain_padded = torch.zeros(batch_size, max_strain_len, emb_dim, dtype=torch.float32)\n",
        "    phage_padded  = torch.zeros(batch_size, max_phage_len,  emb_dim, dtype=torch.float32)\n",
        "\n",
        "    strain_mask   = torch.zeros(batch_size, max_strain_len, dtype=torch.bool)\n",
        "    phage_mask    = torch.zeros(batch_size, max_phage_len,  dtype=torch.bool)\n",
        "\n",
        "    label_batch   = torch.zeros(batch_size, 1, dtype=torch.float32)\n",
        "\n",
        "    # 4) Copy each sample's data into padded Tensors\n",
        "    for i, (s_emb, p_emb, label) in enumerate(zip(strains, phages, labels)):\n",
        "        s_len = s_emb.shape[0]\n",
        "        p_len = p_emb.shape[0]\n",
        "\n",
        "        strain_padded[i, :s_len, :] = s_emb\n",
        "        phage_padded[i, :p_len, :]  = p_emb\n",
        "        strain_mask[i, :s_len]      = True\n",
        "        phage_mask[i,  :p_len]      = True\n",
        "\n",
        "        label_batch[i, 0] = float(label)\n",
        "\n",
        "    return strain_padded, phage_padded, strain_mask, phage_mask, label_batch, strain_ids, phage_ids\n",
        "\n",
        "def create_data_loaders(train_df: Optional[pd.DataFrame],\n",
        "                       test_df: pd.DataFrame,\n",
        "                       strain_embeddings: Dict[str, np.ndarray],\n",
        "                       phage_embeddings: Dict[str, np.ndarray],\n",
        "                       batch_size: int = 1) -> Tuple[DataLoader, DataLoader]:\n",
        "    \"\"\"Create data loaders for training and testing.\"\"\"\n",
        "    train_loader = None\n",
        "    if train_df is not None and not train_df.empty:\n",
        "        train_dataset = StrainPhageDataset(train_df, strain_embeddings, phage_embeddings)\n",
        "        train_loader = DataLoader(\n",
        "            train_dataset,\n",
        "            batch_size=batch_size,\n",
        "            shuffle=True,\n",
        "            num_workers=0,\n",
        "            pin_memory=True,\n",
        "            collate_fn=collate_variable_sets\n",
        "        )\n",
        "\n",
        "    test_dataset = StrainPhageDataset(test_df, strain_embeddings, phage_embeddings)\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=0,\n",
        "        pin_memory=True,\n",
        "        collate_fn=collate_variable_sets\n",
        "    )\n",
        "\n",
        "    return train_loader, test_loader\n",
        "\n",
        "class AttentionVisualizer:\n",
        "    \"\"\"Tools for visualizing attention patterns in the strain-phage transformer.\"\"\"\n",
        "\n",
        "    def __init__(self, model, device):\n",
        "        self.model = model\n",
        "        self.device = device\n",
        "        self.model.eval()\n",
        "\n",
        "    def get_attention_maps(self,\n",
        "                         strain_emb: torch.Tensor,\n",
        "                         phage_emb: torch.Tensor,\n",
        "                         strain_mask: Optional[torch.Tensor] = None,\n",
        "                         phage_mask: Optional[torch.Tensor] = None) -> Tuple[np.ndarray, np.ndarray]:\n",
        "        with torch.no_grad():\n",
        "            _, (strain_attn, phage_attn) = self.model(\n",
        "                strain_emb.to(self.device),\n",
        "                phage_emb.to(self.device),\n",
        "                strain_mask.to(self.device) if strain_mask is not None else None,\n",
        "                phage_mask.to(self.device) if phage_mask is not None else None,\n",
        "                return_attn=True\n",
        "            )\n",
        "\n",
        "            strain_to_phage_attn = strain_attn.cpu().numpy().squeeze(0)\n",
        "            phage_to_strain_attn = phage_attn.cpu().numpy().squeeze(0)\n",
        "\n",
        "            return strain_to_phage_attn, phage_to_strain_attn\n",
        "\n",
        "    def plot_attention_heatmap(self,\n",
        "                             attention_matrix: np.ndarray,\n",
        "                             head_idx: int,\n",
        "                             x_labels: List[str],\n",
        "                             y_labels: List[str],\n",
        "                             title: str = \"Attention Weights\",\n",
        "                             figsize: Tuple[int, int] = (10, 8)):\n",
        "        attn_weights = attention_matrix[head_idx]\n",
        "        plt.figure(figsize=figsize)\n",
        "        sns.heatmap(attn_weights,\n",
        "                   xticklabels=x_labels,\n",
        "                   yticklabels=y_labels,\n",
        "                   cmap='viridis',\n",
        "                   cbar_kws={'label': 'Attention Weight'})\n",
        "        plt.title(f\"{title} (Head {head_idx})\")\n",
        "        plt.xticks(rotation=45, ha='right')\n",
        "        plt.yticks(rotation=0)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def plot_aggregated_attention(self,\n",
        "                                attention_matrix: np.ndarray,\n",
        "                                x_labels: List[str],\n",
        "                                y_labels: List[str],\n",
        "                                aggregation: str = 'mean',\n",
        "                                title: str = \"Aggregated Attention Weights\",\n",
        "                                figsize: Tuple[int, int] = (10, 8)):\n",
        "        if aggregation == 'mean':\n",
        "            attn_weights = attention_matrix.mean(axis=0)\n",
        "        elif aggregation == 'max':\n",
        "            attn_weights = attention_matrix.max(axis=0)\n",
        "        else:\n",
        "            raise ValueError(\"aggregation must be 'mean' or 'max'\")\n",
        "\n",
        "        plt.figure(figsize=figsize)\n",
        "        sns.heatmap(attn_weights,\n",
        "                   xticklabels=x_labels,\n",
        "                   yticklabels=y_labels,\n",
        "                   cmap='viridis',\n",
        "                   cbar_kws={'label': 'Attention Weight'})\n",
        "        plt.title(f\"{title} ({aggregation} across heads)\")\n",
        "        plt.xticks(rotation=45, ha='right')\n",
        "        plt.yticks(rotation=0)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def plot_attention_patterns(self,\n",
        "                              strain_emb: torch.Tensor,\n",
        "                              phage_emb: torch.Tensor,\n",
        "                              strain_genes: List[str],\n",
        "                              phage_genes: List[str],\n",
        "                              strain_mask: Optional[torch.Tensor] = None,\n",
        "                              phage_mask: Optional[torch.Tensor] = None):\n",
        "        strain_to_phage_attn, phage_to_strain_attn = self.get_attention_maps(\n",
        "            strain_emb, phage_emb, strain_mask, phage_mask\n",
        "        )\n",
        "\n",
        "        n_heads = strain_to_phage_attn.shape[0]\n",
        "\n",
        "        for head in range(n_heads):\n",
        "            self.plot_attention_heatmap(\n",
        "                strain_to_phage_attn,\n",
        "                head,\n",
        "                phage_genes,\n",
        "                strain_genes,\n",
        "                title=\"Strain-to-Phage Attention\"\n",
        "            )\n",
        "\n",
        "        for head in range(n_heads):\n",
        "            self.plot_attention_heatmap(\n",
        "                phage_to_strain_attn,\n",
        "                head,\n",
        "                strain_genes,\n",
        "                phage_genes,\n",
        "                title=\"Phage-to-Strain Attention\"\n",
        "            )\n",
        "\n",
        "        self.plot_aggregated_attention(\n",
        "            strain_to_phage_attn,\n",
        "            phage_genes,\n",
        "            strain_genes,\n",
        "            title=\"Aggregated Strain-to-Phage Attention\"\n",
        "        )\n",
        "\n",
        "        self.plot_aggregated_attention(\n",
        "            phage_to_strain_attn,\n",
        "            strain_genes,\n",
        "            phage_genes,\n",
        "            title=\"Aggregated Phage-to-Strain Attention\"\n",
        "        )\n",
        "\n",
        "    def analyze_important_genes(self,\n",
        "                              attention_matrix: np.ndarray,\n",
        "                              gene_names: List[str],\n",
        "                              top_k: int = 5,\n",
        "                              aggregation: str = 'mean') -> List[Tuple[str, float]]:\n",
        "        if aggregation == 'mean':\n",
        "            scores = attention_matrix.mean(axis=(0, 1))\n",
        "        elif aggregation == 'max':\n",
        "            scores = attention_matrix.max(axis=(0, 1))\n",
        "        else:\n",
        "            raise ValueError(\"aggregation must be 'mean' or 'max'\")\n",
        "\n",
        "        top_indices = np.argsort(scores)[-top_k:][::-1]\n",
        "        top_genes = [(gene_names[i], scores[i]) for i in top_indices]\n",
        "        return top_genes\n",
        "\n",
        "def load_and_analyze_attention(\n",
        "    output_path: str,\n",
        "    data_dir: str,\n",
        "    device: str = 'cuda',\n",
        "    num_samples: int = 5,\n",
        "    debug: bool = False\n",
        "):\n",
        "    \"\"\"Load saved model and perform attention analysis.\"\"\"\n",
        "    print(f\"Loading outputs from {output_path}\")\n",
        "    with open(output_path, 'rb') as f:\n",
        "        outputs = pickle.load(f)\n",
        "\n",
        "    model = outputs['model']\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    print(\"Loading embeddings...\")\n",
        "    strain_embeddings = load_embeddings_with_ids(os.path.join(data_dir, 'embedding_sets/strains/'))\n",
        "    phage_embeddings = load_embeddings_with_ids(os.path.join(data_dir, 'embedding_sets/phages/'))\n",
        "\n",
        "    print(\"Loading interaction data...\")\n",
        "    interactions_df = pd.read_csv(os.path.join(data_dir, 'EDGE_interaction_long_172_no2.csv'))\n",
        "\n",
        "    # Filter interactions_df before splitting\n",
        "    strain_keys = set(strain_embeddings.keys())\n",
        "    phage_keys = set(phage_embeddings.keys())\n",
        "    interactions_df = interactions_df[\n",
        "        interactions_df['strain'].isin(strain_keys) &\n",
        "        interactions_df['phage'].isin(phage_keys)\n",
        "    ]\n",
        "\n",
        "    if debug:\n",
        "        print(\"Running in debug mode with reduced dataset...\")\n",
        "        random_strains = random.sample(list(strain_keys), 50)\n",
        "        random_phages = random.sample(list(phage_keys), 50)\n",
        "        interactions_df = interactions_df[\n",
        "            interactions_df['strain'].isin(random_strains) &\n",
        "            interactions_df['phage'].isin(random_phages)\n",
        "        ]\n",
        "\n",
        "    print(\"Splitting data...\")\n",
        "    train_df, test_df = filter_interactions_by_strain(interactions_df)\n",
        "\n",
        "    print(\"Creating data loader...\")\n",
        "    _, test_loader = create_data_loaders(\n",
        "        None, test_df, strain_embeddings, phage_embeddings, batch_size=1\n",
        "    )\n",
        "\n",
        "    visualizer = AttentionVisualizer(model, device)\n",
        "\n",
        "    print(f\"\\nAnalyzing {num_samples} samples...\")\n",
        "    for i, batch in enumerate(test_loader):\n",
        "        if i >= num_samples:\n",
        "            break\n",
        "\n",
        "        # Unpack the batch with all 7 elements\n",
        "        strain_emb, phage_emb, strain_mask, phage_mask, label, strain_ids, phage_ids = batch\n",
        "\n",
        "        # Get individual values since batch_size=1\n",
        "        strain_id = strain_ids[0]\n",
        "        phage_id = phage_ids[0]\n",
        "\n",
        "        print(f\"\\nAnalyzing sample {i+1} (Label: {label.item():.0f})\")\n",
        "        print(f\"Strain: {strain_id}, Phage: {phage_id}\")\n",
        "\n",
        "        strain_to_phage_attn, phage_to_strain_attn = visualizer.get_attention_maps(\n",
        "            strain_emb, phage_emb, strain_mask, phage_mask\n",
        "        )\n",
        "\n",
        "        # Get gene IDs from the embeddings\n",
        "        strain_genes = strain_embeddings[strain_id][1]  # Gene ID list\n",
        "        phage_genes = phage_embeddings[phage_id][1]     # Gene ID list\n",
        "\n",
        "        print(\"\\nPlotting attention patterns...\")\n",
        "        visualizer.plot_attention_patterns(\n",
        "            strain_emb,\n",
        "            phage_emb,\n",
        "            strain_genes,\n",
        "            phage_genes,\n",
        "            strain_mask,\n",
        "            phage_mask\n",
        "        )\n",
        "\n",
        "        print(\"\\nAnalyzing important genes...\")\n",
        "        top_phage_genes = visualizer.analyze_important_genes(\n",
        "            strain_to_phage_attn,\n",
        "            phage_genes,\n",
        "            top_k=5\n",
        "        )\n",
        "        print(\"Most attended-to phage genes:\")\n",
        "        for gene, score in top_phage_genes:\n",
        "            print(f\"{gene}: {score:.3f}\")\n",
        "\n",
        "        top_strain_genes = visualizer.analyze_important_genes(\n",
        "            phage_to_strain_attn,\n",
        "            strain_genes,\n",
        "            top_k=5\n",
        "        )\n",
        "        print(\"\\nMost attended-to strain genes:\")\n",
        "        for gene, score in top_strain_genes:\n",
        "            print(f\"{gene}: {score:.3f}\")\n",
        "\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Import required for train_test_split\n",
        "    from sklearn.model_selection import train_test_split\n",
        "\n",
        "    # Set paths\n",
        "    data_dir = '/content/drive/MyDrive/Arkin/set_transformer_data'\n",
        "    output_path = os.path.join(data_dir, 'outputs/outputs/main_function_outputs_ecoli_1024_5_2layer.pkl')\n",
        "\n",
        "    # Set device\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Run analysis\n",
        "    load_and_analyze_attention(\n",
        "        output_path=output_path,\n",
        "        data_dir=data_dir,\n",
        "        device=device,\n",
        "        num_samples=5,  # Adjust this number to analyze more or fewer samples\n",
        "        debug=False     # Set to True to run with reduced dataset\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RqURyca9BJJC"
      },
      "id": "RqURyca9BJJC",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "version": "3.9"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}