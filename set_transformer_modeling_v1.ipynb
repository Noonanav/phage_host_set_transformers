{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Noonanav/phage_host_set_transformers/blob/main/set_transformer_modeling_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bcvLxj3ltRqd",
      "metadata": {
        "id": "bcvLxj3ltRqd"
      },
      "source": [
        "# Set Transformer for Strain-Phage Interaction Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "VH3zarJ2tRqe",
      "metadata": {
        "id": "VH3zarJ2tRqe"
      },
      "source": [
        "## Block 1: Import Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "lpJZMZIStRqe",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lpJZMZIStRqe",
        "outputId": "95471357-4397-439f-aff2-fdb9be8f9a89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import Dict, List, Tuple, Optional\n",
        "import warnings\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import random\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, average_precision_score, confusion_matrix\n",
        "from sklearn.metrics import roc_curve, precision_recall_curve, auc\n",
        "import seaborn as sns\n",
        "import pickle\n",
        "import os\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "bKbBEmcJ8r7S",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bKbBEmcJ8r7S",
        "outputId": "b2fb49d0-331a-4588-f236-e96230c103a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_w0bWlDjtRqe",
      "metadata": {
        "id": "_w0bWlDjtRqe"
      },
      "source": [
        "# Block 2: Data Loading and Preprocessing\n",
        "Description: This block handles the loading of strain and phage embeddings, splitting the interactions into train and test sets, creating datasets and data loaders, and managing variable-length inputs.\n",
        "\n",
        "## Hyperparameters and Architectural Decisions\n",
        "### `filter_interactions_by_strain` function:\n",
        "- `test_size`: Proportion of strains used for testing.\n",
        "  - Current value: 0.2\n",
        "  - Possible values to try: 0.1, 0.15, 0.25, 0.3\n",
        "\n",
        "### `create_data_loaders` function:  \n",
        "- `batch_size`: Number of samples per batch during training and testing.\n",
        "  - Current value: 1\n",
        "  - Possible values to try: 2, 4, 8, 16, 32\n",
        "- `num_workers`: Number of parallel workers for data loading.\n",
        "  - Current value: 0\n",
        "  - Possible values to try: 2, 4, 8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "zA1vfH63tRqe",
      "metadata": {
        "id": "zA1vfH63tRqe"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "def load_embeddings(embedding_dir: str) -> Dict[str, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Load all embeddings from a directory (.npy files).\n",
        "    Each file should contain shape [num_genes, 1280].\n",
        "    \"\"\"\n",
        "    embedding_dir = Path(embedding_dir)\n",
        "    embeddings = {}\n",
        "\n",
        "    for file_path in embedding_dir.glob('*.npy'):\n",
        "        identifier = file_path.stem  # filename without extension\n",
        "        print('Parsing: ', identifier)\n",
        "        embedding = np.load(file_path)\n",
        "        embeddings[identifier] = embedding\n",
        "\n",
        "    print(f\"Loaded {len(embeddings)} embeddings from {embedding_dir}\")\n",
        "    return embeddings\n",
        "\n",
        "def filter_interactions_by_strain(interactions_df: pd.DataFrame,\n",
        "                                  random_state: int = 42) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Split interactions into train/test by strain.\n",
        "    \"\"\"\n",
        "    unique_strains = interactions_df['strain'].unique()\n",
        "    train_strains, test_strains = train_test_split(\n",
        "        unique_strains,\n",
        "        test_size=0.2,\n",
        "        random_state=random_state\n",
        "    )\n",
        "\n",
        "    train_df = interactions_df[interactions_df['strain'].isin(train_strains)]\n",
        "    test_df = interactions_df[interactions_df['strain'].isin(test_strains)]\n",
        "\n",
        "    print(f\"Train set: {len(train_df)} interactions, {len(train_strains)} strains\")\n",
        "    print(f\"Test set:  {len(test_df)} interactions, {len(test_strains)} strains\")\n",
        "    return train_df, test_df\n",
        "\n",
        "class StrainPhageDataset(Dataset):\n",
        "    def __init__(self,\n",
        "                 interactions_df: pd.DataFrame,\n",
        "                 strain_embeddings: Dict[str, np.ndarray],\n",
        "                 phage_embeddings: Dict[str, np.ndarray]):\n",
        "        \"\"\"\n",
        "        Dataset for strain-phage interactions.\n",
        "        interactions_df has columns: ['strain', 'phage', 'interaction'].\n",
        "        \"\"\"\n",
        "        self.interactions = interactions_df.reset_index(drop=True)\n",
        "        self.strain_embeddings = strain_embeddings\n",
        "        self.phage_embeddings = phage_embeddings\n",
        "\n",
        "        # Check for missing embeddings\n",
        "        missing_strains = set(self.interactions['strain']) - set(strain_embeddings.keys())\n",
        "        missing_phages = set(self.interactions['phage']) - set(phage_embeddings.keys())\n",
        "\n",
        "        if missing_strains or missing_phages:\n",
        "            raise ValueError(\n",
        "                f\"Missing embeddings for {len(missing_strains)} strains \"\n",
        "                f\"and {len(missing_phages)} phages.\"\n",
        "            )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.interactions)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.interactions.iloc[idx]\n",
        "        strain_emb = torch.tensor(\n",
        "            self.strain_embeddings[row['strain']], dtype=torch.float32\n",
        "        )  # shape [n_s, 1280]\n",
        "        phage_emb = torch.tensor(\n",
        "            self.phage_embeddings[row['phage']], dtype=torch.float32\n",
        "        )   # shape [n_p, 1280]\n",
        "        label = torch.tensor(row['interaction'], dtype=torch.float32)\n",
        "        return strain_emb, phage_emb, label\n",
        "\n",
        "def collate_variable_sets(batch):\n",
        "    \"\"\"\n",
        "    Collate function to simply stack each list of embeddings\n",
        "    into a batch dimension.\n",
        "    NOTE: If the sets truly vary in length,\n",
        "    torch.stack(...) will fail unless they match or you pad them.\n",
        "    For now, we assume the user can handle variable-size\n",
        "    embeddings at runtime or that all sets in a single batch\n",
        "    have consistent shapes.\n",
        "    \"\"\"\n",
        "    strains, phages, labels = zip(*batch)\n",
        "    # Each item in 'strains' is shape [n_s, 1280], etc.\n",
        "\n",
        "    # If all n_s differ, stacking might fail.\n",
        "    # For a quick fix, you could feed batch_size=1\n",
        "    # or do custom padding.\n",
        "    # We'll assume they can be stacked.\n",
        "    return (\n",
        "        torch.stack(strains, dim=0),\n",
        "        torch.stack(phages, dim=0),\n",
        "        torch.tensor(labels, dtype=torch.float32).unsqueeze(1)  # Reshape to [B, 1]\n",
        "    )\n",
        "\n",
        "def create_data_loaders(train_df: Optional[pd.DataFrame],\n",
        "                            test_df: pd.DataFrame,\n",
        "                            strain_embeddings: Dict[str, np.ndarray],\n",
        "                            phage_embeddings: Dict[str, np.ndarray],\n",
        "                            batch_size: int = 1) -> Tuple[DataLoader, DataLoader]:\n",
        "\n",
        "        train_loader = None # Initialize train_loader as None\n",
        "        if train_df is not None and not train_df.empty: # add a check before proceeding:\n",
        "            train_dataset = StrainPhageDataset(train_df, strain_embeddings, phage_embeddings)\n",
        "            train_loader = DataLoader(\n",
        "                train_dataset,\n",
        "                batch_size=batch_size,\n",
        "                shuffle=True,\n",
        "                num_workers=0,\n",
        "                pin_memory=True,\n",
        "                collate_fn=collate_variable_sets\n",
        "            )\n",
        "\n",
        "        test_dataset  = StrainPhageDataset(test_df, strain_embeddings, phage_embeddings)\n",
        "        test_loader = DataLoader(\n",
        "            test_dataset,\n",
        "            batch_size=batch_size,\n",
        "            shuffle=False,\n",
        "            num_workers=0,\n",
        "            pin_memory=True,\n",
        "            collate_fn=collate_variable_sets\n",
        "        )\n",
        "\n",
        "        return train_loader, test_loader # now, it can return None for `train_loader`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xVpPLTqbtRqf",
      "metadata": {
        "id": "xVpPLTqbtRqf"
      },
      "source": [
        "# Block 3: Set Transformer Blocks\n",
        "Description: This block implements the core components of the Set Transformer architecture, including the Multi-head Attention Block (MAB), Induced Set Attention Block (ISAB), and Pooling by Multihead Attention (PMA).\n",
        "\n",
        "## Hyperparameters and Architectural Decisions\n",
        "### `MAB` class:\n",
        "- `num_heads`: Number of attention heads in the multi-head attention mechanism.\n",
        "  - Current value: Determined by the `num_heads` parameter passed to the `ISAB` and `PMA` classes\n",
        "  - Possible values to try: 4, 8, 12, 16\n",
        "- `ln`: Whether to apply layer normalization after the attention operation.\n",
        "  - Current value: False\n",
        "  - Possible values to try: True\n",
        "\n",
        "### `ISAB` class:\n",
        "- `num_inds`: Number of inducing points used to reduce computational complexity.\n",
        "  - Current value: Determined by the `strain_inds` and `phage_inds` parameters in the `StrainPhageTransformer` class\n",
        "    - For the strain encoder: `strain_inds=128`\n",
        "    - For the phage encoder: `phage_inds=64`\n",
        "  - Possible values to try: 32, 64, 96, 128\n",
        "- `ln`: Whether to apply layer normalization.\n",
        "  - Current value: False\n",
        "  - Possible values to try: True\n",
        "\n",
        "### `PMA` class:\n",
        "- `num_heads`: Number of attention heads in the pooling mechanism.\n",
        "  - Current value: Determined by the `num_heads` parameter passed to the `StrainPhageTransformer` class\n",
        "  - Possible values to try: 4, 8, 12, 16  \n",
        "- `num_seeds`: Number of output vectors after pooling.\n",
        "  - Current value: 1\n",
        "  - Possible values to try: 2, 4, 8\n",
        "- `ln`: Whether to apply layer normalization.  \n",
        "  - Current value: False\n",
        "  - Possible values to try: True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "kHIKiwyrtRqf",
      "metadata": {
        "id": "kHIKiwyrtRqf"
      },
      "outputs": [],
      "source": [
        "class MAB(nn.Module):\n",
        "    \"\"\"Multi-head Attention Block.\"\"\"\n",
        "    def __init__(self, dim_Q, dim_K, dim_V, num_heads, ln=False):\n",
        "        super(MAB, self).__init__()\n",
        "        self.dim_V = dim_V\n",
        "        self.num_heads = num_heads\n",
        "        self.fc_q = nn.Linear(dim_Q, dim_V)\n",
        "        self.fc_k = nn.Linear(dim_K, dim_V)\n",
        "        self.fc_v = nn.Linear(dim_K, dim_V)\n",
        "        self.fc_o = nn.Linear(dim_V, dim_V)\n",
        "        self.ln = nn.LayerNorm(dim_V) if ln else nn.Identity()\n",
        "\n",
        "    def forward(self, Q, K, return_attn=False):\n",
        "        Q_ = self.fc_q(Q)\n",
        "        K_ = self.fc_k(K)\n",
        "        V_ = self.fc_v(K)\n",
        "\n",
        "        dim_split = self.dim_V // self.num_heads\n",
        "        B, n_q, _ = Q_.shape\n",
        "        n_k = K_.size(1)\n",
        "\n",
        "        Q_ = Q_.view(B, n_q, self.num_heads, dim_split).transpose(1, 2)\n",
        "        K_ = K_.view(B, n_k, self.num_heads, dim_split).transpose(1, 2)\n",
        "        V_ = V_.view(B, n_k, self.num_heads, dim_split).transpose(1, 2)\n",
        "\n",
        "        A = torch.matmul(Q_, K_.transpose(-2, -1)) / np.sqrt(dim_split)\n",
        "        A = torch.softmax(A, dim=-1)  # [B, h, n_q, n_k]\n",
        "\n",
        "        O = torch.matmul(A, V_)  # [B, h, n_q, d_h]\n",
        "        O = O.transpose(1, 2).contiguous().view(B, n_q, self.dim_V)\n",
        "        O = self.ln(O + F.relu(self.fc_o(O)))\n",
        "\n",
        "        if return_attn:\n",
        "            # Return both output and attention\n",
        "            return O, A\n",
        "        else:\n",
        "            return O\n",
        "\n",
        "class ISAB(nn.Module):\n",
        "    \"\"\"\n",
        "    Induced Set Attention Block.\n",
        "    Reduces complexity by using M learned 'inducing' vectors.\n",
        "    \"\"\"\n",
        "    def __init__(self, dim_in, dim_out, num_heads, num_inds, ln=False):\n",
        "        super(ISAB, self).__init__()\n",
        "        self.num_inds = num_inds\n",
        "        self.I = nn.Parameter(torch.Tensor(1, num_inds, dim_out))\n",
        "        nn.init.xavier_uniform_(self.I)\n",
        "\n",
        "        self.mab1 = MAB(dim_out, dim_in, dim_out, num_heads, ln=ln)\n",
        "        self.mab2 = MAB(dim_in, dim_out, dim_out, num_heads, ln=ln)\n",
        "\n",
        "    def forward(self, X):\n",
        "        # 1) Inducing points attend to input\n",
        "        #    I ~ [1, M, d_out], repeat to [B, M, d_out]\n",
        "        H = self.mab1(self.I.repeat(X.size(0),1,1), X)  # [B, M, d_out]\n",
        "        # 2) Input attends back to induced\n",
        "        H = self.mab2(X, H)                             # [B, n, d_out]\n",
        "        return H\n",
        "\n",
        "class PMA(nn.Module):\n",
        "    \"\"\"\n",
        "    Pooling by Multihead Attention to reduce a set to num_seeds outputs.\n",
        "    By default, set num_seeds=1 to get a single vector.\n",
        "    \"\"\"\n",
        "    def __init__(self, dim, num_heads, num_seeds=1, ln=False):\n",
        "        super(PMA, self).__init__()\n",
        "        self.num_seeds = num_seeds\n",
        "        self.S = nn.Parameter(torch.Tensor(1, num_seeds, dim))\n",
        "        nn.init.xavier_uniform_(self.S)\n",
        "        self.mab = MAB(dim, dim, dim, num_heads, ln=ln)\n",
        "\n",
        "    def forward(self, X):\n",
        "        # X: [B, n, d]\n",
        "        # seeds: [1, num_seeds, d]\n",
        "        B = X.size(0)\n",
        "        S = self.S.repeat(B, 1, 1)  # [B, num_seeds, d]\n",
        "        return self.mab(S, X)       # [B, num_seeds, d]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fHd602MPtRqf",
      "metadata": {
        "id": "fHd602MPtRqf"
      },
      "source": [
        "# Block 4: Cross-Attention  \n",
        "Description: This block implements the cross-attention mechanism, allowing strain embeddings to attend to phage embeddings and vice versa.\n",
        "\n",
        "## Hyperparameters and Architectural Decisions\n",
        "### `CrossAttention` class:\n",
        "- `num_heads`: Number of attention heads in the cross-attention mechanism.\n",
        "  - Current value: Determined by the `num_heads` parameter passed to the `StrainPhageTransformer` class\n",
        "  - Possible values to try: 4, 8, 12, 16\n",
        "- `ln`: Whether to apply layer normalization.\n",
        "  - Current value: False\n",
        "  - Possible values to try: True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "y471SGL8tRqg",
      "metadata": {
        "id": "y471SGL8tRqg"
      },
      "outputs": [],
      "source": [
        "class CrossAttention(nn.Module):\n",
        "    \"\"\"Cross-attention between two sets.\"\"\"\n",
        "    def __init__(self, dim, num_heads, ln=False):\n",
        "        super(CrossAttention, self).__init__()\n",
        "        self.mab = MAB(dim, dim, dim, num_heads, ln=ln)\n",
        "\n",
        "    def forward(self, X, Y, return_attn=False):\n",
        "        # Pass the return_attn flag to MAB\n",
        "        return self.mab(X, Y, return_attn=return_attn)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fM2C1Sl7tRqg",
      "metadata": {
        "id": "fM2C1Sl7tRqg"
      },
      "source": [
        "# Block 5: Encoders and Complete Model\n",
        "Description: This block defines the set encoders for strain and phage embeddings and the complete `StrainPhageTransformer` model, which combines the set encoders, cross-attention blocks, pooling, and classification layers.\n",
        "\n",
        "## Hyperparameters and Architectural Decisions\n",
        "### `SetEncoder` class:\n",
        "- `num_heads`: Number of attention heads in the ISAB layers.\n",
        "  - Current value: Determined by the `num_heads` parameter passed to the `StrainPhageTransformer` class\n",
        "  - Possible values to try: 4, 8, 12, 16\n",
        "- `num_inds`: Number of inducing points used in the ISAB layers.\n",
        "  - Current value: Determined by the `strain_inds` and `phage_inds` parameters in the `StrainPhageTransformer` class\n",
        "    - For the strain encoder: `strain_inds=128`\n",
        "    - For the phage encoder: `phage_inds=64`\n",
        "  - Possible values to try: 32, 64, 96, 128\n",
        "- `ln`: Whether to apply layer normalization.\n",
        "  - Current value: False\n",
        "  - Possible values to try: True\n",
        "\n",
        "### `StrainPhageTransformer` class:\n",
        "- `embedding_dim`: Dimension of the input embeddings.\n",
        "  - Current value: 1280\n",
        "  - Possible values to try: 512, 768, 1024 (depending on the embedding model)\n",
        "- `num_heads`: Number of attention heads in the cross-attention blocks and PMA layers.\n",
        "  - Current value: 8\n",
        "  - Possible values to try: 4, 12, 16\n",
        "- `strain_inds`: Number of inducing points used in the strain encoder.\n",
        "  - Current value: 128\n",
        "  - Possible values to try: 64, 96, 160, 192  \n",
        "- `phage_inds`: Number of inducing points used in the phage encoder.\n",
        "  - Current value: 64\n",
        "  - Possible values to try: 32, 48, 80, 96\n",
        "- `dropout`: Dropout rate applied in the classifier.\n",
        "  - Current value: 0.1\n",
        "  - Possible values to try: 0.0, 0.2, 0.3, 0.4\n",
        "- `ln`: Whether to apply layer normalization in the set encoders, cross-attention blocks, PMA layers, and classifier.\n",
        "  - Current value: True\n",
        "  - Possible values to try: False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "TwbLmM_vtRqg",
      "metadata": {
        "id": "TwbLmM_vtRqg"
      },
      "outputs": [],
      "source": [
        "class SetEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Encodes a set of embeddings into a refined set\n",
        "    using ISAB layers.\n",
        "    NOTE: We do NOT pool down to a single token here.\n",
        "          That happens after cross-attention.\n",
        "    \"\"\"\n",
        "    def __init__(self, dim_input, dim_output, num_heads, num_inds, ln=False):\n",
        "        super(SetEncoder, self).__init__()\n",
        "        self.isab1 = ISAB(dim_input, dim_output, num_heads, num_inds, ln=ln)\n",
        "        self.isab2 = ISAB(dim_output, dim_output, num_heads, num_inds, ln=ln)\n",
        "\n",
        "    def forward(self, X):\n",
        "        H = self.isab1(X)  # [B, n, d]\n",
        "        H = self.isab2(H)  # [B, n, d]\n",
        "        return H\n",
        "\n",
        "\n",
        "class StrainPhageTransformer(nn.Module):\n",
        "    \"\"\"\n",
        "    1) Encode strain genes and phage genes via ISAB stacks.\n",
        "    2) Cross-attend them at the gene level.\n",
        "    3) Pool each cross-attended set to a single vector via PMA.\n",
        "    4) Concatenate and classify.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 embedding_dim=1280,\n",
        "                 num_heads=8,\n",
        "                 strain_inds=128,\n",
        "                 phage_inds=64,\n",
        "                 dropout=0.1,\n",
        "                 ln=True):\n",
        "        super().__init__()\n",
        "\n",
        "        # 1) Set Transformer encoders (no final pooling)\n",
        "        self.strain_encoder = SetEncoder(\n",
        "            dim_input=embedding_dim,\n",
        "            dim_output=embedding_dim,\n",
        "            num_heads=num_heads,\n",
        "            num_inds=strain_inds,\n",
        "            ln=ln\n",
        "        )\n",
        "        self.phage_encoder = SetEncoder(\n",
        "            dim_input=embedding_dim,\n",
        "            dim_output=embedding_dim,\n",
        "            num_heads=num_heads,\n",
        "            num_inds=phage_inds,\n",
        "            ln=ln\n",
        "        )\n",
        "\n",
        "        # 2) Cross-attention blocks\n",
        "        self.strain_to_phage = CrossAttention(embedding_dim, num_heads, ln=ln)\n",
        "        self.phage_to_strain = CrossAttention(embedding_dim, num_heads, ln=ln)\n",
        "\n",
        "        # 3) PMA for final pooling to a single vector\n",
        "        self.strain_pma = PMA(embedding_dim, num_heads, num_seeds=1, ln=ln)\n",
        "        self.phage_pma  = PMA(embedding_dim, num_heads, num_seeds=1, ln=ln)\n",
        "\n",
        "        # 4) Final classifier\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(embedding_dim * 2, embedding_dim),\n",
        "            nn.LayerNorm(embedding_dim) if ln else nn.Identity(),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(embedding_dim, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, strain_genes, phage_genes, return_attn=False):\n",
        "        \"\"\"\n",
        "        strain_genes: [B, n_s, 1280]\n",
        "        phage_genes:  [B, n_p, 1280]\n",
        "        return_attn : if True, returns (logits, (strain_attn, phage_attn)) for interpretability\n",
        "        \"\"\"\n",
        "        # 1) Encode each genome with set encoders\n",
        "        # print(\"strain_genes shape:\", strain_genes.shape)\n",
        "        # print(\"phage_genes shape:\", phage_genes.shape)\n",
        "        strain_enc = self.strain_encoder(strain_genes)  # [B, n_s, d]\n",
        "        phage_enc  = self.phage_encoder(phage_genes)    # [B, n_p, d]\n",
        "\n",
        "        # print(\"strain_enc shape:\", strain_enc.shape)\n",
        "        # print(\"phage_enc shape:\", phage_enc.shape)\n",
        "\n",
        "        # 2) Cross-attention\n",
        "        if return_attn:\n",
        "            strain_attended, strain_attn = self.strain_to_phage(strain_enc, phage_enc, return_attn=True)\n",
        "            phage_attended,  phage_attn  = self.phage_to_strain(phage_enc, strain_enc, return_attn=True)\n",
        "        else:\n",
        "            strain_attended = self.strain_to_phage(strain_enc, phage_enc)\n",
        "            phage_attended  = self.phage_to_strain(phage_enc, strain_enc)\n",
        "            strain_attn = None\n",
        "            phage_attn  = None\n",
        "\n",
        "        # 3) Pool each cross-attended set\n",
        "        strain_pooled = self.strain_pma(strain_attended)  # [B, 1, d]\n",
        "        phage_pooled  = self.phage_pma(phage_attended)     # [B, 1, d]\n",
        "\n",
        "        strain_vec = strain_pooled.squeeze(1)  # [B, d]\n",
        "        phage_vec  = phage_pooled.squeeze(1)   # [B, d]\n",
        "\n",
        "        # 4) Classifier\n",
        "        combined = torch.cat([strain_vec, phage_vec], dim=-1)  # [B, 2*d]\n",
        "        logits   = self.classifier(combined)                   # [B, 1]\n",
        "\n",
        "        # print(\"logits shape:\", logits.shape)  # Add this line\n",
        "\n",
        "        # Return attention if requested\n",
        "        if return_attn:\n",
        "            return logits, (strain_attn, phage_attn)\n",
        "        else:\n",
        "            return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "nkthkAkutRqg",
      "metadata": {
        "id": "nkthkAkutRqg"
      },
      "source": [
        "# Block 6: Training Functions\n",
        "Description: This block contains the functions for training the model, including the training loop, validation, early stopping, and visualization of training history.\n",
        "\n",
        "## Hyperparameters and Architectural Decisions\n",
        "### `train_model` function:\n",
        "- `num_epochs`: Maximum number of training epochs.\n",
        "  - Current value: 100\n",
        "  - Possible values to try: 50, 150, 200\n",
        "- `learning_rate`: Learning rate used by the optimizer.\n",
        "  - Current value: 1e-4\n",
        "  - Possible values to try: 1e-3, 5e-4, 1e-5\n",
        "- `patience`: Number of epochs to wait for improvement before triggering early stopping.\n",
        "  - Current value: 7\n",
        "  - Possible values to try: 5, 10, 15\n",
        "- `pos_weight_val`: Weight assigned to the positive class in the loss function to handle class imbalance.\n",
        "  - Current value: Computed based on class distribution in the training data\n",
        "  - Possible values to try: Adjust the computed value by a factor of 0.8, 1.2, 1.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "bu6HhqTttRqg",
      "metadata": {
        "id": "bu6HhqTttRqg"
      },
      "outputs": [],
      "source": [
        "class EarlyStopping:\n",
        "    \"\"\"\n",
        "    Early stopping to stop training when a metric (e.g., MCC) has stopped improving.\n",
        "    If 'mode' is 'max', we look for metric going UP. If 'mode' is 'min', we look for metric going DOWN.\n",
        "    \"\"\"\n",
        "    def __init__(self, patience=7, min_delta=0.0, mode='max'):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.mode = mode  # 'max' or 'min'\n",
        "        self.best_metric = None\n",
        "        self.counter = 0\n",
        "        self.early_stop = False\n",
        "\n",
        "    def __call__(self, current_metric):\n",
        "        if self.best_metric is None:\n",
        "            self.best_metric = current_metric\n",
        "        else:\n",
        "            # For 'max' mode, improvement means current_metric >= best_metric + min_delta\n",
        "            if self.mode == 'max':\n",
        "                if current_metric < (self.best_metric + self.min_delta):\n",
        "                    self.counter += 1\n",
        "                else:\n",
        "                    self.best_metric = current_metric\n",
        "                    self.counter = 0\n",
        "            else:  # 'min' mode\n",
        "                if current_metric > (self.best_metric - self.min_delta):\n",
        "                    self.counter += 1\n",
        "                else:\n",
        "                    self.best_metric = current_metric\n",
        "                    self.counter = 0\n",
        "\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "\n",
        "def train_epoch(model, train_loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    for batch in train_loader:  # Remove tqdm here\n",
        "        strain_emb, phage_emb, labels = batch\n",
        "        strain_emb = strain_emb.to(device)\n",
        "        phage_emb = phage_emb.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(strain_emb, phage_emb)\n",
        "        loss = criterion(logits, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        preds = torch.sigmoid(logits).reshape(-1).detach().cpu().numpy()\n",
        "        all_preds.extend(preds)\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    epoch_loss = total_loss / len(train_loader)\n",
        "    epoch_mcc = matthews_corrcoef(\n",
        "        (np.array(all_labels) > 0.5).astype(int),\n",
        "        (np.array(all_preds) > 0.5).astype(int)\n",
        "    )\n",
        "    return epoch_loss, epoch_mcc\n",
        "\n",
        "def validate(model, val_loader, criterion, device):\n",
        "    \"\"\"Validate the model on a held-out set.\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for strain_emb, phage_emb, labels in val_loader:\n",
        "            strain_emb = strain_emb.to(device)\n",
        "            phage_emb  = phage_emb.to(device)\n",
        "            labels     = labels.to(device)\n",
        "\n",
        "            logits = model(strain_emb, phage_emb)\n",
        "            loss   = criterion(logits, labels)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            preds = torch.sigmoid(logits).cpu().numpy()\n",
        "            all_preds.extend(preds)\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    val_loss = total_loss / len(val_loader)\n",
        "    val_mcc = matthews_corrcoef(\n",
        "        (np.array(all_labels) > 0.5).astype(int),\n",
        "        (np.array(all_preds) > 0.5).astype(int)\n",
        "    )\n",
        "    return val_loss, val_mcc\n",
        "\n",
        "def train_model(model, train_loader, val_loader,\n",
        "                num_epochs=100,\n",
        "                learning_rate=1e-4,\n",
        "                patience=7,\n",
        "                device='cuda',\n",
        "                pos_weight_val=None):\n",
        "\n",
        "    if pos_weight_val is not None:\n",
        "        criterion = nn.BCEWithLogitsLoss(\n",
        "            pos_weight=torch.tensor([pos_weight_val], device=device)\n",
        "        )\n",
        "    else:\n",
        "        criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode='max', factor=0.5, patience=5, verbose=True\n",
        "    )\n",
        "    early_stopping = EarlyStopping(patience=patience, mode='max')\n",
        "    history = {'train_loss': [], 'val_loss': [], 'train_mcc': [], 'val_mcc': []}\n",
        "\n",
        "    # Calculate total steps for tqdm\n",
        "    total_steps = len(train_loader) * num_epochs\n",
        "\n",
        "    # Create a single tqdm loop for the entire training process\n",
        "    pbar_epochs = tqdm(total=num_epochs, desc=\"Training\", position=0, leave=True)\n",
        "\n",
        "    try:\n",
        "        for epoch in range(num_epochs):\n",
        "            # Train and validate\n",
        "            train_loss, train_mcc = train_epoch(model, train_loader, optimizer, criterion, device)\n",
        "            val_loss, val_mcc = validate(model, val_loader, criterion, device)\n",
        "\n",
        "            # Update scheduler and history\n",
        "            scheduler.step(val_mcc)\n",
        "            history['train_loss'].append(train_loss)\n",
        "            history['val_loss'].append(val_loss)\n",
        "            history['train_mcc'].append(train_mcc)\n",
        "            history['val_mcc'].append(val_mcc)\n",
        "\n",
        "            # Update tqdm progress bar after each epoch\n",
        "            pbar_epochs.update(1)  # Increment by 1 for each completed epoch\n",
        "            pbar_epochs.set_postfix({\n",
        "                'Epoch': epoch + 1,\n",
        "                'train_loss': f'{train_loss:.3f}',\n",
        "                'val_loss': f'{val_loss:.3f}',\n",
        "                'train_mcc': f'{train_mcc:.3f}',\n",
        "                'val_mcc': f'{val_mcc:.3f}'\n",
        "            })\n",
        "\n",
        "            # Check early stopping\n",
        "            early_stopping(val_mcc)\n",
        "            if early_stopping.early_stop:\n",
        "                print(\"\\nEarly stopping triggered.\")\n",
        "                break\n",
        "\n",
        "    finally:\n",
        "        pbar_epochs.close()  # Close tqdm progress bar\n",
        "\n",
        "    return history\n",
        "\n",
        "def plot_training_history(history):\n",
        "    epochs = len(history['train_loss'])\n",
        "    plt.figure(figsize=(12,5))\n",
        "\n",
        "    # Loss\n",
        "    plt.subplot(1,2,1)\n",
        "    plt.plot(range(1, epochs+1), history['train_loss'], label='Train Loss')\n",
        "    plt.plot(range(1, epochs+1), history['val_loss'],   label='Val Loss')\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.title(\"Loss vs. Epochs\")\n",
        "    plt.legend()\n",
        "\n",
        "    # MCC\n",
        "    plt.subplot(1,2,2)\n",
        "    plt.plot(range(1, epochs+1), history['train_mcc'], label='Train MCC')\n",
        "    plt.plot(range(1, epochs+1), history['val_mcc'],   label='Val MCC')\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"MCC\")\n",
        "    plt.title(\"MCC vs. Epochs\")\n",
        "    plt.legend()\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xonZjalVvTFv",
      "metadata": {
        "id": "xonZjalVvTFv"
      },
      "source": [
        "## Model Performance Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "YLe8ULcGqqsb",
      "metadata": {
        "id": "YLe8ULcGqqsb"
      },
      "outputs": [],
      "source": [
        "def predict_and_evaluate(model, data_loader, device):\n",
        "    \"\"\"Makes predictions and calculates performance metrics.\"\"\"\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for strain_emb, phage_emb, labels in data_loader:\n",
        "            strain_emb = strain_emb.to(device)\n",
        "            phage_emb = phage_emb.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            logits = model(strain_emb, phage_emb)  # Get raw logits\n",
        "            preds = torch.sigmoid(logits).cpu().numpy()  # Apply sigmoid to get probabilities\n",
        "            all_preds.extend(preds)\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    all_preds = np.array(all_preds).flatten()\n",
        "    all_labels = np.array(all_labels).flatten()\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(all_labels, (all_preds > 0.5).astype(int))\n",
        "    precision = precision_score(all_labels, (all_preds > 0.5).astype(int))\n",
        "    recall = recall_score(all_labels, (all_preds > 0.5).astype(int))\n",
        "    f1 = f1_score(all_labels, (all_preds > 0.5).astype(int))\n",
        "    mcc = matthews_corrcoef(all_labels, (all_preds > 0.5).astype(int))\n",
        "    roc_auc = roc_auc_score(all_labels, all_preds)\n",
        "    pr_auc = average_precision_score(all_labels, all_preds)\n",
        "    conf_matrix = confusion_matrix(all_labels, (all_preds > 0.5).astype(int))\n",
        "\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n",
        "    print(f\"MCC: {mcc:.4f}\")\n",
        "    print(f\"ROC AUC: {roc_auc:.4f}\")\n",
        "    print(f\"PR AUC: {pr_auc:.4f}\")\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(conf_matrix)\n",
        "\n",
        "    return accuracy, precision, recall, f1, mcc, roc_auc, pr_auc, conf_matrix, all_preds, all_labels\n",
        "\n",
        "def plot_confusion_matrix(conf_matrix):\n",
        "    \"\"\"Plots the confusion matrix using seaborn.\"\"\"\n",
        "    plt.figure(figsize=(6, 6))\n",
        "    sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False,\n",
        "                xticklabels=['Predicted Negative', 'Predicted Positive'],\n",
        "                yticklabels=['Actual Negative', 'Actual Positive'])\n",
        "    plt.title(\"Confusion Matrix\")\n",
        "    plt.xlabel(\"Predicted Label\")\n",
        "    plt.ylabel(\"True Label\")\n",
        "    plt.show()\n",
        "\n",
        "def plot_roc_curve(all_labels, all_preds):\n",
        "    \"\"\"Plots the ROC curve.\"\"\"\n",
        "    fpr, tpr, _ = roc_curve(all_labels, all_preds)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.show()\n",
        "\n",
        "def plot_precision_recall_curve(all_labels, all_preds):\n",
        "    \"\"\"Plots the precision-recall curve.\"\"\"\n",
        "    precision, recall, _ = precision_recall_curve(all_labels, all_preds)\n",
        "    pr_auc = auc(recall, precision)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(recall, precision, color='darkorange', lw=2, label=f'PR curve (area = {pr_auc:.2f})')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('Recall')\n",
        "    plt.ylabel('Precision')\n",
        "    plt.title('Precision-Recall Curve')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.show()\n",
        "\n",
        "def get_attention_scores(model, strain_emb, phage_emb, device):\n",
        "    \"\"\"Gets attention scores from the model.\"\"\"\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        strain_emb = strain_emb.to(device)\n",
        "        phage_emb = phage_emb.to(device)\n",
        "        _, (strain_attn, phage_attn) = model(strain_emb, phage_emb, return_attn=True)\n",
        "    return strain_attn, phage_attn # these are still on device, need to .cpu() if needed\n",
        "\n",
        "# accuracy, precision, recall, f1, mcc, roc_auc, pr_auc, conf_matrix = predict_and_evaluate(model, test_loader, device)\n",
        "# plot_confusion_matrix(conf_matrix)\n",
        "# plot_roc_curve(all_labels, all_preds)  # Assuming you saved all_labels and all_preds in predict_and_evaluate\n",
        "# plot_precision_recall_curve(all_labels, all_preds)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39aQPVY0tRqg",
      "metadata": {
        "id": "39aQPVY0tRqg"
      },
      "source": [
        "## Main Training Script\n",
        "\n",
        "Description: This block contains a function for the full workflow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "HDRboY6CtRqg",
      "metadata": {
        "id": "HDRboY6CtRqg"
      },
      "outputs": [],
      "source": [
        "def main(debug=False):\n",
        "    # 1. Load data\n",
        "    strain_embeddings = load_embeddings('/content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/strains/')\n",
        "    phage_embeddings  = load_embeddings('/content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/phages/')\n",
        "\n",
        "    # 2. Load and split interactions\n",
        "    interactions_df = pd.read_csv('/content/drive/MyDrive/Arkin/set_transformer_data/EDGE_interaction_long_172_no2.csv')\n",
        "\n",
        "    # Filter interactions_df before splitting\n",
        "    strain_keys = set(strain_embeddings.keys())\n",
        "    phage_keys = set(phage_embeddings.keys())\n",
        "    interactions_df = interactions_df[interactions_df['strain'].isin(strain_keys) & interactions_df['phage'].isin(phage_keys)]\n",
        "\n",
        "    # Debug mode filtering\n",
        "    if debug:\n",
        "        random_strains = random.sample(list(strain_keys), 30)\n",
        "        random_phages = random.sample(list(phage_keys), 30)\n",
        "        interactions_df = interactions_df[interactions_df['strain'].isin(random_strains) & interactions_df['phage'].isin(random_phages)]\n",
        "\n",
        "    train_df, test_df = filter_interactions_by_strain(interactions_df)\n",
        "\n",
        "    # 2a. Compute pos_weight from train_df\n",
        "    num_pos = (train_df['interaction'] == 1).sum()\n",
        "    num_neg = (train_df['interaction'] == 0).sum()\n",
        "    pos_weight_val = num_neg / max(num_pos, 1)  # avoid divide-by-zero\n",
        "\n",
        "    print(f\"pos_weight_val = {pos_weight_val:.3f}\")\n",
        "\n",
        "    # 3. Create data loaders\n",
        "    train_loader, test_loader = create_data_loaders(\n",
        "        train_df,\n",
        "        test_df,\n",
        "        strain_embeddings,\n",
        "        phage_embeddings,\n",
        "        batch_size=1\n",
        "    )\n",
        "\n",
        "    # 4. Initialize model\n",
        "    model = StrainPhageTransformer(\n",
        "        embedding_dim=1280,\n",
        "        num_heads=8,\n",
        "        strain_inds=128,\n",
        "        phage_inds=64,\n",
        "        dropout=0.1,\n",
        "        ln=True\n",
        "    ).to(device)\n",
        "\n",
        "    # 5. Train model, passing pos_weight_val\n",
        "    history = train_model(\n",
        "        model,\n",
        "        train_loader,\n",
        "        test_loader,\n",
        "        num_epochs=100,\n",
        "        learning_rate=1e-4,\n",
        "        patience=7,\n",
        "        device=device,\n",
        "        pos_weight_val=pos_weight_val\n",
        "    )\n",
        "\n",
        "    # 6. Save model and results\n",
        "    torch.save({\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'history': history,\n",
        "    }, 'strain_phage_transformer.pt')\n",
        "\n",
        "    # 7. Plot training history\n",
        "    plot_training_history(history)\n",
        "\n",
        "    # 6. Prediction and Evaluation within main()\n",
        "    accuracy, precision, recall, f1, mcc, roc_auc, pr_auc, conf_matrix, all_preds, all_labels = predict_and_evaluate(model, test_loader, device)\n",
        "\n",
        "    # 7. Plotting within main() (optional, but convenient)\n",
        "    plot_confusion_matrix(conf_matrix)\n",
        "    plot_roc_curve(all_labels, all_preds)  # Assuming you have all_labels and all_preds from predict_and_evaluate\n",
        "    plot_precision_recall_curve(all_labels, all_preds)\n",
        "\n",
        "    # Return the desired values\n",
        "    return model, history, accuracy, precision, recall, f1, mcc, roc_auc, pr_auc, conf_matrix, all_preds, all_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UbMcF8fXwE4w",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UbMcF8fXwE4w",
        "outputId": "c9656aab-7fbe-4afa-b3f6-b8f4bda4e643"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parsing:  ECOR17\n",
            "Parsing:  ECOR71\n",
            "Parsing:  ECRC22\n",
            "Parsing:  ECRC46\n",
            "Parsing:  ECRC21\n",
            "Parsing:  ECRC35\n",
            "Parsing:  NILS34\n",
            "Parsing:  NILS33\n",
            "Parsing:  ECOR62\n",
            "Parsing:  DEC1A\n",
            "Parsing:  ECOR20\n",
            "Parsing:  NILS40\n",
            "Parsing:  ECOR5\n",
            "Parsing:  ECOR50\n",
            "Parsing:  ECOR7\n",
            "Parsing:  ECOR37\n",
            "Parsing:  ECRC13\n",
            "Parsing:  B6A1\n",
            "Parsing:  ECOR10\n",
            "Parsing:  ECOR47\n",
            "Parsing:  ECOR29\n",
            "Parsing:  ECOR36\n",
            "Parsing:  916A\n",
            "Parsing:  ECOR38\n",
            "Parsing:  ECOR59\n",
            "Parsing:  ECOR61\n",
            "Parsing:  ECRC17\n",
            "Parsing:  ECRC12\n",
            "Parsing:  ECOR40\n",
            "Parsing:  ECOR70\n",
            "Parsing:  ECOR9\n",
            "Parsing:  ECOR43\n",
            "Parsing:  ECOR41\n",
            "Parsing:  ECOR14\n",
            "Parsing:  ECOR39\n",
            "Parsing:  ECOR6\n",
            "Parsing:  ECOR52\n",
            "Parsing:  ECOR44\n",
            "Parsing:  ECOR56\n",
            "Parsing:  BW25113\n",
            "Parsing:  ECOR58\n",
            "Parsing:  ECOR69\n",
            "Parsing:  ECOR15\n",
            "Parsing:  ECOR55\n",
            "Parsing:  ECOR16\n",
            "Parsing:  ECOR31\n",
            "Parsing:  ECOR54\n",
            "Parsing:  55989\n",
            "Parsing:  ECOR63\n",
            "Parsing:  ECOR24\n",
            "Parsing:  ECOR67\n",
            "Parsing:  ECOR53\n",
            "Parsing:  ECOR2\n",
            "Parsing:  ECOR49\n",
            "Parsing:  ECOR35\n",
            "Parsing:  ECRC14\n",
            "Parsing:  ECOR23\n",
            "Parsing:  ECRC2\n",
            "Parsing:  ECRC1\n",
            "Parsing:  ECOR42\n",
            "Parsing:  ECOR18\n",
            "Parsing:  AL505\n",
            "Parsing:  ECOR26\n",
            "Parsing:  ECOR25\n",
            "Parsing:  ECOR13\n",
            "Parsing:  B49\n",
            "Parsing:  ECOR34\n",
            "Parsing:  381A\n",
            "Parsing:  ECOR60\n",
            "Parsing:  ECOR12\n",
            "Parsing:  ECOR68\n",
            "Parsing:  ECOR27\n",
            "Parsing:  ECOR8\n",
            "Parsing:  ECRC15\n",
            "Parsing:  ECRC23\n",
            "Parsing:  ECOR19\n",
            "Parsing:  ECOR46\n",
            "Parsing:  ECRC19\n",
            "Parsing:  ECOR66\n",
            "Parsing:  ECOR64\n",
            "Parsing:  ECOR57\n",
            "Parsing:  ECOR21\n",
            "Parsing:  ECRC25\n",
            "Parsing:  DEC2A\n",
            "Parsing:  ECRC24\n",
            "Parsing:  ECOR32\n",
            "Parsing:  536\n",
            "Parsing:  ECOR30\n",
            "Parsing:  ECOR33\n",
            "Parsing:  370D\n",
            "Parsing:  BL21\n",
            "Parsing:  ECOR22\n",
            "Parsing:  ECOR3\n",
            "Parsing:  ECOR1\n",
            "Parsing:  ECOR65\n",
            "Parsing:  ECOR51\n",
            "Parsing:  ECRC11\n",
            "Parsing:  ECOR28\n",
            "Parsing:  ECRC16\n",
            "Parsing:  ECOR45\n",
            "Parsing:  ECOR48\n",
            "Parsing:  ECOR11\n",
            "Parsing:  921A\n",
            "Parsing:  ECOR72\n",
            "Parsing:  ECRC39\n",
            "Parsing:  ECRC3\n",
            "Parsing:  LMR3158\n",
            "Parsing:  NILS82\n",
            "Parsing:  NILS44\n",
            "Parsing:  NILS55\n",
            "Parsing:  NILS57\n",
            "Parsing:  NILS14\n",
            "Parsing:  H48\n",
            "Parsing:  ECRC7\n",
            "Parsing:  NILS68\n",
            "Parsing:  NILS36\n",
            "Parsing:  LF82\n",
            "Parsing:  IAI39\n",
            "Parsing:  NILS43\n",
            "Parsing:  NILS76\n",
            "Parsing:  NILS22\n",
            "Parsing:  MT1B1\n",
            "Parsing:  ECRC43\n",
            "Parsing:  NILS16\n",
            "Parsing:  H4\n",
            "Parsing:  NILS48\n",
            "Parsing:  EDL933\n",
            "Parsing:  ECRC18\n",
            "Parsing:  NILS41\n",
            "Parsing:  ECRC34\n",
            "Parsing:  ECRC38\n",
            "Parsing:  NILS78\n",
            "Parsing:  NILS09\n",
            "Parsing:  IAI1\n",
            "Parsing:  NILS13\n",
            "Parsing:  M863\n",
            "Parsing:  ROAR059\n",
            "Parsing:  NILS02\n",
            "Parsing:  NILS21\n",
            "Parsing:  NILS77\n",
            "Parsing:  NILS39\n",
            "Parsing:  NILS65\n",
            "Parsing:  NILS53\n",
            "Parsing:  NILS71\n",
            "Parsing:  NILS64\n",
            "Parsing:  NILS25\n",
            "Parsing:  NILS04\n",
            "Parsing:  NILS32\n",
            "Parsing:  NILS30\n",
            "Parsing:  NILS20\n",
            "Parsing:  ECRC41\n",
            "Parsing:  NILS54\n",
            "Parsing:  ROAR036\n",
            "Parsing:  NILS45\n",
            "Parsing:  ROAR400\n",
            "Parsing:  ROAR434\n",
            "Parsing:  NILS17\n",
            "Parsing:  NILS52\n",
            "Parsing:  NILS67\n",
            "Parsing:  ECRC27\n",
            "Parsing:  ROAR012\n",
            "Parsing:  ECRC44\n",
            "Parsing:  ROAR131\n",
            "Parsing:  NILS29\n",
            "Parsing:  ECRC28\n",
            "Parsing:  NILS05\n",
            "Parsing:  T145\n",
            "Parsing:  NILS38\n",
            "Parsing:  NILS01\n",
            "Parsing:  ECRC9\n",
            "Parsing:  NILS11\n",
            "Parsing:  NILS56\n",
            "Parsing:  NILS15\n",
            "Parsing:  NILS46\n",
            "Parsing:  NILS42\n",
            "Parsing:  NILS73\n",
            "Parsing:  ECRC32\n",
            "Parsing:  ECRC6\n",
            "Parsing:  NILS23\n",
            "Parsing:  ECRC36\n",
            "Parsing:  LM33\n",
            "Parsing:  NILS47\n",
            "Parsing:  NILS66\n",
            "Parsing:  NILS63\n",
            "Parsing:  ECRC37\n",
            "Parsing:  NILS59\n",
            "Parsing:  NILS51\n",
            "Parsing:  ED1A\n",
            "Parsing:  NILS81\n",
            "Parsing:  NILS58\n",
            "Parsing:  ECRC49\n",
            "Parsing:  ECRC45\n",
            "Parsing:  NILS80\n",
            "Parsing:  NILS49\n",
            "Parsing:  NILS60\n",
            "Parsing:  NILS26\n",
            "Parsing:  NILS12\n",
            "Parsing:  ECRC10\n",
            "Parsing:  NILS50\n",
            "Parsing:  NILS61\n",
            "Parsing:  NILS70\n",
            "Parsing:  NILS31\n",
            "Parsing:  NILS19\n",
            "Parsing:  ECRC40\n",
            "Parsing:  ECRC48\n",
            "Parsing:  NILS10\n",
            "Parsing:  NILS18\n",
            "Parsing:  ECRC50\n",
            "Parsing:  ECOR4\n",
            "Parsing:  ECRC8\n",
            "Parsing:  ROAR387\n",
            "Parsing:  NILS28\n",
            "Parsing:  NILS07\n",
            "Parsing:  NILS74\n",
            "Parsing:  ECRC4\n",
            "Parsing:  H14\n",
            "Parsing:  NILS72\n",
            "Parsing:  NILS24\n",
            "Parsing:  NILS08\n",
            "Parsing:  ECRC5\n",
            "Parsing:  NILS37\n",
            "Parsing:  ECRC30\n",
            "Parsing:  ECRC26\n",
            "Parsing:  ROAR274\n",
            "Parsing:  NILS69\n",
            "Parsing:  NILS75\n",
            "Parsing:  ECRC33\n",
            "Parsing:  NILS06\n",
            "Parsing:  TW10509\n",
            "Parsing:  ECRC31\n",
            "Parsing:  ECRC47\n",
            "Parsing:  ROAR029\n",
            "Parsing:  NILS79\n",
            "Parsing:  NILS27\n",
            "Parsing:  ECRC29\n",
            "Parsing:  NILS62\n",
            "Parsing:  NILS35\n",
            "Parsing:  NILS03\n",
            "Parsing:  ECRC20\n",
            "Parsing:  ECRC42\n",
            "Loaded 240 embeddings from /content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/strains\n",
            "Parsing:  Bas03\n",
            "Parsing:  Bas01\n",
            "Parsing:  Bas05\n",
            "Parsing:  Bas17\n",
            "Parsing:  1260\n",
            "Parsing:  Bas02\n",
            "Parsing:  Bas14\n",
            "Parsing:  Bas06\n",
            "Parsing:  Bas10\n",
            "Parsing:  Bas12\n",
            "Parsing:  1374\n",
            "Parsing:  1265\n",
            "Parsing:  1255\n",
            "Parsing:  1262\n",
            "Parsing:  1258\n",
            "Parsing:  Bas25\n",
            "Parsing:  1257\n",
            "Parsing:  Bas29\n",
            "Parsing:  Bas21\n",
            "Parsing:  1269\n",
            "Parsing:  1263\n",
            "Parsing:  Bas08\n",
            "Parsing:  Bas09\n",
            "Parsing:  1261\n",
            "Parsing:  Bas27\n",
            "Parsing:  Bas20\n",
            "Parsing:  Bas34\n",
            "Parsing:  Bas15\n",
            "Parsing:  Bas04\n",
            "Parsing:  1259\n",
            "Parsing:  Bas07\n",
            "Parsing:  Bas31\n",
            "Parsing:  1266\n",
            "Parsing:  Bas22\n",
            "Parsing:  Bas13\n",
            "Parsing:  1267\n",
            "Parsing:  Bas16\n",
            "Parsing:  1264\n",
            "Parsing:  Bas19\n",
            "Parsing:  1268\n",
            "Parsing:  lambda\n",
            "Parsing:  Bas36\n",
            "Parsing:  JK45\n",
            "Parsing:  CM8\n",
            "Parsing:  T4\n",
            "Parsing:  JK23\n",
            "Parsing:  Bas52\n",
            "Parsing:  Bas37\n",
            "Parsing:  Bas60\n",
            "Parsing:  T2\n",
            "Parsing:  Bas47\n",
            "Parsing:  Bas59\n",
            "Parsing:  Bas64\n",
            "Parsing:  Bas39\n",
            "Parsing:  CM1\n",
            "Parsing:  Bas51\n",
            "Parsing:  Bas63\n",
            "Parsing:  JK16\n",
            "Parsing:  JK38\n",
            "Parsing:  Bas43\n",
            "Parsing:  JK36\n",
            "Parsing:  JK32\n",
            "Parsing:  T5\n",
            "Parsing:  T7\n",
            "Parsing:  EV219\n",
            "Parsing:  Bas46\n",
            "Parsing:  Bas38\n",
            "Parsing:  Bas48\n",
            "Parsing:  Bas28\n",
            "Parsing:  Bas58\n",
            "Parsing:  Bas44\n",
            "Parsing:  Bas57\n",
            "Parsing:  Bas33\n",
            "Parsing:  Bas45\n",
            "Parsing:  Bas65\n",
            "Parsing:  Bas49\n",
            "Parsing:  Bas68\n",
            "Parsing:  Bas42\n",
            "Parsing:  Bas54\n",
            "Parsing:  EV146\n",
            "Parsing:  EV240\n",
            "Parsing:  Bas41\n",
            "Parsing:  Bas56\n",
            "Parsing:  Bas69\n",
            "Parsing:  Bas35\n",
            "Parsing:  T3\n",
            "Parsing:  Bas61\n",
            "Parsing:  N4\n",
            "Parsing:  T6\n",
            "Loaded 89 embeddings from /content/drive/MyDrive/Arkin/set_transformer_data/embedding_sets/phages\n",
            "Train set: 348 interactions, 12 strains\n",
            "Test set:  89 interactions, 3 strains\n",
            "pos_weight_val = 3.971\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  12%|        | 12/100 [03:45<27:25, 18.70s/it, Epoch=12, train_loss=3.837, val_loss=10.079, train_mcc=0.000, val_mcc=0.000]"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    model, history, accuracy, precision, recall, f1, mcc, roc_auc, pr_auc, conf_matrix, all_preds, all_labels = main(debug=True)\n",
        "\n",
        "    # Create a dictionary to store the outputs\n",
        "    outputs = {\n",
        "         'model': model,\n",
        "         'history': history,\n",
        "         'accuracy': accuracy,\n",
        "         'precision': precision,\n",
        "         'recall': recall,\n",
        "         'f1': f1,\n",
        "         'mcc': mcc,\n",
        "         'roc_auc': roc_auc,\n",
        "         'pr_auc': pr_auc,\n",
        "         'conf_matrix': conf_matrix,\n",
        "         'all_preds': all_preds,\n",
        "         'all_labels': all_labels\n",
        "    }\n",
        "\n",
        "    # Get the directory of the interaction matrix\n",
        "    output_dir = '/content/drive/MyDrive/Arkin/set_transformer_data/'\n",
        "\n",
        "    # Create a subdirectory for outputs if it doesn't exist\n",
        "    output_subdir = os.path.join(output_dir, 'outputs')\n",
        "    os.makedirs(output_subdir, exist_ok=True)\n",
        "\n",
        "    # Construct the full file path for the output file\n",
        "    output_file_path = os.path.join(output_subdir, 'main_function_outputs_test.pkl')  # or .joblib\n",
        "\n",
        "    # Save the outputs\n",
        "    with open(output_file_path, 'wb') as f:  # or 'wb' for joblib\n",
        "        pickle.dump(outputs, f)  # or joblib.dump(outputs, f)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9bNqQJacqrcJ",
      "metadata": {
        "id": "9bNqQJacqrcJ"
      },
      "source": [
        "## Model Performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "S3ATjCfRrAqH",
      "metadata": {
        "id": "S3ATjCfRrAqH"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "version": "3.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}